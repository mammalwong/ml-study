{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 2.74546980e-03  1.28543901e-05 -1.67614549e-03 -1.60000849e-02\n",
      "  9.25640911e-02  3.89107061e-03  8.59707221e-01 -1.69117749e-03\n",
      "  1.00000000e+00  3.28603275e-02  3.89091601e-03  8.53495002e-01\n",
      " -2.64752253e-03  1.00000000e+00  4.40813392e-01  4.45819497e-01\n",
      "  4.61422116e-01  4.89549488e-01  5.34102023e-01  6.02460206e-01\n",
      "  7.09147871e-01  8.85930538e-01  1.00000000e+00  1.00000000e+00]\n",
      "Box(24,)\n",
      "Box(4,) [-1. -1. -1. -1.] [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space, env.action_space.low, env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'action', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, dis_output_ns=[], beta=0.0001):\n",
    "        \n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__dis_output_ns = dis_output_ns\n",
    "        self.__l_shared = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.LSTM(128, return_sequences=False),\n",
    "            keras.layers.Dropout(0.5)]\n",
    "        self.__l_policy_mu = [\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n)]\n",
    "        self.__l_policy_sigma = [\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n, kernel_initializer='he_uniform'),\n",
    "            keras.layers.Activation('softplus')]\n",
    "        self.__l_policy_dis = [[\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(dis),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Softmax()]\n",
    "            for dis in self.__dis_output_ns]\n",
    "        self.__l_value = [\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m_mu = apply_layers(m, self.__l_policy_mu)\n",
    "        m_sigma = apply_layers(m, self.__l_policy_sigma)\n",
    "        m_dis = [apply_layers(m, dis) for dis in self.__l_policy_dis]\n",
    "        m = keras.layers.Concatenate()([m_mu, m_sigma] + m_dis)\n",
    "        self.__m_policy = keras.models.Model([m_input], [m])\n",
    "        self.__m_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_value)\n",
    "        self.__m_value = keras.models.Model([m_input], [m])\n",
    "        self.__m_value.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared = apply_layers(m, self.__l_shared)\n",
    "        m_policy_mu = apply_layers(m_shared, self.__l_policy_mu)\n",
    "        m_policy_sigma = apply_layers(m_shared, self.__l_policy_sigma)\n",
    "        m_policy_dis = [apply_layers(m_shared, dis) for dis in self.__l_policy_dis]\n",
    "        m_policy = keras.layers.Concatenate()([m_policy_mu, m_policy_sigma] + m_policy_dis)\n",
    "        m_value = apply_layers(m_shared, self.__l_value)\n",
    "        self.__m_value_policy = keras.models.Model([m_input], [m_value, m_policy])\n",
    "        self.__m_value_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_value, m_policy = self.__m_value_policy(m)\n",
    "        m = keras.layers.Concatenate()([m_value, m_policy])\n",
    "        self.__m_optimizer = keras.optimizers.Adam(clipnorm=5.)\n",
    "        self.__m_train = keras.models.Model([m_input], [m])\n",
    "        self.__m_train.compile(self.__m_optimizer,\n",
    "            lambda y_true, y_pred: model.__loss(\n",
    "                y_true, y_pred, beta, self.__output_n, self.__dis_output_ns))\n",
    "        \n",
    "        self.__m_policy.summary()\n",
    "        self.__m_value.summary()\n",
    "        self.__m_train.summary()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __loss(y_true, y_pred, beta, output_size, dis_sizes):\n",
    "        \n",
    "        r, action, dis_actions, = y_true[:,:1], y_true[:,1:1+output_size], [y_true[:, \\\n",
    "            1+output_size*2+sum(dis_sizes[:i]):\n",
    "            1+output_size*2+sum(dis_sizes[:i+1])]\n",
    "            for i in range(len(dis_sizes))]\n",
    "        value, policy_mu, policy_sigma, policy_dis = \\\n",
    "            y_pred[:,:1], y_pred[:,1:1+output_size], \\\n",
    "            y_pred[:,1+output_size:1+output_size*2], [y_pred[:, \\\n",
    "            1+output_size*2+sum(dis_sizes[:i]):\n",
    "            1+output_size*2+sum(dis_sizes[:i+1])]\n",
    "            for i in range(len(dis_sizes))]\n",
    "        \n",
    "        advantage = r - value\n",
    "        value_loss = 0.5 * K.mean(K.square(advantage))\n",
    "        \n",
    "        log_choosen_action_prob = K.sum(K.exp( \\\n",
    "            -K.square(action - policy_mu)/ \\\n",
    "            (2*K.square(policy_sigma)+K.epsilon())), axis=-1, keepdims=True)\n",
    "        action_loss = -K.mean(log_choosen_action_prob * advantage)\n",
    "        entropy = K.mean(K.sum(0.5*K.log( \\\n",
    "            2*np.pi*np.e*K.square(policy_sigma)+K.epsilon()), axis=-1, keepdims=True))\n",
    "        \n",
    "        for action_onehot, policy in zip(dis_actions, policy_dis):\n",
    "            log_policy = K.log(policy + K.epsilon())\n",
    "            log_choosen_action_prob = K.sum(action_onehot * log_policy, axis=-1, keepdims=True)\n",
    "            action_loss += -K.mean(log_choosen_action_prob * advantage)\n",
    "            entropy += K.mean(-K.sum(policy * log_policy, axis=-1, keepdims=True))\n",
    "        \n",
    "        return action_loss + value_loss - beta * entropy\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        self.__m_train.fit(\n",
    "            x=np.array([s.state for s in samples], dtype=np.float32),\n",
    "            y=np.hstack([\n",
    "                np.reshape(np.array([s.reward for s in samples], dtype=np.float32), (-1, 1)),\n",
    "                np.array([s.action[0] for s in samples]),\n",
    "                np.zeros((len(samples),len(samples[0].action)))] + [\n",
    "                np.array([s.action[i+1] for s in samples])\n",
    "                for i in range(len(self.__dis_output_ns))]),\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "    \n",
    "    def evalute_value(self, state, verbose=False):\n",
    "        v = self.__m_value.predict(\n",
    "            np.array([state], dtype=np.float32))[0,0]\n",
    "        if verbose:\n",
    "            print(v)\n",
    "        return v\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_policy.predict(\n",
    "            np.array([state], dtype=np.float32))[0]\n",
    "        action_prob_mu = action_prob[:self.__output_n]\n",
    "        action_prob_sigma = action_prob[self.__output_n:self.__output_n*2]\n",
    "        action_prob_dis = [action_prob[\n",
    "            self.__output_n*2+sum(self.__dis_output_ns[:i]):\n",
    "            self.__output_n*2+sum(self.__dis_output_ns[:i+1])]\n",
    "            for i in range(len(self.__dis_output_ns))]\n",
    "        if verbose:\n",
    "            print(action_prob_mu, action_prob_sigma, action_prob_dis)\n",
    "        return action_prob_mu, action_prob_sigma, action_prob_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_to_continuous(discretes, sample=True):\n",
    "    if sample:\n",
    "        choosen_actions = [np.random.choice(\n",
    "            list(range(dis.shape[-1])), p=dis)\n",
    "            for dis in discretes]\n",
    "    else:\n",
    "        choosen_actions = [np.argmax(dis) for dis in discretes]\n",
    "    return np.array([\n",
    "        env.action_space.low[i] + \\\n",
    "        (choosen_actions[i]/(discretes[i].shape[-1]-1)) * \\\n",
    "        (env.action_space.high[i]-env.action_space.low[i])\n",
    "        if discretes[i].shape[-1] > 1 else \\\n",
    "        (env.action_space.high[i]+env.action_space.low[i])/2\n",
    "        for i in range(len(discretes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, gamma=0.98, max_steps=1000, n_prev_states=32, verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx):\n",
    "        states = [e.state for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode, done):\n",
    "        if done:\n",
    "            discounted_reward = 0.\n",
    "        else:\n",
    "            discounted_reward = m.evalute_value(get_prev_states(episode, len(episode)-1))\n",
    "        episode[-1] = model_sample(\n",
    "                get_prev_states(episode, len(episode)-1),\n",
    "                episode[-1].action,\n",
    "                discounted_reward)\n",
    "        for i in reversed(range(len(episode)-1)):\n",
    "            discounted_reward = episode[i].reward + \\\n",
    "                gamma * discounted_reward\n",
    "            episode[i] = model_sample(\n",
    "                get_prev_states(episode, i),\n",
    "                episode[i].action,\n",
    "                discounted_reward)\n",
    "        samples.extend(episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob_mu, action_prob_sigma, action_prob_dis = \\\n",
    "            m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(np.hstack(action_prob_dis))\n",
    "        action = action_prob_mu + \\\n",
    "            np.random.randn(*action_prob_sigma.shape) * action_prob_sigma\n",
    "        state_1, reward, done, _ = \\\n",
    "            env.step(action + discrete_to_continuous(action_prob_dis, sample=True))\n",
    "        episode.append(model_sample(state_0, [action]+action_prob_dis, reward))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode, True)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode, False)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 24)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 24)     96          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          78336       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           256         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           256         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64)           256         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64)           256         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64)           256         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 15)           975         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 15)           975         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 15)           975         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 15)           975         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            260         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15)           60          dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15)           60          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15)           60          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15)           60          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            260         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 4)            0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 15)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "softmax_2 (Softmax)             (None, 15)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, 15)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "softmax_4 (Softmax)             (None, 15)           0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 68)           0           dense_2[0][0]                    \n",
      "                                                                 activation_3[0][0]               \n",
      "                                                                 softmax_1[0][0]                  \n",
      "                                                                 softmax_2[0][0]                  \n",
      "                                                                 softmax_3[0][0]                  \n",
      "                                                                 softmax_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 134,164\n",
      "Trainable params: 133,228\n",
      "Non-trainable params: 936\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 24)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 24)          96        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               78336     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 87,009\n",
      "Trainable params: 86,833\n",
      "Non-trainable params: 176\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, 24)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 [(None, 1), (None, 6 142741      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 69)           0           model_3[1][0]                    \n",
      "                                                                 model_3[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 142,741\n",
      "Trainable params: 141,677\n",
      "Non-trainable params: 1,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(24, 4, [15]*4, beta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(m, render=False, n_prev_states=32, sample=False):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    while True:\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action_mu, action_sigma, action_dis = m.get_action_prob(state_queue)\n",
    "        if sample:\n",
    "            action = action_mu + np.random.randn(*action_sigma.shape) * action_sigma\n",
    "        else:\n",
    "            action = action_mu\n",
    "        state, reward, done, _ = \\\n",
    "            env.step(action + discrete_to_continuous(action_dis, sample=sample))\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/30)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = gym.make('BipedalWalker-v2')\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std[action_prob] 0.0063002533\n",
      "max game len 279\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: -34.0220\n",
      "epoch 0 completed\n",
      "std[action_prob] 0.032328524\n",
      "max game len 294\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: -33.2661\n",
      "epoch 1 completed\n",
      "std[action_prob] 0.0180429\n",
      "max game len 1000\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: -42.3615\n",
      "epoch 2 completed\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    samples = play(env, m, max_steps=1000, verbose=True)\n",
    "    m.train(samples, epochs=1, verbose=True)\n",
    "    print('epoch {} completed'.format(i))\n",
    "    if (i+1) % 10 == 0:\n",
    "        test_result = [test(m, render=False) for _ in range(3)]\n",
    "        print('test result',\n",
    "              'mean', np.mean(test_result),\n",
    "              'std', np.std(test_result, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-136.425818450002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(m, render=True, sample=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
