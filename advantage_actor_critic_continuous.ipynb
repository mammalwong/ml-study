{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.virtualenvs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[-0.41720302  0.        ]\n",
      "Box(2,)\n",
      "Box(1,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'action', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, beta=0.0001):\n",
    "        \n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__l_shared = [\n",
    "            keras.layers.LSTM(16, return_sequences=False)]\n",
    "        self.__l_policy_mu = [\n",
    "            keras.layers.Dense(16, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n)]\n",
    "        self.__l_policy_sigma = [\n",
    "            keras.layers.Dense(16, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n, kernel_initializer='he_uniform'),\n",
    "            keras.layers.Activation('softplus')]\n",
    "        self.__l_value = [\n",
    "            keras.layers.Dense(16, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m_mu = apply_layers(m, self.__l_policy_mu)\n",
    "        m_sigma = apply_layers(m, self.__l_policy_sigma)\n",
    "        m = keras.layers.Concatenate()([m_mu, m_sigma])\n",
    "        self.__m_policy = keras.models.Model([m_input], [m])\n",
    "        self.__m_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_value)\n",
    "        self.__m_value = keras.models.Model([m_input], [m])\n",
    "        self.__m_value.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared = apply_layers(m, self.__l_shared)\n",
    "        m_policy_mu = apply_layers(m_shared, self.__l_policy_mu)\n",
    "        m_policy_sigma = apply_layers(m_shared, self.__l_policy_sigma)\n",
    "        m_policy = keras.layers.Concatenate()([m_policy_mu, m_policy_sigma])\n",
    "        m_value = apply_layers(m_shared, self.__l_value)\n",
    "        self.__m_value_policy = keras.models.Model([m_input], [m_value, m_policy])\n",
    "        self.__m_value_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_value, m_policy = self.__m_value_policy(m)\n",
    "        m = keras.layers.Concatenate()([m_value, m_policy])\n",
    "        self.__m_optimizer = keras.optimizers.Adam(clipnorm=5.)\n",
    "        self.__m_train = keras.models.Model([m_input], [m])\n",
    "        self.__m_train.compile(self.__m_optimizer,\n",
    "            lambda y_true, y_pred: model.__loss(y_true, y_pred, beta, self.__output_n))\n",
    "        \n",
    "        self.__m_policy.summary()\n",
    "        self.__m_value.summary()\n",
    "        self.__m_train.summary()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __loss(y_true, y_pred, beta, output_size):\n",
    "        r, action = y_true[:,:1], y_true[:,1:1+output_size]\n",
    "        value, policy_mu, policy_sigma = \\\n",
    "            y_pred[:,:1], y_pred[:,1:1+output_size], y_pred[:,1+output_size:1+output_size*2] \n",
    "        advantage = r - value\n",
    "        log_choosen_action_prob = \\\n",
    "            K.sum(K.exp(-K.square(action - policy_mu)/(2*K.square(policy_sigma)+K.epsilon())), axis=-1)\n",
    "        action_loss = -K.mean(log_choosen_action_prob * advantage)\n",
    "        value_loss = 0.5 * K.mean(K.square(advantage))\n",
    "        entropy = K.mean(K.sum(0.5*K.log( \\\n",
    "            2*np.pi*np.e*K.square(policy_sigma)+K.epsilon()), axis=-1))\n",
    "        return action_loss + value_loss - beta * entropy\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        self.__m_train.fit(\n",
    "            x=np.array([s.state for s in samples], dtype=np.float32),\n",
    "            y=np.hstack([\n",
    "                np.reshape(np.array([s.reward for s in samples], dtype=np.float32), (-1, 1)),\n",
    "                np.array([s.action for s in samples]),\n",
    "                np.zeros((len(samples),len(samples[0].action)))]),\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "    \n",
    "    def evalute_value(self, state, verbose=False):\n",
    "        v = self.__m_value.predict(\n",
    "            np.array([state], dtype=np.float32))[0,0]\n",
    "        if verbose:\n",
    "            print(v)\n",
    "        return v\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_policy.predict(\n",
    "            np.array([state], dtype=np.float32))[0]\n",
    "        action_prob_mu = action_prob[:self.__output_n]\n",
    "        action_prob_sigma = action_prob[self.__output_n:]\n",
    "        if verbose:\n",
    "            print(action_prob_mu, action_prob_sigma)\n",
    "        return action_prob_mu, action_prob_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, m, gamma=0.98, max_steps=1000, n_prev_states=8, verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx):\n",
    "        states = [e.state for e in episode[(idx-n_prev_states)+1:idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode, done):\n",
    "        if done:\n",
    "            discounted_reward = 0.\n",
    "        else:\n",
    "            discounted_reward = m.evalute_value(get_prev_states(episode, len(episode)-1))\n",
    "        episode[-1] = model_sample(\n",
    "                get_prev_states(episode, len(episode)-1),\n",
    "                episode[-1].action,\n",
    "                discounted_reward)\n",
    "        for i in reversed(range(len(episode)-1)):\n",
    "            discounted_reward = episode[i].reward + \\\n",
    "                gamma * discounted_reward\n",
    "            episode[i] = model_sample(\n",
    "                get_prev_states(episode, i),\n",
    "                episode[i].action,\n",
    "                discounted_reward)\n",
    "        samples.extend(episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob_mu, action_prob_sigma = m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(np.hstack([action_prob_mu, action_prob_sigma]))\n",
    "        action = action_prob_mu + \\\n",
    "            np.random.randn(*action_prob_sigma.shape) * action_prob_sigma\n",
    "        state_1, reward, done, _ = env.step(action)\n",
    "        episode.append(model_sample(state_0, action, reward))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode, True)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode, False)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 16)           1216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           272         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           272         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            17          activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            17          activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 activation_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,922\n",
      "Trainable params: 1,858\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                1216      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,569\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 [(None, 1), (None, 2 2275        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 3)            0           model_3[1][0]                    \n",
      "                                                                 model_3[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 2,275\n",
      "Trainable params: 2,179\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(2, 1, beta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(m, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    while True:\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action, _ = m.get_action_prob(state_queue)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/30)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = gym.make('MountainCarContinuous-v0')\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std[action_prob] 0.011637534\n",
      "max game len 999\n",
      "Epoch 1/1\n",
      " 448/1000 [============>.................] - ETA: 1s - loss: 3.9162"
     ]
    }
   ],
   "source": [
    "replays = []\n",
    "for i in range(500):\n",
    "    samples = play(env, m, max_steps=1000, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    m.train(replays, epochs=1, verbose=True)\n",
    "    if len(replays) > 10000:\n",
    "        replays.sort(key=lambda x: x.reward, reverse=True)\n",
    "        replays_best = replays[:3000]\n",
    "        replays_worst = replays[-3000:]\n",
    "        replays_middle = replays[3000:-3000]\n",
    "        random.shuffle(replays_middle)\n",
    "        replays = replays_best + replays_worst + replays_middle[:4000]\n",
    "    print('epoch {} completed'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(m, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
