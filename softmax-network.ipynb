{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train[:,:,:,np.newaxis]/255, X_test[:,:,:,np.newaxis]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 28, 28, 8)         208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 14, 14, 16)        3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 6, 6, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 2, 2, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 14,410\n",
      "Trainable params: 14,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = X_input = keras.layers.Input(X_train.shape[1:])\n",
    "X = keras.layers.Conv2D(8, (5,5), padding='same', activation='softmax')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Conv2D(16, (5,5), padding='same', activation='softmax')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Conv2D(32, (2,2), padding='valid', activation='softmax')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Conv2D(64, (2,2), padding='valid', activation='softmax')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Flatten()(X)\n",
    "X = keras.layers.Dense(np.max(Y_train)+1, activation='softmax')(X)\n",
    "M = keras.Model(X_input, X)\n",
    "M.compile('nadam', 'sparse_categorical_crossentropy', ['acc'])\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 1.2379 - acc: 0.5945 - val_loss: 0.3349 - val_acc: 0.9268\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.2441 - acc: 0.9383 - val_loss: 0.1660 - val_acc: 0.9557\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.1548 - acc: 0.9575 - val_loss: 0.1277 - val_acc: 0.9637\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.1211 - acc: 0.9655 - val_loss: 0.1008 - val_acc: 0.9696\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.0993 - acc: 0.9714 - val_loss: 0.0917 - val_acc: 0.9729\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.0850 - acc: 0.9749 - val_loss: 0.0791 - val_acc: 0.9753\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.0751 - acc: 0.9777 - val_loss: 0.0759 - val_acc: 0.9766\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.0681 - acc: 0.9797 - val_loss: 0.0683 - val_acc: 0.9779\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 18s 302us/step - loss: 0.0625 - acc: 0.9812 - val_loss: 0.0640 - val_acc: 0.9802\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 18s 303us/step - loss: 0.0579 - acc: 0.9830 - val_loss: 0.0613 - val_acc: 0.9805\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.0534 - acc: 0.9841 - val_loss: 0.0572 - val_acc: 0.9812\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0502 - acc: 0.9850 - val_loss: 0.0567 - val_acc: 0.9823\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0477 - acc: 0.9854 - val_loss: 0.0512 - val_acc: 0.9832\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0449 - acc: 0.9864 - val_loss: 0.0519 - val_acc: 0.9833\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0425 - acc: 0.9873 - val_loss: 0.0495 - val_acc: 0.9847\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0406 - acc: 0.9879 - val_loss: 0.0507 - val_acc: 0.9846\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0383 - acc: 0.9885 - val_loss: 0.0501 - val_acc: 0.9836\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0366 - acc: 0.9891 - val_loss: 0.0494 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0303 - acc: 0.9912 - val_loss: 0.0466 - val_acc: 0.9846\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0294 - acc: 0.9918 - val_loss: 0.0470 - val_acc: 0.9845\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0290 - acc: 0.9920 - val_loss: 0.0471 - val_acc: 0.9842\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0288 - acc: 0.9918 - val_loss: 0.0470 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0282 - acc: 0.9921 - val_loss: 0.0466 - val_acc: 0.9845\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0280 - acc: 0.9922 - val_loss: 0.0466 - val_acc: 0.9847\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0279 - acc: 0.9921 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0279 - acc: 0.9921 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.000000222324161e-07.\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-08.\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-09.\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-10.\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-11.\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-12.\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 17s 291us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.000000208848829e-13.\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 18s 292us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 18s 293us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 0.0465 - val_acc: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb425aa4390>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=64, epochs=50, callbacks=[\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recurrent network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.reuters.load_data()\n",
    "word_index = keras.datasets.reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30980"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = max(word_index.values())+1\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, None, 32)          991360    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1024)              2232320   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 46)                47150     \n",
      "=================================================================\n",
      "Total params: 3,270,830\n",
      "Trainable params: 3,270,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = X_input = keras.layers.Input((None,), dtype='int32')\n",
    "X = keras.layers.Embedding(embedding_size, 32)(X)\n",
    "X = keras.layers.Bidirectional(keras.layers.LSTM(512, recurrent_activation='softmax'))(X)\n",
    "X = keras.layers.Dense(np.max(Y_train)+1, activation='softmax')(X)\n",
    "M = keras.Model(X_input, X)\n",
    "M.compile('nadam', 'sparse_categorical_crossentropy', ['acc'])\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "250/250 [==============================] - 397s 2s/step - loss: 3.3464 - acc: 0.2192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7cdb35aef0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def datagen():\n",
    "    BUCKET_WINDOW = 16\n",
    "    while True:\n",
    "        shuffled = list(zip(X_train, Y_train))\n",
    "        np.random.shuffle(shuffled)\n",
    "        buckets = {}\n",
    "        for x,y in shuffled:\n",
    "            b = int(np.ceil(len(x)/BUCKET_WINDOW))\n",
    "            if b in buckets:\n",
    "                bucket = buckets[b]\n",
    "            else:\n",
    "                bucket = []\n",
    "                buckets[b] = bucket\n",
    "            bucket.append((x+[0]*(BUCKET_WINDOW*b-len(x)),y))\n",
    "            if len(bucket) >= 64:\n",
    "                batch, bucket = bucket[:64], bucket[64:]\n",
    "                buckets[b] = bucket\n",
    "                yield np.array([row[0] for row in batch]), np.array([row[1] for row in batch])\n",
    "        for batch in buckets.values():\n",
    "            if not batch:\n",
    "                continue\n",
    "            yield np.array([row[0] for row in batch]), np.array([row[1] for row in batch])\n",
    "M.fit_generator(datagen(), steps_per_epoch=250, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
