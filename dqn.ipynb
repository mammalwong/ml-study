{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 63  12   0  40 214   3  14 244  16 244  13 244  15 244 255 160  27  89\n",
      "   0   0  59   0   0   0   8   0   8   0   8   0   8   0  30 109   4  87\n",
      "   0 245   0 247   0 245   0 246   0 255  72   0  18  32  20   2   0   0\n",
      "   0   0   0   0   0   0   0   0   0   4  87   4  21  38   4  87 104 121\n",
      "  87  95 159 191   0  95 159 191   0   0   0   0   0 208   0  43   0   1\n",
      "  62   0   0 136   3   2   0  30   4 128   0 128   0 128   0 167   0 128\n",
      "   0 128   0 254   0 167   0   0   0 120 243 109  86 243 104 120  63 246\n",
      " 238 240]\n",
      "Box(128,)\n",
      "Discrete(18)\n"
     ]
    }
   ],
   "source": [
    "def env_create():\n",
    "    env = gym.make('Boxing-ram-v0')\n",
    "    return env\n",
    "env = env_create()\n",
    "env_max_possible_score = -1\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'state1', 'action', 'reward', 'gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, tau=0.9):\n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__tau = tau\n",
    "        self.__m_train = self.__build_model()\n",
    "        self.__m_target = self.__build_model()\n",
    "        self.__m_train.compile('nadam', 'mse')\n",
    "        self.__m_train.summary()\n",
    "        self.__sync_target(0.)\n",
    "        \n",
    "    def __build_model(self):\n",
    "        \n",
    "        l_state = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False)]\n",
    "        l_action = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu')]\n",
    "        l_value = [\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, l_state)\n",
    "        m_a = m_input_a = keras.layers.Input((self.__output_n,))\n",
    "        m_a = apply_layers(m_a, l_action)\n",
    "        m = keras.layers.Concatenate()([m, m_a])\n",
    "        m = apply_layers(m, l_value)\n",
    "        return keras.models.Model([m_input, m_input_a], [m])\n",
    "    \n",
    "    def __sync_target(self, tau):\n",
    "        w_train = self.__m_train.get_weights()\n",
    "        w_target = self.__m_target.get_weights()\n",
    "        w = [tau*x + (1-tau)*y for x,y in zip(w_target,w_train)]\n",
    "        self.__m_target.set_weights(w)\n",
    "    \n",
    "    def __get_sample_q(self, samples):\n",
    "        q = self.__m_train.predict([\n",
    "            np.repeat(np.array([\n",
    "                s.state1 for s in samples], dtype=np.float32),\n",
    "                self.__output_n, axis=0),\n",
    "            np.tile(np.identity(self.__output_n, dtype=np.float32),\n",
    "                (len(samples), 1))], batch_size=64)\n",
    "        q = np.reshape(q, (len(samples), self.__output_n))\n",
    "        q = np.argmax(q, axis=-1)\n",
    "        q = keras.utils.to_categorical(q, num_classes=self.__output_n)\n",
    "        q = self.__m_target.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),q],\n",
    "            batch_size=64)\n",
    "        q = np.array([s.reward for s in samples])[...,np.newaxis] + \\\n",
    "            q * np.array([s.gamma for s in samples])[...,np.newaxis]\n",
    "        return q\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        a = keras.utils.to_categorical([\n",
    "            s.action for s in samples], num_classes=self.__output_n)\n",
    "        q = self.__get_sample_q(samples)\n",
    "        self.__m_train.fit(\n",
    "            x=[np.array([s.state for s in samples], dtype=np.float32),a],\n",
    "            y=q,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "        self.__sync_target(self.__tau)\n",
    "    \n",
    "    def get_sample_priority(self, samples):\n",
    "        q = self.__get_sample_q(samples)[:,0]\n",
    "        t = self.__m_train.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),\n",
    "            keras.utils.to_categorical([\n",
    "                s.action for s in samples], num_classes=self.__output_n)],\n",
    "            batch_size=64)[:,0]\n",
    "        p = np.abs(q - t)\n",
    "        p = p / np.max(p)\n",
    "        p = p / np.sum(p)\n",
    "        return p\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_train.predict([\n",
    "            np.array([state]*self.__output_n, dtype=np.float32),\n",
    "            np.identity(self.__output_n, dtype=np.float32)])\n",
    "        action_prob = np.argmax(action_prob[:,0])\n",
    "        action_prob = keras.utils.to_categorical([\n",
    "            action_prob], num_classes=self.__output_n)[0]\n",
    "        if verbose:\n",
    "            print(action_prob)\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, max_steps=1000, n_prev_states=8,\n",
    "         reward_steps=1, gamma=0.98, epsilon=0., verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx, get_state):\n",
    "        states = [get_state(e) for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode):\n",
    "        processed_episode = []\n",
    "        for i in range(len(episode)):\n",
    "            episode_reward = 0\n",
    "            episode_gamma = 1.\n",
    "            for j in range(reward_steps):\n",
    "                if i + j >= len(episode):\n",
    "                    break\n",
    "                episode_reward += episode[i+j].reward * episode_gamma\n",
    "                episode_gamma *= episode[i+j].gamma * gamma\n",
    "            processed_episode.append(model_sample(\n",
    "                get_prev_states(episode, i, lambda e: e.state),\n",
    "                get_prev_states(episode, i, lambda e: e.state1),\n",
    "                episode[i].action,\n",
    "                episode_reward,\n",
    "                episode_gamma))\n",
    "        samples.extend(processed_episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob = m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(action_prob)\n",
    "        action_prob = epsilon/action_prob.shape[-1] + (1-epsilon)*action_prob\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state_1, reward, done, _ = env.step(action)\n",
    "        episode.append(model_sample(\n",
    "            state_0, state_1, action, reward, 0. if done else 1.))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 128)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 128)    16512       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 128)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 128)    16512       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 18)           72          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 128)    512         time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           608         batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           128         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          131584      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 160)          0           lstm_1[0][0]                     \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          20608       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           8256        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64)           256         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          activation_5[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 196,649\n",
      "Trainable params: 195,397\n",
      "Non-trainable params: 1,252\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.n, tau=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(m, max_steps=10000, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    for _ in range(max_steps):\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action_prob = m.get_action_prob(state_queue)\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/60)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = env_create()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std[action_prob] 0.16865271\n",
      "max game len 2401\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 11s 375us/step - loss: 0.3669\n",
      "epoch 0 completed\n",
      "std[action_prob] 0.10125823\n",
      "max game len 2391\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 304us/step - loss: 0.1319\n",
      "epoch 1 completed\n",
      "std[action_prob] 0.16699557\n",
      "max game len 2387\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 309us/step - loss: 0.1590\n",
      "epoch 2 completed\n",
      "std[action_prob] 0.17667902\n",
      "max game len 2290\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 313us/step - loss: 0.1995\n",
      "epoch 3 completed\n",
      "std[action_prob] 0.16961138\n",
      "max game len 2379\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 307us/step - loss: 0.1479\n",
      "epoch 4 completed\n",
      "test result mean 5.666666666666667 std 8.32666399786453\n",
      "std[action_prob] 0.18276687\n",
      "max game len 2383\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 306us/step - loss: 0.2262\n",
      "epoch 5 completed\n",
      "std[action_prob] 0.20950584\n",
      "max game len 2396\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 307us/step - loss: 0.1652\n",
      "epoch 6 completed\n",
      "std[action_prob] 0.18701033\n",
      "max game len 2393\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 8s 272us/step - loss: 0.2286\n",
      "epoch 7 completed\n",
      "std[action_prob] 0.21008737\n",
      "max game len 2380\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 309us/step - loss: 0.2062\n",
      "epoch 8 completed\n",
      "std[action_prob] 0.22011615\n",
      "max game len 2354\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 309us/step - loss: 0.1668\n",
      "epoch 9 completed\n",
      "test result mean 4.0 std 5.5677643628300215\n",
      "std[action_prob] 0.19591929\n",
      "max game len 2380\n",
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 9s 308us/step - loss: 0.1984\n",
      "epoch 10 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-31a26bcc9cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     samples = play(\n\u001b[1;32m      6\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prev_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         reward_steps=5, epsilon=0.05, verbose=True)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mreplays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8e7ed8fc9109>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, m, max_steps, n_prev_states, reward_steps, gamma, epsilon, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mstate_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mstate_queue_padded\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mstate_null\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_prev_states\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_queue_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a262df632743>\u001b[0m in \u001b[0;36mget_action_prob\u001b[0;34m(self, state, verbose)\u001b[0m\n\u001b[1;32m    100\u001b[0m         action_prob = self.__m_train.predict([\n\u001b[1;32m    101\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__output_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             np.identity(self.__output_n, dtype=np.float32)])\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         action_prob = keras.utils.to_categorical([\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replays = play(\n",
    "    env, m, max_steps=30000, n_prev_states=4,\n",
    "    reward_steps=5, epsilon=0.05, verbose=False)\n",
    "for i in range(200):\n",
    "    samples = play(\n",
    "        env, m, max_steps=3000, n_prev_states=4,\n",
    "        reward_steps=5, epsilon=0.05, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    if len(replays) > 100000:\n",
    "        replays = list(replays[-100000:])\n",
    "    samples = [model_sample(*s) for s in np.array(replays)[\n",
    "        np.random.choice(np.arange(len(replays), dtype=np.int32),\n",
    "            p=m.get_sample_priority(replays), size=30000)]]\n",
    "    m.train(samples, epochs=1, verbose=True)\n",
    "    print('epoch {} completed'.format(i))\n",
    "    if i % 5 != 4:\n",
    "        continue\n",
    "    test_result = [test(m, max_steps=3000,\n",
    "        n_prev_states=4, render=(i==0)) for i in range(3)]\n",
    "    print('test result',\n",
    "          'mean', np.mean(test_result),\n",
    "          'std', np.std(test_result, ddof=1))\n",
    "    if np.mean(test_result) == env_max_possible_score and \\\n",
    "        np.std(test_result, ddof=1) == 0:\n",
    "        print('the network always gets full score, early exit')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(m, n_prev_states=4, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
