{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 63  12   0  40 214   3  14 244  16 244  13 244  15 244 255 160  27  89\n",
      "   0   0  59   0   0   0   8   0   8   0   8   0   8   0  30 109   4  87\n",
      "   0 245   0 247   0 245   0 246   0 255  72   0  18  32  20   2   0   0\n",
      "   0   0   0   0   0   0   0   0   0   4  87   4  21  38   4  87 104 121\n",
      "  87  95 159 191   0  95 159 191   0   0   0   0   0 208   0  43   0   1\n",
      "  62   0   0 136   3   2   0  30   4 128   0 128   0 128   0 167   0 128\n",
      "   0 128   0 254   0 167   0   0   0 120 243 109  86 243 104 120  63 246\n",
      " 238 240]\n",
      "Box(128,)\n",
      "Discrete(18)\n"
     ]
    }
   ],
   "source": [
    "def env_create():\n",
    "    env = gym.make('Boxing-ram-v0')\n",
    "    return env\n",
    "env = env_create()\n",
    "env_max_possible_score = -1\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'state1', 'action', 'reward', 'gamma', 'priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, tau=0.9):\n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__tau = tau\n",
    "        self.__m_train = self.__build_model()\n",
    "        self.__m_target = self.__build_model()\n",
    "        self.__m_train.compile('nadam', 'mse')\n",
    "        self.__m_train.summary()\n",
    "        self.__sync_target(0.)\n",
    "        \n",
    "    def __build_model(self):\n",
    "        \n",
    "        l_state = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False)]\n",
    "        l_action = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu')]\n",
    "        l_value = [\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, l_state)\n",
    "        m_a = m_input_a = keras.layers.Input((self.__output_n,))\n",
    "        m_a = apply_layers(m_a, l_action)\n",
    "        m = keras.layers.Concatenate()([m, m_a])\n",
    "        m = apply_layers(m, l_value)\n",
    "        return keras.models.Model([m_input, m_input_a], [m])\n",
    "    \n",
    "    def __sync_target(self, tau):\n",
    "        w_train = self.__m_train.get_weights()\n",
    "        w_target = self.__m_target.get_weights()\n",
    "        w = [tau*x + (1-tau)*y for x,y in zip(w_target,w_train)]\n",
    "        self.__m_target.set_weights(w)\n",
    "    \n",
    "    def __get_sample_q(self, samples):\n",
    "        q = self.__m_train.predict([\n",
    "            np.repeat(np.array([\n",
    "                s.state1 for s in samples], dtype=np.float32),\n",
    "                self.__output_n, axis=0),\n",
    "            np.tile(np.identity(self.__output_n, dtype=np.float32),\n",
    "                (len(samples), 1))], batch_size=64)\n",
    "        q = np.reshape(q, (len(samples), self.__output_n))\n",
    "        q = np.argmax(q, axis=-1)\n",
    "        q = keras.utils.to_categorical(q, num_classes=self.__output_n)\n",
    "        q = self.__m_target.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),q],\n",
    "            batch_size=64)\n",
    "        q = np.array([s.reward for s in samples])[...,np.newaxis] + \\\n",
    "            q * np.array([s.gamma for s in samples])[...,np.newaxis]\n",
    "        return q\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        a = keras.utils.to_categorical([\n",
    "            s.action for s in samples], num_classes=self.__output_n)\n",
    "        q = self.__get_sample_q(samples)\n",
    "        self.__m_train.fit(\n",
    "            x=[np.array([s.state for s in samples], dtype=np.float32),a],\n",
    "            y=q,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "        self.__sync_target(self.__tau)\n",
    "    \n",
    "    def get_sample_priority(self, samples):\n",
    "        q = self.__get_sample_q(samples)[:,0]\n",
    "        t = self.__m_train.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),\n",
    "            keras.utils.to_categorical([\n",
    "                s.action for s in samples], num_classes=self.__output_n)],\n",
    "            batch_size=64)[:,0]\n",
    "        p = np.abs(q - t)\n",
    "        return p\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_train.predict([\n",
    "            np.array([state]*self.__output_n, dtype=np.float32),\n",
    "            np.identity(self.__output_n, dtype=np.float32)])\n",
    "        action_prob = np.argmax(action_prob[:,0])\n",
    "        action_prob = keras.utils.to_categorical([\n",
    "            action_prob], num_classes=self.__output_n)[0]\n",
    "        if verbose:\n",
    "            print(action_prob)\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, max_steps=1000, n_prev_states=8,\n",
    "         reward_steps=1, gamma=0.98, epsilon=0., verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx, get_state):\n",
    "        states = [get_state(e) for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode):\n",
    "        processed_episode = []\n",
    "        for i in range(len(episode)):\n",
    "            episode_reward = 0\n",
    "            episode_gamma = 1.\n",
    "            for j in range(reward_steps):\n",
    "                if i + j >= len(episode):\n",
    "                    break\n",
    "                episode_reward += episode[i+j].reward * episode_gamma\n",
    "                episode_gamma *= episode[i+j].gamma * gamma\n",
    "            processed_episode.append(model_sample(\n",
    "                get_prev_states(episode, i, lambda e: e.state),\n",
    "                get_prev_states(episode, i, lambda e: e.state1),\n",
    "                episode[i].action,\n",
    "                episode_reward,\n",
    "                episode_gamma, 0.))\n",
    "        priority = m.get_sample_priority(processed_episode)\n",
    "        for i in range(len(processed_episode)):\n",
    "            processed_episode[i] = processed_episode[i]._replace(priority=priority[i])\n",
    "        samples.extend(processed_episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob = m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(action_prob)\n",
    "        action_prob = epsilon/action_prob.shape[-1] + (1-epsilon)*action_prob\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state_1, reward, done, _ = env.step(action)\n",
    "        episode.append(model_sample(\n",
    "            state_0, state_1, action, reward, 0. if done else 1., 0.))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 128)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 128)    16512       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 128)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 128)    16512       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 18)           72          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 128)    512         time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           608         batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           128         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          131584      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 160)          0           lstm_1[0][0]                     \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          20608       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           8256        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64)           256         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          activation_5[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 196,649\n",
      "Trainable params: 195,397\n",
      "Non-trainable params: 1,252\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.n, tau=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(m, max_steps=10000, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    for _ in range(max_steps):\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action_prob = m.get_action_prob(state_queue)\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/60)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = env_create()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std[action_prob] 0.15046412\n",
      "max game len 2365\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 292us/step - loss: 0.3424\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 296us/step - loss: 0.2917\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 275us/step - loss: 0.2734\n",
      "epoch 0 completed\n",
      "std[action_prob] 0.12234422\n",
      "max game len 2377\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 306us/step - loss: 0.2690\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.3118\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 307us/step - loss: 0.3099\n",
      "epoch 1 completed\n",
      "std[action_prob] 0.15077625\n",
      "max game len 2362\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 273us/step - loss: 0.2746\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 267us/step - loss: 0.1839\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.1942\n",
      "epoch 2 completed\n",
      "std[action_prob] 0.18777987\n",
      "max game len 2375\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 380us/step - loss: 0.1991\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 294us/step - loss: 0.1875\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 266us/step - loss: 0.1837\n",
      "epoch 3 completed\n",
      "std[action_prob] 0.17584518\n",
      "max game len 2387\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 274us/step - loss: 0.2379\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.1975\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 344us/step - loss: 0.2611\n",
      "epoch 4 completed\n",
      "test result mean 4.333333333333333 std 10.214368964029708\n",
      "std[action_prob] 0.15709653\n",
      "max game len 2349\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 267us/step - loss: 0.2049\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 271us/step - loss: 0.3513\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.3244\n",
      "epoch 5 completed\n",
      "std[action_prob] 0.19755958\n",
      "max game len 2398\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 266us/step - loss: 0.3343\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 275us/step - loss: 0.2343\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 277us/step - loss: 0.3520\n",
      "epoch 6 completed\n",
      "std[action_prob] 0.17191829\n",
      "max game len 2369\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 272us/step - loss: 0.4771\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 284us/step - loss: 0.4381\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 307us/step - loss: 0.3857\n",
      "epoch 7 completed\n",
      "std[action_prob] 0.16469988\n",
      "max game len 2359\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 290us/step - loss: 0.3832\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 267us/step - loss: 0.3893\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 264us/step - loss: 0.6446\n",
      "epoch 8 completed\n",
      "std[action_prob] 0.19154175\n",
      "max game len 2381\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 276us/step - loss: 0.3197\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 277us/step - loss: 0.3149\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 263us/step - loss: 0.8566\n",
      "epoch 9 completed\n",
      "test result mean 22.0 std 18.520259177452136\n",
      "std[action_prob] 0.1921301\n",
      "max game len 2365\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 305us/step - loss: 0.7152\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 346us/step - loss: 1.3926\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 269us/step - loss: 0.4371\n",
      "epoch 10 completed\n",
      "std[action_prob] 0.18040898\n",
      "max game len 2369\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 267us/step - loss: 0.3175\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 269us/step - loss: 0.5775\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.3918\n",
      "epoch 11 completed\n",
      "std[action_prob] 0.17513904\n",
      "max game len 2414\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 294us/step - loss: 0.8238\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 326us/step - loss: 1.0639\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.9340\n",
      "epoch 12 completed\n",
      "std[action_prob] 0.17724556\n",
      "max game len 2377\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 262us/step - loss: 1.0247\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 265us/step - loss: 0.5322\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 270us/step - loss: 0.5792\n",
      "epoch 13 completed\n",
      "std[action_prob] 0.1919979\n",
      "max game len 2385\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 269us/step - loss: 0.9218\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 358us/step - loss: 1.1644\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 268us/step - loss: 1.0518\n",
      "epoch 14 completed\n",
      "test result mean -6.666666666666667 std 4.618802153517007\n",
      "std[action_prob] 0.19531761\n",
      "max game len 2395\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 269us/step - loss: 0.6520\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 331us/step - loss: 0.5857\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 264us/step - loss: 0.8942\n",
      "epoch 15 completed\n",
      "std[action_prob] 0.13794869\n",
      "max game len 2381\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 280us/step - loss: 0.7310\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 268us/step - loss: 0.7914\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 266us/step - loss: 0.6976\n",
      "epoch 16 completed\n",
      "std[action_prob] 0.20446435\n",
      "max game len 2381\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 348us/step - loss: 0.8204\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 268us/step - loss: 1.0034\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 276us/step - loss: 0.7044\n",
      "epoch 17 completed\n",
      "std[action_prob] 0.1741314\n",
      "max game len 2371\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 313us/step - loss: 0.8421\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 279us/step - loss: 0.8870\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 277us/step - loss: 0.6600\n",
      "epoch 18 completed\n",
      "std[action_prob] 0.20183426\n",
      "max game len 2397\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 271us/step - loss: 0.7959\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 285us/step - loss: 0.6047\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 267us/step - loss: 1.1637\n",
      "epoch 19 completed\n",
      "test result mean 2.0 std 2.0\n",
      "std[action_prob] 0.13126911\n",
      "max game len 2385\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 266us/step - loss: 0.7880\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 271us/step - loss: 1.1121\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 263us/step - loss: 0.8057\n",
      "epoch 20 completed\n",
      "std[action_prob] 0.19713034\n",
      "max game len 2376\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 263us/step - loss: 0.8708\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 271us/step - loss: 1.0224\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 268us/step - loss: 1.0998\n",
      "epoch 21 completed\n",
      "std[action_prob] 0.17732379\n",
      "max game len 2380\n",
      "Epoch 1/1\n",
      "128/128 [==============================] - 0s 423us/step - loss: 0.7767\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b92cb30a3302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         sample_indices = np.random.choice(\n\u001b[1;32m     16\u001b[0m             np.arange(len(replays), dtype=np.int32), p=sample_p, size=128)\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0msample_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sample_priority\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replays = play(\n",
    "    env, m, max_steps=30000, n_prev_states=4,\n",
    "    reward_steps=5, epsilon=0.05, verbose=False)\n",
    "for i in range(200):\n",
    "    samples = play(\n",
    "        env, m, max_steps=3000, n_prev_states=4,\n",
    "        reward_steps=5, epsilon=0.05, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    if len(replays) > 100000:\n",
    "        replays = list(replays[-100000:])\n",
    "    for j in range(300):\n",
    "        sample_p = np.array([r.priority for r in replays])\n",
    "        sample_p = np.power(sample_p, 0.5)\n",
    "        sample_p = sample_p / np.sum(sample_p)\n",
    "        sample_indices = np.random.choice(\n",
    "            np.arange(len(replays), dtype=np.int32), p=sample_p, size=128)\n",
    "        samples = [model_sample(*s) \\\n",
    "            for s in np.array(replays)[sample_indices]]\n",
    "        sample_p = m.get_sample_priority(samples)\n",
    "        for s,p in zip(sample_indices, sample_p):\n",
    "            replays[s] = replays[s]._replace(priority=p)\n",
    "        m.train(samples, epochs=1, verbose=(j+1)%100==0)\n",
    "    print('epoch {} completed'.format(i))\n",
    "    if i % 5 != 4:\n",
    "        continue\n",
    "    test_result = [test(m, max_steps=3000,\n",
    "        n_prev_states=4, render=(i==0)) for i in range(3)]\n",
    "    print('test result',\n",
    "          'mean', np.mean(test_result),\n",
    "          'std', np.std(test_result, ddof=1))\n",
    "    if np.mean(test_result) == env_max_possible_score and \\\n",
    "        np.std(test_result, ddof=1) == 0:\n",
    "        print('the network always gets full score, early exit')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(m, n_prev_states=4, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
