{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.virtualenvs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NALU(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        super(NALU, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.weight_sigmoid = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform', name='weight_sigmoid')\n",
    "        self.weight_tanh = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform', name='weight_tanh')\n",
    "        self.weight_gate = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform', name='weight_gate')\n",
    "        super(NALU, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        w = K.sigmoid(self.weight_sigmoid) * K.tanh(self.weight_tanh)\n",
    "        adds = inputs @ w\n",
    "        muls = K.exp((K.log(K.abs(inputs) + K.epsilon())) @ w)\n",
    "        gates = K.sigmoid(inputs @ self.weight_gate)\n",
    "        return gates * adds + (1-gates) * muls\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "nalu_14 (NALU)               (None, 8)                 48        \n",
      "_________________________________________________________________\n",
      "nalu_15 (NALU)               (None, 1)                 24        \n",
      "=================================================================\n",
      "Total params: 72\n",
      "Trainable params: 72\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = X_input = keras.layers.Input((2,))\n",
    "X = NALU(8)(X)\n",
    "X = NALU(1)(X)\n",
    "M = keras.Model(X_input, X)\n",
    "M.compile('nadam', 'mse')\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data(a, b):\n",
    "    return (2*a) / b\n",
    "data_input = np.random.random(size=(10000,2))*100 + 1\n",
    "data_label = toy_data(*np.swapaxes(data_input,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "10000/10000 [==============================] - 2s 226us/step - loss: 169.1597\n",
      "Epoch 2/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 134.5190\n",
      "Epoch 3/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 136.7152\n",
      "Epoch 4/1000\n",
      "10000/10000 [==============================] - 2s 203us/step - loss: 127.2089\n",
      "Epoch 5/1000\n",
      "10000/10000 [==============================] - 2s 203us/step - loss: 117.5212\n",
      "Epoch 6/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 102.5434\n",
      "Epoch 7/1000\n",
      "10000/10000 [==============================] - 2s 204us/step - loss: 86.7570\n",
      "Epoch 8/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 72.1196\n",
      "Epoch 9/1000\n",
      "10000/10000 [==============================] - 2s 200us/step - loss: 59.0948\n",
      "Epoch 10/1000\n",
      "10000/10000 [==============================] - 2s 203us/step - loss: 47.9800\n",
      "Epoch 11/1000\n",
      "10000/10000 [==============================] - 2s 205us/step - loss: 39.0884\n",
      "Epoch 12/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 32.5467\n",
      "Epoch 13/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 26.9122\n",
      "Epoch 14/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 21.8636\n",
      "Epoch 15/1000\n",
      "10000/10000 [==============================] - 2s 200us/step - loss: 18.0925\n",
      "Epoch 16/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 13.9055\n",
      "Epoch 17/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 11.3406\n",
      "Epoch 18/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 9.0756\n",
      "Epoch 19/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 7.2277\n",
      "Epoch 20/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 5.9334\n",
      "Epoch 21/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 5.1271\n",
      "Epoch 22/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 3.9949\n",
      "Epoch 23/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 3.4011\n",
      "Epoch 24/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 2.4145\n",
      "Epoch 25/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 2.0778\n",
      "Epoch 26/1000\n",
      "10000/10000 [==============================] - 2s 200us/step - loss: 1.5252\n",
      "Epoch 27/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 1.1261\n",
      "Epoch 28/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 0.8008\n",
      "Epoch 29/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 0.7342\n",
      "Epoch 30/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.4537\n",
      "Epoch 31/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.3038\n",
      "Epoch 32/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.3147\n",
      "Epoch 33/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.1869\n",
      "Epoch 34/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.1225\n",
      "Epoch 35/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.1278\n",
      "Epoch 36/1000\n",
      "10000/10000 [==============================] - 2s 193us/step - loss: 0.1452\n",
      "Epoch 37/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0994\n",
      "Epoch 38/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 0.0783\n",
      "Epoch 39/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.2395\n",
      "Epoch 40/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 0.0831\n",
      "Epoch 41/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0722\n",
      "Epoch 42/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0791\n",
      "Epoch 43/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 0.0403\n",
      "Epoch 44/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.0509\n",
      "Epoch 45/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.0366\n",
      "Epoch 46/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.0581\n",
      "Epoch 47/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0328\n",
      "Epoch 48/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0500\n",
      "Epoch 49/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.0275\n",
      "Epoch 50/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0470\n",
      "Epoch 51/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0269\n",
      "Epoch 52/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0256\n",
      "Epoch 53/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.0207\n",
      "Epoch 54/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0250\n",
      "Epoch 55/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0727\n",
      "Epoch 56/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0202\n",
      "Epoch 57/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0161\n",
      "Epoch 58/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0437\n",
      "Epoch 59/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0244\n",
      "Epoch 60/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0184\n",
      "Epoch 61/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0211\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 62/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0094\n",
      "Epoch 63/1000\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 0.0091\n",
      "Epoch 64/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0086\n",
      "Epoch 65/1000\n",
      "10000/10000 [==============================] - 2s 193us/step - loss: 0.0084\n",
      "Epoch 66/1000\n",
      "10000/10000 [==============================] - 2s 192us/step - loss: 0.0086\n",
      "Epoch 67/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.0087\n",
      "Epoch 68/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.0087\n",
      "Epoch 69/1000\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 0.0084\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 70/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.0076\n",
      "Epoch 71/1000\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 0.0074\n",
      "Epoch 72/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0073\n",
      "Epoch 73/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.0074\n",
      "Epoch 74/1000\n",
      "10000/10000 [==============================] - 2s 193us/step - loss: 0.0074\n",
      "Epoch 75/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0073\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "Epoch 76/1000\n",
      "10000/10000 [==============================] - 2s 191us/step - loss: 0.0073\n",
      "Epoch 77/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0072\n",
      "Epoch 78/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0072\n",
      "Epoch 79/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0072\n",
      "Epoch 80/1000\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 2.000000222324161e-07.\n",
      "Epoch 81/1000\n",
      "10000/10000 [==============================] - 2s 193us/step - loss: 0.0072\n",
      "Epoch 82/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0072\n",
      "Epoch 83/1000\n",
      "10000/10000 [==============================] - 2s 193us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-08.\n",
      "Epoch 84/1000\n",
      "10000/10000 [==============================] - 2s 195us/step - loss: 0.0072\n",
      "Epoch 85/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0072\n",
      "Epoch 86/1000\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-09.\n",
      "Epoch 87/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 207us/step - loss: 0.0072\n",
      "Epoch 88/1000\n",
      "10000/10000 [==============================] - 2s 211us/step - loss: 0.0072\n",
      "Epoch 89/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-10.\n",
      "Epoch 90/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.0072\n",
      "Epoch 91/1000\n",
      "10000/10000 [==============================] - 2s 205us/step - loss: 0.0072\n",
      "Epoch 92/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-11.\n",
      "Epoch 93/1000\n",
      "10000/10000 [==============================] - 2s 206us/step - loss: 0.0072\n",
      "Epoch 94/1000\n",
      "10000/10000 [==============================] - 2s 203us/step - loss: 0.0072\n",
      "Epoch 95/1000\n",
      "10000/10000 [==============================] - 2s 203us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-12.\n",
      "Epoch 96/1000\n",
      "10000/10000 [==============================] - 2s 205us/step - loss: 0.0072\n",
      "Epoch 97/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.0072\n",
      "Epoch 98/1000\n",
      "10000/10000 [==============================] - 2s 202us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 2.000000208848829e-13.\n",
      "Epoch 99/1000\n",
      "10000/10000 [==============================] - 2s 200us/step - loss: 0.0072\n",
      "Epoch 100/1000\n",
      "10000/10000 [==============================] - 2s 201us/step - loss: 0.0072\n",
      "Epoch 101/1000\n",
      "10000/10000 [==============================] - 2s 200us/step - loss: 0.0072\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 2.0000002359538835e-14.\n",
      "Epoch 00101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4c43047710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.fit(data_input, data_label, batch_size=16, epochs=1000, callbacks=[\n",
    "    keras.callbacks.ReduceLROnPlateau('loss', patience=3, verbose=1),\n",
    "    keras.callbacks.EarlyStopping('loss', patience=10, verbose=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666667\n",
      "[[66.33641]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[100, 3]])\n",
    "print(toy_data(*test[0]))\n",
    "print(M.predict(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
