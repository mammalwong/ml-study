{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/marco/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate wordnet relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for s in nltk.corpus.wordnet.all_synsets():\n",
    "        for v in s.also_sees():\n",
    "            writer.writerow((f's.{s.name()}', 's.also_sees', f's.{v.name()}'))\n",
    "        for v in s.attributes():\n",
    "            writer.writerow((f's.{s.name()}', 's.attributes', f's.{v.name()}'))\n",
    "        for v in s.causes():\n",
    "            writer.writerow((f's.{s.name()}', 's.causes', f's.{v.name()}'))\n",
    "        for v in s.entailments():\n",
    "            writer.writerow((f's.{s.name()}', 's.entailments', f's.{v.name()}'))\n",
    "        for v in s.hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hypernyms', f's.{v.name()}'))\n",
    "        for v in s.hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hyponyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hyponyms', f's.{v.name()}'))\n",
    "        for v in s.lemmas():\n",
    "            writer.writerow((f's.{s.name()}', 's.lemmas', f'l.{v.name()}'))\n",
    "        for v in s.member_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_holonyms', f's.{v.name()}'))\n",
    "        for v in s.member_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_meronyms', f's.{v.name()}'))\n",
    "        for v in s.part_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_holonyms', f's.{v.name()}'))\n",
    "        writer.writerow((f's.{s.name()}', 's.pos', f'p.{s.pos()}'))\n",
    "        for v in s.region_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.region_domains', f's.{v.name()}'))\n",
    "        for v in s.root_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.root_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.similar_tos():\n",
    "            writer.writerow((f's.{s.name()}', 's.similar_tos', f's.{v.name()}'))\n",
    "        for v in s.substance_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_holonyms', f's.{v.name()}'))\n",
    "        for v in s.substance_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_meronyms', f's.{v.name()}'))\n",
    "        for v in s.topic_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.topic_domains', f's.{v.name()}'))\n",
    "        for v in s.usage_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.usage_domains', f's.{v.name()}'))\n",
    "        for v in s.verb_groups():\n",
    "            writer.writerow((f's.{s.name()}', 's.verb_groups', f's.{v.name()}'))\n",
    "        for v in s.frame_ids():\n",
    "            writer.writerow((f's.{s.name()}', 's.frame_ids', f'f.{v}'))\n",
    "    seen_lemma_keys = set()\n",
    "    for l_name in nltk.corpus.wordnet.all_lemma_names():\n",
    "        ls = nltk.corpus.wordnet.lemmas(l_name)\n",
    "        for l in ls:\n",
    "            if l.key() in seen_lemma_keys:\n",
    "                continue\n",
    "            seen_lemma_keys.add(l.key())\n",
    "            for v in l.also_sees():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.also_sees', f'l.{v.key()}'))\n",
    "            for v in l.antonyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.antonyms', f'l.{v.key()}'))\n",
    "            for v in l.derivationally_related_forms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.derivationally_related_forms', f'l.{v.key()}'))\n",
    "            for v in l.frame_ids():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.frame_ids', f'f.{v}'))\n",
    "            for v in l.pertainyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.pertainyms', f'l.{v.key()}'))\n",
    "            for v in l.region_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.region_domains', f'l.{v.key()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.synset', f's.{l.synset().name()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.syntactic_marker', f'sm.{l.syntactic_marker()}'))\n",
    "            for v in l.topic_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.topic_domains', f'l.{v.key()}'))\n",
    "            for v in l.usage_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.usage_domains', f'l.{v.key()}'))\n",
    "            for v in l.verb_groups():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.verb_groups', f'l.{v.key()}'))\n",
    "    del seen_lemma_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473382, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set = set()\n",
    "relation_set = set()\n",
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entity_set.add(row[0])\n",
    "        entity_set.add(row[2])\n",
    "        relation_set.add(row[1])\n",
    "entity_set = list(sorted(entity_set))\n",
    "relation_set = list(sorted(relation_set))\n",
    "entity_lookup = { k+1:v for (k,v) in enumerate(entity_set)}\n",
    "entity_index = { v:k+1 for (k,v) in enumerate(entity_set)}\n",
    "entity_max = len(entity_set)\n",
    "relation_lookup = { k+1:v for (k,v) in enumerate(relation_set)}\n",
    "relation_index = { v:k+1 for (k,v) in enumerate(relation_set)}\n",
    "relation_max = len(relation_set)\n",
    "del entity_set\n",
    "del relation_set\n",
    "len(entity_lookup), len(relation_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1278206, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dataset.append((entity_index[row[0]], relation_index[row[1]], entity_index[row[2]]))\n",
    "dataset = np.array(dataset, dtype='int')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern = {}\n",
    "for h,r,t in dataset:\n",
    "    r_stat = bern.get(r)\n",
    "    if r_stat is None:\n",
    "        r_stat = (collections.Counter(), collections.Counter())\n",
    "        bern[r] = r_stat\n",
    "    h_stat, t_stat = r_stat\n",
    "    h_stat[h] += 1\n",
    "    t_stat[t] += 1\n",
    "bern = {r:(\n",
    "    np.mean(list(r_stat[0].values())),\n",
    "    np.mean(list(r_stat[1].values()))) for r,r_stat in bern.items()}\n",
    "bern = {r:r_stat[0]/(r_stat[0]+r_stat[1]) for r,r_stat in bern.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransF(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TransF, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        entity_shape, relation_shape = input_shape\n",
    "        self.kernel_width = entity_shape[1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(relation_shape[1], entity_shape[1], entity_shape[1]),\n",
    "            initializer='zeros', name='kernel')\n",
    "        super(TransF, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        entity, relation = inputs\n",
    "        relation = K.reshape(relation, K.concatenate([K.shape(relation), (1,1)]))\n",
    "        relation = relation * self.kernel\n",
    "        relation = K.sum(relation, axis=1, keepdims=False)\n",
    "        relation = relation + K.eye(self.kernel_width)\n",
    "        return (K.expand_dims(entity, axis=1) @ relation)[:,0]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        entity_shape, _ = input_shape\n",
    "        return entity_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 32)        15148256    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 8)         272         input_9[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 32)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 8)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 32)        1088        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_5 (TransF)              (None, 32)           8192        flatten_11[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 32)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 32)           0           embedding_9[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 8)            0           embedding_11[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32)           0           trans_f_5[0][0]                  \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_6 (TransF)              (None, 32)           8192        flatten_12[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 32)           0           add_3[0][0]                      \n",
      "                                                                 trans_f_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           subtract_3[0][0]                 \n",
      "                                                                 subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 15,166,000\n",
      "Trainable params: 15,166,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 1)            15166000    input_10[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 1)            0           model_3[1][0]                    \n",
      "                                                                 model_3[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,166,000\n",
      "Trainable params: 15,166,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_h = X_input_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_t = X_input_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_r = X_input_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_e_embedding = keras.layers.Embedding(entity_max+1, 50,\n",
    "    embeddings_constraint=keras.constraints.unit_norm(axis=-1))\n",
    "X_r_embedding = keras.layers.Embedding(relation_max+1, 50,\n",
    "    embeddings_constraint=keras.constraints.max_norm(max_value=1., axis=-1))\n",
    "X_hr_embedding = keras.layers.Embedding(relation_max+1, 6)\n",
    "X_tr_embedding = keras.layers.Embedding(relation_max+1, 6)\n",
    "X_hr_trans_f = TransF()\n",
    "X_tr_trans_f = TransF()\n",
    "X_h = X_e_embedding(X_h)\n",
    "X_t = X_e_embedding(X_t)\n",
    "X_hr = X_hr_embedding(X_r)\n",
    "X_tr = X_hr_embedding(X_r)\n",
    "X_r = X_r_embedding(X_r)\n",
    "X_h = keras.layers.Flatten()(X_h)\n",
    "X_t = keras.layers.Flatten()(X_t)\n",
    "X_hr = keras.layers.Flatten()(X_hr)\n",
    "X_tr = keras.layers.Flatten()(X_tr)\n",
    "X_r = keras.layers.Flatten()(X_r)\n",
    "X_h = X_hr_trans_f([X_h, X_hr])\n",
    "X_t = X_tr_trans_f([X_t, X_tr])\n",
    "X = keras.layers.Add()([X_h, X_r])\n",
    "X = keras.layers.Subtract()([X, X_t])\n",
    "X = keras.layers.Dot(-1)([X, X])\n",
    "X = keras.layers.Lambda(lambda x: K.sqrt(x))(X)\n",
    "M_sample = keras.Model([X_input_h, X_input_r, X_input_t], X)\n",
    "M_sample.compile('adam', 'mse')\n",
    "M_sample.summary()\n",
    "X_p_h = X_input_p_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_t = X_input_p_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_r = X_input_p_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_h = X_input_n_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_t = X_input_n_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_r = X_input_n_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_p = M_sample([X_p_h, X_p_r, X_p_t])\n",
    "X_n = M_sample([X_n_h, X_n_r, X_n_t])\n",
    "X = keras.layers.Subtract()([X_p, X_n])\n",
    "M = keras.Model([X_p_h, X_p_r, X_p_t, X_n_h, X_n_r, X_n_t], X)\n",
    "def TransF_loss(t, y):\n",
    "    return K.sum(K.clip(y + 3, 0, np.inf) + 0*t)\n",
    "M.compile('adam', TransF_loss)\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling():\n",
    "    negative_type = np.array([bern[r] for r in dataset[:,1]])\n",
    "    negative_type = np.random.rand(dataset.shape[0]) < negative_type\n",
    "    d0 = np.where(negative_type, np.random.randint(\n",
    "        0, entity_max+1, size=dataset.shape[:1]), dataset[:,0])\n",
    "    d1 = np.where(~negative_type, np.random.randint(\n",
    "        0, entity_max+1, size=dataset.shape[:1]), dataset[:,2])\n",
    "    return np.stack([d0, dataset[:,1], d1], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 640.2434\n",
      "epoch 1\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 605.6389\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 568.7242\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 543.8918\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 518.4492\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 498.2861\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 481.3704\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      " 323072/1278206 [======>.......................] - ETA: 7:26 - loss: 459.9190"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-84d6c4528506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         nsamples[:,0],nsamples[:,1],nsamples[:,2]],\n\u001b[0;32m----> 7\u001b[0;31m         np.zeros((dataset.shape[0],1)), batch_size=512, epochs=1)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    nsamples = negative_sampling()\n",
    "    M.fit([\n",
    "        dataset[:,0],dataset[:,1],dataset[:,2],\n",
    "        nsamples[:,0],nsamples[:,1],nsamples[:,2]],\n",
    "        np.zeros((dataset.shape[0],1)), batch_size=512, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.models.save_model(M, './models/knowledge-graphj-embedding-analogic/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('drink.v.01', [Synset('consume.v.02')])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_synset = nltk.corpus.wordnet.lemmas('drink', pos='v')[0].synset()\n",
    "test_synset.name(), test_synset.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s.declare.v.01',\n",
       " 's.evaluate.v.02',\n",
       " 's.praise.v.01',\n",
       " 's.mark.v.05',\n",
       " 's.communication.n.02',\n",
       " 's.displace.v.01',\n",
       " 's.rise.v.01',\n",
       " 's.propel.v.01',\n",
       " 's.reduce.v.01',\n",
       " 's.worsen.v.01',\n",
       " 's.send.v.01',\n",
       " 's.plan.v.02',\n",
       " 's.unpack.v.01',\n",
       " 's.radiate.v.05',\n",
       " 's.try.v.01',\n",
       " 's.draw.v.07',\n",
       " 's.forgive.v.01',\n",
       " 's.state.v.01',\n",
       " 's.re-create.v.01',\n",
       " 's.reception_room.n.01',\n",
       " 's.separate.v.12',\n",
       " 's.end.v.02',\n",
       " 's.weaken.v.01',\n",
       " 's.decrease.v.02',\n",
       " 's.explain.v.01',\n",
       " 's.pass.v.05',\n",
       " 's.grow.v.02',\n",
       " 's.convert.v.02',\n",
       " 's.play.v.01',\n",
       " 's.hash_out.v.01']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = M_sample.predict([\n",
    "    np.repeat(np.array([[entity_index['s.drink.v.01']]]), entity_max+1, axis=0),\n",
    "    np.repeat(np.array([[relation_index['s.hypernyms']]]), entity_max+1, axis=0),\n",
    "    np.arange(0, entity_max+1, 1, dtype='int32')[:, np.newaxis]], batch_size=512)\n",
    "order = np.argsort(result, axis=0)\n",
    "[entity_lookup[i] for i in order[:30,0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
