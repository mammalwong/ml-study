{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/marco/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate wordnet relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for s in nltk.corpus.wordnet.all_synsets():\n",
    "        for v in s.also_sees():\n",
    "            writer.writerow((f's.{s.name()}', 's.also_sees', f's.{v.name()}'))\n",
    "        for v in s.attributes():\n",
    "            writer.writerow((f's.{s.name()}', 's.attributes', f's.{v.name()}'))\n",
    "        for v in s.causes():\n",
    "            writer.writerow((f's.{s.name()}', 's.causes', f's.{v.name()}'))\n",
    "        for v in s.entailments():\n",
    "            writer.writerow((f's.{s.name()}', 's.entailments', f's.{v.name()}'))\n",
    "        for v in s.hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hypernyms', f's.{v.name()}'))\n",
    "        for v in s.hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hyponyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hyponyms', f's.{v.name()}'))\n",
    "        for v in s.lemmas():\n",
    "            writer.writerow((f's.{s.name()}', 's.lemmas', f'l.{v.name()}'))\n",
    "        for v in s.member_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_holonyms', f's.{v.name()}'))\n",
    "        for v in s.member_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_meronyms', f's.{v.name()}'))\n",
    "        for v in s.part_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_holonyms', f's.{v.name()}'))\n",
    "        for v in s.part_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_meronyms', f's.{v.name()}'))\n",
    "        writer.writerow((f's.{s.name()}', 's.pos', f'p.{s.pos()}'))\n",
    "        for v in s.region_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.region_domains', f's.{v.name()}'))\n",
    "        for v in s.root_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.root_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.similar_tos():\n",
    "            writer.writerow((f's.{s.name()}', 's.similar_tos', f's.{v.name()}'))\n",
    "        for v in s.substance_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_holonyms', f's.{v.name()}'))\n",
    "        for v in s.substance_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_meronyms', f's.{v.name()}'))\n",
    "        for v in s.topic_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.topic_domains', f's.{v.name()}'))\n",
    "        for v in s.usage_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.usage_domains', f's.{v.name()}'))\n",
    "        for v in s.verb_groups():\n",
    "            writer.writerow((f's.{s.name()}', 's.verb_groups', f's.{v.name()}'))\n",
    "        for v in s.frame_ids():\n",
    "            writer.writerow((f's.{s.name()}', 's.frame_ids', f'f.{v}'))\n",
    "    seen_lemma_keys = set()\n",
    "    for l_name in nltk.corpus.wordnet.all_lemma_names():\n",
    "        ls = nltk.corpus.wordnet.lemmas(l_name)\n",
    "        for l in ls:\n",
    "            if l.key() in seen_lemma_keys:\n",
    "                continue\n",
    "            seen_lemma_keys.add(l.key())\n",
    "            for v in l.also_sees():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.also_sees', f'l.{v.key()}'))\n",
    "            for v in l.antonyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.antonyms', f'l.{v.key()}'))\n",
    "            for v in l.derivationally_related_forms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.derivationally_related_forms', f'l.{v.key()}'))\n",
    "            for v in l.frame_ids():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.frame_ids', f'f.{v}'))\n",
    "            for v in l.pertainyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.pertainyms', f'l.{v.key()}'))\n",
    "            for v in l.region_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.region_domains', f'l.{v.key()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.synset', f's.{l.synset().name()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.syntactic_marker', f'sm.{l.syntactic_marker()}'))\n",
    "            for v in l.topic_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.topic_domains', f'l.{v.key()}'))\n",
    "            for v in l.usage_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.usage_domains', f'l.{v.key()}'))\n",
    "            for v in l.verb_groups():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.verb_groups', f'l.{v.key()}'))\n",
    "    del seen_lemma_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473382, 34)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set = set()\n",
    "relation_set = set()\n",
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entity_set.add(row[0])\n",
    "        entity_set.add(row[2])\n",
    "        relation_set.add(row[1])\n",
    "entity_set = list(sorted(entity_set))\n",
    "relation_set = list(sorted(relation_set))\n",
    "entity_lookup = { k+1:v for (k,v) in enumerate(entity_set)}\n",
    "entity_index = { v:k+1 for (k,v) in enumerate(entity_set)}\n",
    "entity_max = len(entity_set)\n",
    "relation_lookup = { k+1:v for (k,v) in enumerate(relation_set)}\n",
    "relation_index = { v:k+1 for (k,v) in enumerate(relation_set)}\n",
    "relation_max = len(relation_set)\n",
    "del entity_set\n",
    "del relation_set\n",
    "len(entity_lookup), len(relation_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1287303, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dataset.append((entity_index[row[0]], relation_index[row[1]], entity_index[row[2]]))\n",
    "dataset = np.array(dataset, dtype='int')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bern = {}\n",
    "for h,r,t in dataset:\n",
    "    r_stat = bern.get(r)\n",
    "    if r_stat is None:\n",
    "        r_stat = (collections.Counter(), collections.Counter())\n",
    "        bern[r] = r_stat\n",
    "    h_stat, t_stat = r_stat\n",
    "    h_stat[h] += 1\n",
    "    t_stat[t] += 1\n",
    "bern = {r:(\n",
    "    np.mean(list(r_stat[0].values())),\n",
    "    np.mean(list(r_stat[1].values()))) for r,r_stat in bern.items()}\n",
    "bern = {r:r_stat[0]/(r_stat[0]+r_stat[1]) for r,r_stat in bern.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransF(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TransF, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        entity_shape, relation_shape = input_shape\n",
    "        self.kernel_width = entity_shape[1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(relation_shape[1], entity_shape[1], entity_shape[1]),\n",
    "            initializer='zeros', name='kernel')\n",
    "        super(TransF, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self: keras.layers.Layer\n",
    "        entity, relation = inputs\n",
    "        relation = K.reshape(relation, K.concatenate([K.shape(relation), (1,1)]))\n",
    "        relation = relation * self.kernel\n",
    "        relation = K.sum(relation, axis=1, keepdims=False)\n",
    "        relation = relation + K.eye(self.kernel_width)\n",
    "        result = (K.expand_dims(entity, axis=1) @ relation)[:,0]\n",
    "        unit_norm_output_loss = \\\n",
    "            K.abs(K.sqrt(K.sum(K.square(result), axis=-1, keepdims=True)) - 1.)\n",
    "        self.add_loss(unit_norm_output_loss)\n",
    "        return result\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        entity_shape, _ = input_shape\n",
    "        return entity_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 8)         280         input_3[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 32)        15148256    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 32)        1120        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8)            0           embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_1 (TransF)              (None, 32)           8192        flatten_1[0][0]                  \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 32)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 8)            0           flatten_4[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32)           0           trans_f_1[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_2 (TransF)              (None, 32)           8192        flatten_2[0][0]                  \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 32)           0           add_1[0][0]                      \n",
      "                                                                 trans_f_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           subtract_1[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 15,166,040\n",
      "Trainable params: 15,166,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 1)            15166040    input_5[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,166,040\n",
      "Trainable params: 15,166,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_h = X_input_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_t = X_input_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_r = X_input_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_s = X_input_s = keras.layers.Input((1,))\n",
    "X_e_embedding = keras.layers.Embedding(entity_max+1, 32)\n",
    "X_r_embedding = keras.layers.Embedding(relation_max+1, 32,\n",
    "    embeddings_constraint=keras.constraints.max_norm(max_value=1., axis=-1))\n",
    "X_hr_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_tr_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_hr_trans_f = TransF()\n",
    "X_tr_trans_f = TransF()\n",
    "X_h = X_e_embedding(X_h)\n",
    "X_t = X_e_embedding(X_t)\n",
    "X_hr = X_hr_embedding(X_r)\n",
    "X_tr = X_hr_embedding(X_r)\n",
    "X_r = X_r_embedding(X_r)\n",
    "X_h = keras.layers.Flatten()(X_h)\n",
    "X_t = keras.layers.Flatten()(X_t)\n",
    "X_hr = keras.layers.Flatten()(X_hr)\n",
    "X_tr = keras.layers.Flatten()(X_tr)\n",
    "X_hr = keras.layers.Multiply()([X_hr, X_s])\n",
    "X_tr = keras.layers.Multiply()([X_tr, X_s])\n",
    "X_r = keras.layers.Flatten()(X_r)\n",
    "X_h = X_hr_trans_f([X_h, X_hr])\n",
    "X_t = X_tr_trans_f([X_t, X_tr])\n",
    "X = keras.layers.Add()([X_h, X_r])\n",
    "X = keras.layers.Subtract()([X, X_t])\n",
    "X = keras.layers.Dot(-1)([X, X])\n",
    "X = keras.layers.Lambda(lambda x: K.sqrt(x))(X)\n",
    "M_transf = keras.Model([X_input_h, X_input_r, X_input_t, X_input_s], X)\n",
    "M_transf.compile('adam', 'mse')\n",
    "M_transf.summary()\n",
    "X_p_h = X_input_p_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_t = X_input_p_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_r = X_input_p_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_h = X_input_n_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_t = X_input_n_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_r = X_input_n_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_p = M_transf([X_p_h, X_p_r, X_p_t, X_input_s])\n",
    "X_n = M_transf([X_n_h, X_n_r, X_n_t, X_input_s])\n",
    "X = keras.layers.Concatenate()([X_p, X_n])\n",
    "M = keras.Model([\n",
    "    X_input_p_h, X_input_p_r, X_input_p_t,\n",
    "    X_input_n_h, X_input_n_r, X_input_n_t,\n",
    "    X_input_s], X)\n",
    "def TransF_loss(t, y):\n",
    "    p,n = y[:,0], y[:,1]\n",
    "    margin_loss = K.sum(K.clip(p + 3 - n, 0, np.inf))\n",
    "    energy_loss = K.sum(p)\n",
    "    return margin_loss + energy_loss + 0*K.sum(t)\n",
    "M.compile('adam', TransF_loss)\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_sampling():\n",
    "    negative_type = np.array([bern[r] for r in dataset[:,1]])\n",
    "    negative_type = np.random.rand(dataset.shape[0]) < negative_type\n",
    "    d0 = np.where(negative_type, np.random.randint(\n",
    "        0, entity_max+1, size=dataset.shape[:1]), dataset[:,0])\n",
    "    d1 = np.where(~negative_type, np.random.randint(\n",
    "        0, entity_max+1, size=dataset.shape[:1]), dataset[:,2])\n",
    "    return np.stack([d0, dataset[:,1], d1], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1287303/1287303 [==============================] - 607s 471us/step - loss: 1488.7420\n",
      "Epoch 2/2\n",
      "1287303/1287303 [==============================] - 603s 468us/step - loss: 1427.7045\n",
      "Epoch 3/3\n",
      "1287303/1287303 [==============================] - 603s 468us/step - loss: 1340.9948\n",
      "Epoch 4/4\n",
      "1287303/1287303 [==============================] - 603s 468us/step - loss: 1245.0671\n",
      "Epoch 5/5\n",
      "1287303/1287303 [==============================] - 603s 468us/step - loss: 1183.5045\n",
      "Epoch 6/6\n",
      "1287303/1287303 [==============================] - 602s 468us/step - loss: 1173.0245\n",
      "Epoch 7/7\n",
      "1287303/1287303 [==============================] - 602s 468us/step - loss: 1170.5612\n",
      "Epoch 8/8\n",
      "1287303/1287303 [==============================] - 602s 468us/step - loss: 1169.8090\n",
      "Epoch 9/9\n",
      "1287303/1287303 [==============================] - 603s 468us/step - loss: 1169.8935\n",
      "Epoch 10/10\n",
      "1287303/1287303 [==============================] - 603s 468us/step - loss: 1167.4487\n",
      "Epoch 11/11\n",
      "1287303/1287303 [==============================] - 603s 469us/step - loss: 1161.8341\n",
      "Epoch 12/12\n",
      " 986112/1287303 [=====================>........] - ETA: 2:21 - loss: 1148.0149"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cb46e6ac3705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         s((dataset.shape[0],1))],\n\u001b[1;32m      9\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         initial_epoch=epoch, epochs=epoch+1)\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCH = 25\n",
    "for epoch in range(N_EPOCH):\n",
    "    nsamples = negative_sampling()\n",
    "    s = np.zeros if epoch < (N_EPOCH*2/3) else np.ones\n",
    "    M.fit([\n",
    "        dataset[:,0],dataset[:,1],dataset[:,2],\n",
    "        nsamples[:,0],nsamples[:,1],nsamples[:,2],\n",
    "        s((dataset.shape[0],1))],\n",
    "        np.zeros((dataset.shape[0],2)), batch_size=512,\n",
    "        initial_epoch=epoch, epochs=epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.models.save_model(M, './models/knowledge-graphj-embedding-analogic/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fc1588764e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models.load_model('./models/knowledge-graphj-embedding-analogic/model.hdf5', custom_objects={\n",
    "    'TransF': TransF,\n",
    "    'TransF_loss': TransF_loss,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('united_states.n.01',\n",
       " [Synset('alabama.n.01'),\n",
       "  Synset('alaska.n.01'),\n",
       "  Synset('american_state.n.01'),\n",
       "  Synset('arizona.n.01'),\n",
       "  Synset('arkansas.n.01'),\n",
       "  Synset('california.n.01'),\n",
       "  Synset('colony.n.03'),\n",
       "  Synset('colorado.n.01'),\n",
       "  Synset('connecticut.n.01'),\n",
       "  Synset('connecticut.n.02'),\n",
       "  Synset('dakota.n.02'),\n",
       "  Synset('delaware.n.04'),\n",
       "  Synset('district_of_columbia.n.01'),\n",
       "  Synset('east.n.03'),\n",
       "  Synset('florida.n.01'),\n",
       "  Synset('georgia.n.01'),\n",
       "  Synset('great_lakes.n.01'),\n",
       "  Synset('hawaii.n.01'),\n",
       "  Synset('idaho.n.01'),\n",
       "  Synset('illinois.n.01'),\n",
       "  Synset('indiana.n.01'),\n",
       "  Synset('iowa.n.02'),\n",
       "  Synset('kansas.n.01'),\n",
       "  Synset('kentucky.n.01'),\n",
       "  Synset('louisiana.n.01'),\n",
       "  Synset('louisiana_purchase.n.01'),\n",
       "  Synset('maine.n.01'),\n",
       "  Synset('maryland.n.01'),\n",
       "  Synset('massachusetts.n.01'),\n",
       "  Synset('michigan.n.01'),\n",
       "  Synset('mid-atlantic_states.n.01'),\n",
       "  Synset('midwest.n.01'),\n",
       "  Synset('minnesota.n.01'),\n",
       "  Synset('mississippi.n.01'),\n",
       "  Synset('mississippi.n.02'),\n",
       "  Synset('missouri.n.01'),\n",
       "  Synset('missouri.n.02'),\n",
       "  Synset('montana.n.01'),\n",
       "  Synset('nebraska.n.01'),\n",
       "  Synset('nevada.n.01'),\n",
       "  Synset('new_england.n.01'),\n",
       "  Synset('new_hampshire.n.01'),\n",
       "  Synset('new_jersey.n.01'),\n",
       "  Synset('new_mexico.n.01'),\n",
       "  Synset('new_river.n.01'),\n",
       "  Synset('new_york.n.02'),\n",
       "  Synset('niagara.n.02'),\n",
       "  Synset('niobrara.n.01'),\n",
       "  Synset('north.n.01'),\n",
       "  Synset('north_carolina.n.01'),\n",
       "  Synset('north_dakota.n.01'),\n",
       "  Synset('ohio.n.01'),\n",
       "  Synset('ohio.n.02'),\n",
       "  Synset('oklahoma.n.01'),\n",
       "  Synset('oregon.n.01'),\n",
       "  Synset('pacific_northwest.n.01'),\n",
       "  Synset('pennsylvania.n.01'),\n",
       "  Synset('rhode_island.n.01'),\n",
       "  Synset('rio_grande.n.01'),\n",
       "  Synset('saint_lawrence.n.02'),\n",
       "  Synset('south.n.01'),\n",
       "  Synset('south_carolina.n.02'),\n",
       "  Synset('south_dakota.n.01'),\n",
       "  Synset('sunbelt.n.01'),\n",
       "  Synset('tennessee.n.01'),\n",
       "  Synset('texas.n.01'),\n",
       "  Synset('twin.n.03'),\n",
       "  Synset('utah.n.01'),\n",
       "  Synset('vermont.n.01'),\n",
       "  Synset('virginia.n.01'),\n",
       "  Synset('washington.n.02'),\n",
       "  Synset('west.n.03'),\n",
       "  Synset('west_virginia.n.01'),\n",
       "  Synset('wisconsin.n.02'),\n",
       "  Synset('wyoming.n.01'),\n",
       "  Synset('yosemite.n.01'),\n",
       "  Synset('yukon.n.01')])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_synset = nltk.corpus.wordnet.lemmas('usa', pos='n')[0].synset()\n",
    "test_synset.name(), test_synset.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l.also_sees': 1, 'l.antonyms': 2, 'l.derivationally_related_forms': 3, 'l.frame_ids': 4, 'l.pertainyms': 5, 'l.region_domains': 6, 'l.synset': 7, 'l.syntactic_marker': 8, 'l.topic_domains': 9, 'l.usage_domains': 10, 'l.verb_groups': 11, 's.also_sees': 12, 's.attributes': 13, 's.causes': 14, 's.entailments': 15, 's.frame_ids': 16, 's.hypernyms': 17, 's.hyponyms': 18, 's.instance_hypernyms': 19, 's.instance_hyponyms': 20, 's.lemmas': 21, 's.member_holonyms': 22, 's.member_meronyms': 23, 's.part_holonyms': 24, 's.pos': 25, 's.region_domains': 26, 's.root_hypernyms': 27, 's.similar_tos': 28, 's.substance_holonyms': 29, 's.substance_meronyms': 30, 's.topic_domains': 31, 's.usage_domains': 32, 's.verb_groups': 33}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'s.part_meronym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7c1937cf2e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m M_transf.predict([\n\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's.united_states.n.01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelation_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's.part_meronym'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentity_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's.alaska.n.01'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     np.ones((1, 1))], batch_size=512)\n",
      "\u001b[0;31mKeyError\u001b[0m: 's.part_meronym'"
     ]
    }
   ],
   "source": [
    "print(relation_index)\n",
    "M_transf.predict([\n",
    "    np.array([[entity_index['s.united_states.n.01']]]),\n",
    "    np.array([[relation_index['s.part_meronym']]]),\n",
    "    np.array([[entity_index['s.alaska.n.01']]]),\n",
    "    np.ones((1, 1))], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s.belong.v.05',\n",
       " 's.decide.v.03',\n",
       " 's.ferment.v.02',\n",
       " 's.download.v.01',\n",
       " 's.separate.v.08',\n",
       " 's.zoom.v.02',\n",
       " 's.confederate.v.02',\n",
       " 's.seed.v.05',\n",
       " 's.heckle.v.02',\n",
       " 's.cope.v.01',\n",
       " 's.film.v.01',\n",
       " 's.madden.v.03',\n",
       " 's.rerun.v.03',\n",
       " 's.computerize.v.01',\n",
       " 's.sack.v.01',\n",
       " 's.neutralize.v.02',\n",
       " 's.sleet.v.01',\n",
       " 's.even.v.03',\n",
       " 's.declare.v.07',\n",
       " 's.shuffle.v.03',\n",
       " 's.legislate.v.01',\n",
       " 's.fictionalize.v.01',\n",
       " 's.alkalinize.v.01',\n",
       " 's.raddle.v.01',\n",
       " 's.ting.v.01',\n",
       " 's.madder.v.01',\n",
       " 's.gate.v.02',\n",
       " 's.score.v.06',\n",
       " 's.better.v.01',\n",
       " 's.recast.v.02',\n",
       " 's.join_battle.v.01',\n",
       " 's.rout.v.03',\n",
       " 's.drop.v.20',\n",
       " 's.oppose.v.03',\n",
       " 's.suffuse.v.02',\n",
       " 's.outlaw.v.01',\n",
       " 's.hob.v.01',\n",
       " 's.rush_off.v.01',\n",
       " 's.eke_out.v.01',\n",
       " 's.down.v.04',\n",
       " 's.uncover.v.01',\n",
       " 's.pull.v.08',\n",
       " 's.blank.v.01',\n",
       " 's.secure.v.03',\n",
       " 's.outgrow.v.02',\n",
       " 's.notice.v.02',\n",
       " 's.truss.v.03',\n",
       " 's.encrust.v.03',\n",
       " 's.ennoble.v.01',\n",
       " 's.flip.v.01']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = M_transf.predict([\n",
    "    np.repeat(np.array([[entity_index['s.drink.v.01']]]), entity_max+1, axis=0),\n",
    "    np.repeat(np.array([[relation_index['s.hyponyms']]]), entity_max+1, axis=0),\n",
    "    np.arange(0, entity_max+1, 1, dtype='int32')[:, np.newaxis],\n",
    "    np.ones((entity_max+1, 1))], batch_size=512)\n",
    "[entity_lookup[i] for i in np.argsort(result, axis=0)[:50,0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
