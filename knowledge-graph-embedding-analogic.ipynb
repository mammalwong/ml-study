{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/marco/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate wordnet relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for s in nltk.corpus.wordnet.all_synsets():\n",
    "        for v in s.also_sees():\n",
    "            writer.writerow((f's.{s.name()}', 's.also_sees', f's.{v.name()}'))\n",
    "        for v in s.attributes():\n",
    "            writer.writerow((f's.{s.name()}', 's.attributes', f's.{v.name()}'))\n",
    "        for v in s.causes():\n",
    "            writer.writerow((f's.{s.name()}', 's.causes', f's.{v.name()}'))\n",
    "        for v in s.entailments():\n",
    "            writer.writerow((f's.{s.name()}', 's.entailments', f's.{v.name()}'))\n",
    "        for v in s.hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hypernyms', f's.{v.name()}'))\n",
    "        for v in s.hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hyponyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hyponyms', f's.{v.name()}'))\n",
    "        for v in s.lemmas():\n",
    "            writer.writerow((f's.{s.name()}', 's.lemmas', f'l.{v.name()}'))\n",
    "        for v in s.member_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_holonyms', f's.{v.name()}'))\n",
    "        for v in s.member_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_meronyms', f's.{v.name()}'))\n",
    "        for v in s.part_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_holonyms', f's.{v.name()}'))\n",
    "        for v in s.part_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_meronyms', f's.{v.name()}'))\n",
    "        writer.writerow((f's.{s.name()}', 's.pos', f'p.{s.pos()}'))\n",
    "        for v in s.region_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.region_domains', f's.{v.name()}'))\n",
    "        for v in s.root_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.root_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.similar_tos():\n",
    "            writer.writerow((f's.{s.name()}', 's.similar_tos', f's.{v.name()}'))\n",
    "        for v in s.substance_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_holonyms', f's.{v.name()}'))\n",
    "        for v in s.substance_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_meronyms', f's.{v.name()}'))\n",
    "        for v in s.topic_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.topic_domains', f's.{v.name()}'))\n",
    "        for v in s.usage_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.usage_domains', f's.{v.name()}'))\n",
    "        for v in s.verb_groups():\n",
    "            writer.writerow((f's.{s.name()}', 's.verb_groups', f's.{v.name()}'))\n",
    "        for v in s.frame_ids():\n",
    "            writer.writerow((f's.{s.name()}', 's.frame_ids', f'f.{v}'))\n",
    "    seen_lemma_keys = set()\n",
    "    for l_name in nltk.corpus.wordnet.all_lemma_names():\n",
    "        ls = nltk.corpus.wordnet.lemmas(l_name)\n",
    "        for l in ls:\n",
    "            if l.key() in seen_lemma_keys:\n",
    "                continue\n",
    "            seen_lemma_keys.add(l.key())\n",
    "            for v in l.also_sees():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.also_sees', f'l.{v.key()}'))\n",
    "            for v in l.antonyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.antonyms', f'l.{v.key()}'))\n",
    "            for v in l.derivationally_related_forms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.derivationally_related_forms', f'l.{v.key()}'))\n",
    "            for v in l.frame_ids():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.frame_ids', f'f.{v}'))\n",
    "            for v in l.pertainyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.pertainyms', f'l.{v.key()}'))\n",
    "            for v in l.region_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.region_domains', f'l.{v.key()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.synset', f's.{l.synset().name()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.syntactic_marker', f'sm.{l.syntactic_marker()}'))\n",
    "            for v in l.topic_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.topic_domains', f'l.{v.key()}'))\n",
    "            for v in l.usage_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.usage_domains', f'l.{v.key()}'))\n",
    "            for v in l.verb_groups():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.verb_groups', f'l.{v.key()}'))\n",
    "    del seen_lemma_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473382, 34)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set = set()\n",
    "relation_set = set()\n",
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entity_set.add(row[0])\n",
    "        entity_set.add(row[2])\n",
    "        relation_set.add(row[1])\n",
    "entity_set = list(sorted(entity_set))\n",
    "relation_set = list(sorted(relation_set))\n",
    "entity_lookup = { k+1:v for (k,v) in enumerate(entity_set)}\n",
    "entity_index = { v:k+1 for (k,v) in enumerate(entity_set)}\n",
    "entity_max = len(entity_set)\n",
    "relation_lookup = { k+1:v for (k,v) in enumerate(relation_set)}\n",
    "relation_index = { v:k+1 for (k,v) in enumerate(relation_set)}\n",
    "relation_max = len(relation_set)\n",
    "del entity_set\n",
    "del relation_set\n",
    "len(entity_lookup), len(relation_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1287303, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('./models/knowledge-graphj-embedding-analogic/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dataset.append((entity_index[row[0]], relation_index[row[1]], entity_index[row[2]]))\n",
    "dataset = np.array(dataset, dtype='int')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bern = {}\n",
    "for h,r,t in dataset:\n",
    "    r_stat = bern.get(r)\n",
    "    if r_stat is None:\n",
    "        r_stat = (collections.Counter(), collections.Counter())\n",
    "        bern[r] = r_stat\n",
    "    h_stat, t_stat = r_stat\n",
    "    h_stat[h] += 1\n",
    "    t_stat[t] += 1\n",
    "bern = {r:(\n",
    "    np.mean(list(r_stat[0].values())),\n",
    "    np.mean(list(r_stat[1].values()))) for r,r_stat in bern.items()}\n",
    "bern = {r:r_stat[0]/(r_stat[0]+r_stat[1]) for r,r_stat in bern.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransF(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TransF, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        entity_shape, relation_shape = input_shape\n",
    "        self.kernel_width = entity_shape[1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(relation_shape[1], entity_shape[1], entity_shape[1]),\n",
    "            initializer='zeros', name='kernel')\n",
    "        super(TransF, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self: keras.layers.Layer\n",
    "        entity, relation = inputs\n",
    "        relation = K.reshape(relation, K.concatenate([K.shape(relation), (1,1)]))\n",
    "        relation = relation * self.kernel\n",
    "        relation = K.sum(relation, axis=1, keepdims=False)\n",
    "        relation = relation + K.eye(self.kernel_width)\n",
    "        result = (K.expand_dims(entity, axis=1) @ relation)[:,0]\n",
    "        unit_norm_output_loss = \\\n",
    "            K.sum(K.abs(K.sqrt(K.sum(K.square(result), axis=-1, keepdims=True)) - 1.))\n",
    "        self.add_loss(unit_norm_output_loss, inputs=inputs)\n",
    "        return result\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        entity_shape, _ = input_shape\n",
    "        return entity_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 8)         280         input_3[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 32)        15148256    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 8)            0           flatten_3[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 32)        1120        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8)            0           embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_1 (TransF)              (None, 32)           8192        flatten_1[0][0]                  \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 32)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 8)            0           flatten_4[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32)           0           trans_f_1[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_2 (TransF)              (None, 32)           8192        flatten_2[0][0]                  \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 32)           0           add_1[0][0]                      \n",
      "                                                                 trans_f_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           subtract_1[0][0]                 \n",
      "                                                                 subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 15,166,040\n",
      "Trainable params: 15,166,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 1)            15166040    input_5[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,166,040\n",
      "Trainable params: 15,166,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_h = X_input_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_t = X_input_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_r = X_input_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_s = X_input_s = keras.layers.Input((1,))\n",
    "X_e_embedding = keras.layers.Embedding(entity_max+1, 32)\n",
    "X_r_embedding = keras.layers.Embedding(relation_max+1, 32,\n",
    "    embeddings_constraint=keras.constraints.max_norm(max_value=1., axis=-1))\n",
    "X_hr_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_tr_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_hr_trans_f = TransF()\n",
    "X_tr_trans_f = TransF()\n",
    "X_h = X_e_embedding(X_h)\n",
    "X_t = X_e_embedding(X_t)\n",
    "X_hr = X_hr_embedding(X_r)\n",
    "X_tr = X_hr_embedding(X_r)\n",
    "X_r = X_r_embedding(X_r)\n",
    "X_h = keras.layers.Flatten()(X_h)\n",
    "X_t = keras.layers.Flatten()(X_t)\n",
    "X_hr = keras.layers.Flatten()(X_hr)\n",
    "X_tr = keras.layers.Flatten()(X_tr)\n",
    "X_hr = keras.layers.Multiply()([X_hr, X_s])\n",
    "X_tr = keras.layers.Multiply()([X_tr, X_s])\n",
    "X_r = keras.layers.Flatten()(X_r)\n",
    "X_h = X_hr_trans_f([X_h, X_hr])\n",
    "X_t = X_tr_trans_f([X_t, X_tr])\n",
    "X = keras.layers.Add()([X_h, X_r])\n",
    "X = keras.layers.Subtract()([X, X_t])\n",
    "X = keras.layers.Dot(-1)([X, X])\n",
    "X = keras.layers.Lambda(lambda x: K.sqrt(x))(X)\n",
    "M_transf = keras.Model([X_input_h, X_input_r, X_input_t, X_input_s], X)\n",
    "M_transf.compile('adam', 'mse')\n",
    "M_transf.summary()\n",
    "X_p_h = X_input_p_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_t = X_input_p_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_r = X_input_p_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_h = X_input_n_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_t = X_input_n_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_r = X_input_n_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_p = M_transf([X_p_h, X_p_r, X_p_t, X_input_s])\n",
    "X_n = M_transf([X_n_h, X_n_r, X_n_t, X_input_s])\n",
    "X = keras.layers.Concatenate()([X_p, X_n])\n",
    "M = keras.Model([\n",
    "    X_input_p_h, X_input_p_r, X_input_p_t,\n",
    "    X_input_n_h, X_input_n_r, X_input_n_t,\n",
    "    X_input_s], X)\n",
    "def TransF_loss(t, y):\n",
    "    p,n = y[:,0], y[:,1]\n",
    "    margin_loss = K.sum(K.clip(p + 3 - n, 0, np.inf))\n",
    "    return margin_loss + 0*K.sum(t)\n",
    "M.compile('adam', TransF_loss)\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_sampling():\n",
    "    negative_type = np.array([bern[r] for r in dataset[:,1]])\n",
    "    negative_type = np.random.rand(dataset.shape[0]) < negative_type\n",
    "    d0 = np.where(negative_type, np.random.randint(\n",
    "        0, entity_max+1, size=dataset.shape[:1]), dataset[:,0])\n",
    "    d1 = np.where(~negative_type, np.random.randint(\n",
    "        0, entity_max+1, size=dataset.shape[:1]), dataset[:,2])\n",
    "    return np.stack([d0, dataset[:,1], d1], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1287303/1287303 [==============================] - 604s 470us/step - loss: 2644.8216\n",
      "Epoch 2/2\n",
      "1287303/1287303 [==============================] - 604s 470us/step - loss: 1979.6596\n",
      "Epoch 3/3\n",
      "1287303/1287303 [==============================] - 605s 470us/step - loss: 1636.4646\n",
      "Epoch 4/4\n",
      "1287303/1287303 [==============================] - 605s 470us/step - loss: 1533.1313\n",
      "Epoch 5/5\n",
      "1287303/1287303 [==============================] - 604s 470us/step - loss: 1482.3481\n",
      "Epoch 6/6\n",
      "1287303/1287303 [==============================] - 604s 469us/step - loss: 1448.4733\n",
      "Epoch 7/7\n",
      "1287303/1287303 [==============================] - 604s 469us/step - loss: 1422.2481\n",
      "Epoch 8/8\n",
      "1287303/1287303 [==============================] - 604s 469us/step - loss: 1180.3356\n",
      "Epoch 9/9\n",
      "1287303/1287303 [==============================] - 604s 469us/step - loss: 1015.1699\n",
      "Epoch 10/10\n",
      "1287303/1287303 [==============================] - 604s 469us/step - loss: 944.4752\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 10\n",
    "for epoch in range(N_EPOCH):\n",
    "    nsamples = negative_sampling()\n",
    "    s = np.zeros if epoch < (N_EPOCH*2/3) else np.ones\n",
    "    M.fit([\n",
    "        dataset[:,0],dataset[:,1],dataset[:,2],\n",
    "        nsamples[:,0],nsamples[:,1],nsamples[:,2],\n",
    "        s((dataset.shape[0],1))],\n",
    "        np.zeros((dataset.shape[0],2)), batch_size=512,\n",
    "        initial_epoch=epoch, epochs=epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.models.save_model(M, './models/knowledge-graphj-embedding-analogic/model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fc1588764e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models.load_model('./models/knowledge-graphj-embedding-analogic/model.hdf5', custom_objects={\n",
    "    'TransF': TransF,\n",
    "    'TransF_loss': TransF_loss,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eat.v.01',\n",
       " [Synset('devour.v.03'),\n",
       "  Synset('devour.v.04'),\n",
       "  Synset('dunk.v.03'),\n",
       "  Synset('eat_up.v.01'),\n",
       "  Synset('fare.v.02'),\n",
       "  Synset('fill_up.v.04'),\n",
       "  Synset('garbage_down.v.01'),\n",
       "  Synset('gluttonize.v.01'),\n",
       "  Synset('gobble.v.01'),\n",
       "  Synset('nibble.v.03'),\n",
       "  Synset('peck.v.02'),\n",
       "  Synset('pick_at.v.02'),\n",
       "  Synset('pitch_in.v.01'),\n",
       "  Synset('ruminate.v.01'),\n",
       "  Synset('slurp.v.01'),\n",
       "  Synset('wash_down.v.01'),\n",
       "  Synset('wolf.v.01')])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_synset = nltk.corpus.wordnet.lemmas('eat', pos='v')[0].synset()\n",
    "test_synset_truth = test_synset.hyponyms()\n",
    "test_synset.name(), test_synset_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0803591]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_transf.predict([\n",
    "    np.array([[entity_index['s.eat.v.01']]]),\n",
    "    np.array([[relation_index['s.hyponyms']]]),\n",
    "    np.array([[entity_index['s.devour.v.03']]]),\n",
    "    np.ones((1, 1))], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([291197,  88672,  49676, 160076, 337285, 186441, 109567, 294292,\n",
       "         21743, 162167,  66331, 224378, 118056, 206297, 158475, 288275,\n",
       "        139392]),\n",
       " array([0.08035908, 0.04940243, 0.05714422, 0.04039836, 0.02479357,\n",
       "        0.10461397, 0.08998181, 0.04257538, 0.12528042, 0.02447703,\n",
       "        0.03853939, 0.07060257, 0.03144142, 0.08907203, 0.09094734,\n",
       "        0.05514144, 0.03026585], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = M_transf.predict([\n",
    "    np.repeat(np.array([[entity_index['s.eat.v.01']]]), entity_max+1, axis=0),\n",
    "    np.repeat(np.array([[relation_index['s.hyponyms']]]), entity_max+1, axis=0),\n",
    "    np.arange(0, entity_max+1, 1, dtype='int32')[:, np.newaxis],\n",
    "    np.ones((entity_max+1, 1))], batch_size=512)\n",
    "result_argsort = np.argsort(result, axis=0)\n",
    "test_synset_truth_indices = [entity_index[f's.{s.name()}'] for s in test_synset_truth]\n",
    "result_argsort[test_synset_truth_indices, 0], result[test_synset_truth_indices, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
