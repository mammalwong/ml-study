{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/marco/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate wordnet relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./utils/wordnet_pairs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for s in nltk.corpus.wordnet.all_synsets():\n",
    "        for v in s.also_sees():\n",
    "            writer.writerow((f's.{s.name()}', 's.also_sees', f's.{v.name()}'))\n",
    "        for v in s.attributes():\n",
    "            writer.writerow((f's.{s.name()}', 's.attributes', f's.{v.name()}'))\n",
    "        for v in s.causes():\n",
    "            writer.writerow((f's.{s.name()}', 's.causes', f's.{v.name()}'))\n",
    "        for v in s.entailments():\n",
    "            writer.writerow((f's.{s.name()}', 's.entailments', f's.{v.name()}'))\n",
    "        for v in s.hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hypernyms', f's.{v.name()}'))\n",
    "        for v in s.hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hyponyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hyponyms', f's.{v.name()}'))\n",
    "        for v in s.lemmas():\n",
    "            writer.writerow((f's.{s.name()}', 's.lemmas', f'l.{v.name()}'))\n",
    "        for v in s.member_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_holonyms', f's.{v.name()}'))\n",
    "        for v in s.member_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_meronyms', f's.{v.name()}'))\n",
    "        for v in s.part_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_holonyms', f's.{v.name()}'))\n",
    "        writer.writerow((f's.{s.name()}', 's.pos', f'p.{s.pos()}'))\n",
    "        for v in s.region_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.region_domains', f's.{v.name()}'))\n",
    "        for v in s.root_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.root_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.similar_tos():\n",
    "            writer.writerow((f's.{s.name()}', 's.similar_tos', f's.{v.name()}'))\n",
    "        for v in s.substance_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_holonyms', f's.{v.name()}'))\n",
    "        for v in s.substance_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_meronyms', f's.{v.name()}'))\n",
    "        for v in s.topic_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.topic_domains', f's.{v.name()}'))\n",
    "        for v in s.usage_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.usage_domains', f's.{v.name()}'))\n",
    "        for v in s.verb_groups():\n",
    "            writer.writerow((f's.{s.name()}', 's.verb_groups', f's.{v.name()}'))\n",
    "        for v in s.frame_ids():\n",
    "            writer.writerow((f's.{s.name()}', 's.frame_ids', f'f.{v}'))\n",
    "    seen_lemma_keys = set()\n",
    "    for l_name in nltk.corpus.wordnet.all_lemma_names():\n",
    "        ls = nltk.corpus.wordnet.lemmas(l_name)\n",
    "        for l in ls:\n",
    "            if l.key() in seen_lemma_keys:\n",
    "                continue\n",
    "            seen_lemma_keys.add(l.key())\n",
    "            for v in l.also_sees():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.also_sees', f'l.{v.key()}'))\n",
    "            for v in l.antonyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.antonyms', f'l.{v.key()}'))\n",
    "            for v in l.derivationally_related_forms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.derivationally_related_forms', f'l.{v.key()}'))\n",
    "            for v in l.frame_ids():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.frame_ids', f'f.{v}'))\n",
    "            for v in l.pertainyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.pertainyms', f'l.{v.key()}'))\n",
    "            for v in l.region_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.region_domains', f'l.{v.key()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.synset', f's.{l.synset().name()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.syntactic_marker', f'sm.{l.syntactic_marker()}'))\n",
    "            for v in l.topic_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.topic_domains', f'l.{v.key()}'))\n",
    "            for v in l.usage_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.usage_domains', f'l.{v.key()}'))\n",
    "            for v in l.verb_groups():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.verb_groups', f'l.{v.key()}'))\n",
    "    del seen_lemma_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473382, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set = set()\n",
    "relation_set = set()\n",
    "with open('./utils/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entity_set.add(row[0])\n",
    "        entity_set.add(row[2])\n",
    "        relation_set.add(row[1])\n",
    "entity_lookup = { k+1:v for (k,v) in enumerate(entity_set)}\n",
    "entity_index = { v:k+1 for (k,v) in enumerate(entity_set)}\n",
    "entity_max = len(entity_set)\n",
    "relation_lookup = { k+1:v for (k,v) in enumerate(relation_set)}\n",
    "relation_index = { v:k+1 for (k,v) in enumerate(relation_set)}\n",
    "relation_max = len(relation_set)\n",
    "del entity_set\n",
    "del relation_set\n",
    "len(entity_lookup), len(relation_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1278206, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('./utils/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dataset.append((entity_index[row[0]], relation_index[row[1]], entity_index[row[2]]))\n",
    "dataset = np.array(dataset, dtype='int')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransF(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TransF, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        entity_shape, relation_shape = input_shape\n",
    "        self.kernel_width = entity_shape[1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(relation_shape[1], entity_shape[1], entity_shape[1]),\n",
    "            initializer='glorot_uniform', name='kernel')\n",
    "        super(TransF, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        entity, relation = inputs\n",
    "        relation = K.reshape(relation, K.concatenate([K.shape(relation), (1,1)]))\n",
    "        relation = relation * self.kernel\n",
    "        relation = K.sum(relation, axis=1, keepdims=False)\n",
    "        relation = relation + K.eye(self.kernel_width)\n",
    "        return (K.expand_dims(entity, axis=1) @ relation)[:,0]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        entity_shape, _ = input_shape\n",
    "        return entity_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_88 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_90 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_89 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_59 (Embedding)        (None, 1, 32)        15148256    input_88[0][0]                   \n",
      "                                                                 input_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_60 (Embedding)        (None, 1, 8)         272         input_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 32)           0           embedding_59[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8)            0           embedding_60[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 32)           0           embedding_59[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_30 (TransF)             (None, 32)           8192        flatten_16[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 32)           0           trans_f_30[0][0]                 \n",
      "                                                                 trans_f_30[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           subtract_2[0][0]                 \n",
      "                                                                 subtract_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 15,156,720\n",
      "Trainable params: 15,156,720\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_h = X_input_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_t = X_input_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_r = X_input_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_e_embedding = keras.layers.Embedding(entity_max+1, 32)\n",
    "X_r_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_trans_f = TransF()\n",
    "X_h = X_e_embedding(X_h)\n",
    "X_h = keras.layers.Flatten()(X_h)\n",
    "X_t = X_e_embedding(X_t)\n",
    "X_t = keras.layers.Flatten()(X_t)\n",
    "X_r = X_r_embedding(X_r)\n",
    "X_r = keras.layers.Flatten()(X_r)\n",
    "X_h = X_trans_f([X_h, X_r])\n",
    "X_t = X_trans_f([X_t, X_r])\n",
    "X = keras.layers.Subtract()([X_h, X_t])\n",
    "X = keras.layers.Dot(-1)([X, X])\n",
    "M_sample = keras.Model([X_input_h, X_input_r, X_input_t], X)\n",
    "M_sample.compile('adam', 'mse')\n",
    "M_sample.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  49664/1278206 [>.............................] - ETA: 8:40 - loss: 0.0020"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c95214151d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "M_sample.fit([dataset[:,0],dataset[:,1],dataset[:,2]], np.zeros((dataset.shape[0],1)), batch_size=512, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
