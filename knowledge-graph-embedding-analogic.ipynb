{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marco/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/marco/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate wordnet relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./utils/wordnet_pairs.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for s in nltk.corpus.wordnet.all_synsets():\n",
    "        for v in s.also_sees():\n",
    "            writer.writerow((f's.{s.name()}', 's.also_sees', f's.{v.name()}'))\n",
    "        for v in s.attributes():\n",
    "            writer.writerow((f's.{s.name()}', 's.attributes', f's.{v.name()}'))\n",
    "        for v in s.causes():\n",
    "            writer.writerow((f's.{s.name()}', 's.causes', f's.{v.name()}'))\n",
    "        for v in s.entailments():\n",
    "            writer.writerow((f's.{s.name()}', 's.entailments', f's.{v.name()}'))\n",
    "        for v in s.hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hypernyms', f's.{v.name()}'))\n",
    "        for v in s.hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.hyponyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.instance_hyponyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.instance_hyponyms', f's.{v.name()}'))\n",
    "        for v in s.lemmas():\n",
    "            writer.writerow((f's.{s.name()}', 's.lemmas', f'l.{v.name()}'))\n",
    "        for v in s.member_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_holonyms', f's.{v.name()}'))\n",
    "        for v in s.member_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.member_meronyms', f's.{v.name()}'))\n",
    "        for v in s.part_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.part_holonyms', f's.{v.name()}'))\n",
    "        writer.writerow((f's.{s.name()}', 's.pos', f'p.{s.pos()}'))\n",
    "        for v in s.region_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.region_domains', f's.{v.name()}'))\n",
    "        for v in s.root_hypernyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.root_hypernyms', f's.{v.name()}'))\n",
    "        for v in s.similar_tos():\n",
    "            writer.writerow((f's.{s.name()}', 's.similar_tos', f's.{v.name()}'))\n",
    "        for v in s.substance_holonyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_holonyms', f's.{v.name()}'))\n",
    "        for v in s.substance_meronyms():\n",
    "            writer.writerow((f's.{s.name()}', 's.substance_meronyms', f's.{v.name()}'))\n",
    "        for v in s.topic_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.topic_domains', f's.{v.name()}'))\n",
    "        for v in s.usage_domains():\n",
    "            writer.writerow((f's.{s.name()}', 's.usage_domains', f's.{v.name()}'))\n",
    "        for v in s.verb_groups():\n",
    "            writer.writerow((f's.{s.name()}', 's.verb_groups', f's.{v.name()}'))\n",
    "        for v in s.frame_ids():\n",
    "            writer.writerow((f's.{s.name()}', 's.frame_ids', f'f.{v}'))\n",
    "    seen_lemma_keys = set()\n",
    "    for l_name in nltk.corpus.wordnet.all_lemma_names():\n",
    "        ls = nltk.corpus.wordnet.lemmas(l_name)\n",
    "        for l in ls:\n",
    "            if l.key() in seen_lemma_keys:\n",
    "                continue\n",
    "            seen_lemma_keys.add(l.key())\n",
    "            for v in l.also_sees():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.also_sees', f'l.{v.key()}'))\n",
    "            for v in l.antonyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.antonyms', f'l.{v.key()}'))\n",
    "            for v in l.derivationally_related_forms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.derivationally_related_forms', f'l.{v.key()}'))\n",
    "            for v in l.frame_ids():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.frame_ids', f'f.{v}'))\n",
    "            for v in l.pertainyms():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.pertainyms', f'l.{v.key()}'))\n",
    "            for v in l.region_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.region_domains', f'l.{v.key()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.synset', f's.{l.synset().name()}'))\n",
    "            writer.writerow((f'l.{l.key()}', 'l.syntactic_marker', f'sm.{l.syntactic_marker()}'))\n",
    "            for v in l.topic_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.topic_domains', f'l.{v.key()}'))\n",
    "            for v in l.usage_domains():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.usage_domains', f'l.{v.key()}'))\n",
    "            for v in l.verb_groups():\n",
    "                writer.writerow((f'l.{l.key()}', 'l.verb_groups', f'l.{v.key()}'))\n",
    "    del seen_lemma_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocess relation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473382, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_set = set()\n",
    "relation_set = set()\n",
    "with open('./utils/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        entity_set.add(row[0])\n",
    "        entity_set.add(row[2])\n",
    "        relation_set.add(row[1])\n",
    "entity_lookup = { k+1:v for (k,v) in enumerate(entity_set)}\n",
    "entity_index = { v:k+1 for (k,v) in enumerate(entity_set)}\n",
    "entity_max = len(entity_set)\n",
    "relation_lookup = { k+1:v for (k,v) in enumerate(relation_set)}\n",
    "relation_index = { v:k+1 for (k,v) in enumerate(relation_set)}\n",
    "relation_max = len(relation_set)\n",
    "del entity_set\n",
    "del relation_set\n",
    "len(entity_lookup), len(relation_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1278206, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('./utils/wordnet_pairs.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dataset.append((entity_index[row[0]], relation_index[row[1]], entity_index[row[2]]))\n",
    "dataset = np.array(dataset, dtype='int')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransF(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(TransF, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        entity_shape, relation_shape = input_shape\n",
    "        self.kernel_width = entity_shape[1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(relation_shape[1], entity_shape[1], entity_shape[1]),\n",
    "            initializer='glorot_uniform', name='kernel')\n",
    "        super(TransF, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        entity, relation = inputs\n",
    "        relation = K.reshape(relation, K.concatenate([K.shape(relation), (1,1)]))\n",
    "        relation = relation * self.kernel\n",
    "        relation = K.sum(relation, axis=1, keepdims=False)\n",
    "        relation = relation + K.eye(self.kernel_width)\n",
    "        return (K.expand_dims(entity, axis=1) @ relation)[:,0]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        entity_shape, _ = input_shape\n",
    "        return entity_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 32)        15148256    input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 8)         272         input_9[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 32)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 8)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 32)        1088        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_5 (TransF)              (None, 32)           8192        flatten_11[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 32)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 32)           0           embedding_9[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 8)            0           embedding_11[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32)           0           trans_f_5[0][0]                  \n",
      "                                                                 flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "trans_f_6 (TransF)              (None, 32)           8192        flatten_12[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 32)           0           add_3[0][0]                      \n",
      "                                                                 trans_f_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           subtract_3[0][0]                 \n",
      "                                                                 subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 15,166,000\n",
      "Trainable params: 15,166,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 1)            15166000    input_10[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 1)            0           model_3[1][0]                    \n",
      "                                                                 model_3[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,166,000\n",
      "Trainable params: 15,166,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X_h = X_input_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_t = X_input_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_r = X_input_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_e_embedding = keras.layers.Embedding(entity_max+1, 32)\n",
    "X_r_embedding = keras.layers.Embedding(relation_max+1, 32)\n",
    "X_hr_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_tr_embedding = keras.layers.Embedding(relation_max+1, 8)\n",
    "X_hr_trans_f = TransF()\n",
    "X_tr_trans_f = TransF()\n",
    "X_h = X_e_embedding(X_h)\n",
    "X_t = X_e_embedding(X_t)\n",
    "X_hr = X_hr_embedding(X_r)\n",
    "X_tr = X_hr_embedding(X_r)\n",
    "X_r = X_r_embedding(X_r)\n",
    "X_h = keras.layers.Flatten()(X_h)\n",
    "X_t = keras.layers.Flatten()(X_t)\n",
    "X_hr = keras.layers.Flatten()(X_hr)\n",
    "X_tr = keras.layers.Flatten()(X_tr)\n",
    "X_r = keras.layers.Flatten()(X_r)\n",
    "X_h = X_hr_trans_f([X_h, X_hr])\n",
    "X_t = X_tr_trans_f([X_t, X_tr])\n",
    "X = keras.layers.Add()([X_h, X_r])\n",
    "X = keras.layers.Subtract()([X, X_t])\n",
    "X = keras.layers.Dot(-1)([X, X])\n",
    "X = keras.layers.Lambda(lambda x: K.sqrt(x))(X)\n",
    "M_sample = keras.Model([X_input_h, X_input_r, X_input_t], X)\n",
    "M_sample.compile('adam', 'mse')\n",
    "M_sample.summary()\n",
    "X_p_h = X_input_p_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_t = X_input_p_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_p_r = X_input_p_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_h = X_input_n_h = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_t = X_input_n_t = keras.layers.Input((1,), dtype='int32')\n",
    "X_n_r = X_input_n_r = keras.layers.Input((1,), dtype='int32')\n",
    "X_p = M_sample([X_p_h, X_p_r, X_p_t])\n",
    "X_n = M_sample([X_n_h, X_n_r, X_n_t])\n",
    "X = keras.layers.Subtract()([X_p, X_n])\n",
    "M = keras.Model([X_p_h, X_p_r, X_p_t, X_n_h, X_n_r, X_n_t], X)\n",
    "M_loss = lambda t,y: K.sum(K.clip(y + 10, 0, np.inf) + 0*t)\n",
    "M.compile('adam', M_loss)\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling():\n",
    "    negative_type = np.random.randint(0, 3, size=dataset.shape[:1])\n",
    "    d0 = np.where(np.equal(negative_type, 0),\n",
    "                 np.random.randint(0, entity_max+1, size=dataset.shape[:1]), dataset[:,0])\n",
    "    d1 = np.where(np.equal(negative_type, 1),\n",
    "                 np.random.randint(0, relation_max+1, size=dataset.shape[:1]), dataset[:,1])\n",
    "    d2 = np.where(np.equal(negative_type, 2),\n",
    "                 np.random.randint(0, entity_max+1, size=dataset.shape[:1]), dataset[:,2])\n",
    "    return np.stack([d0, d1, d2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 3411.5648\n",
      "epoch 1\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 2315.1134\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 1691.5842\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 1416.0610\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 597s 467us/step - loss: 1199.2983\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 1034.8470\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 907.7265\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 817.3675\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 742.3552\n",
      "epoch 9\n",
      "Epoch 1/1\n",
      "1278206/1278206 [==============================] - 598s 468us/step - loss: 689.2537\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print('epoch', epoch)\n",
    "    nsamples = negative_sampling()\n",
    "    M.fit([\n",
    "        dataset[:,0],dataset[:,1],dataset[:,2],\n",
    "        nsamples[:,0],nsamples[:,1],nsamples[:,2]],\n",
    "        np.zeros((dataset.shape[0],1)), batch_size=512, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.models.save_model(M, './models/knowledge-graphj-embedding-analogic/model.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
