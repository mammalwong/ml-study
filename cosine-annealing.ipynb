{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train[:,:,:,np.newaxis]/255, X_test[:,:,:,np.newaxis]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 8)         208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 16)        3216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 14,414\n",
      "Trainable params: 14,412\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = X_input = keras.layers.Input(X_train.shape[1:])\n",
    "X = keras.layers.BatchNormalization()(X)\n",
    "X = keras.layers.Conv2D(8, (5,5), padding='same', activation='sigmoid')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Conv2D(16, (5,5), padding='same', activation='sigmoid')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Conv2D(32, (2,2), padding='valid', activation='sigmoid')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Conv2D(64, (2,2), padding='valid', activation='sigmoid')(X)\n",
    "X = keras.layers.MaxPooling2D()(X)\n",
    "X = keras.layers.Flatten()(X)\n",
    "X = keras.layers.Dense(np.max(Y_train)+1, activation='softmax')(X)\n",
    "M = keras.Model(X_input, X)\n",
    "M.compile('nadam', 'sparse_categorical_crossentropy', ['acc'])\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search for best cosine annealing max learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.002.\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.9203 - acc: 0.7157 - val_loss: 0.2474 - val_acc: 0.9341\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.004.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.1512 - acc: 0.9569 - val_loss: 0.0906 - val_acc: 0.9734\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.006.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0894 - acc: 0.9736 - val_loss: 0.0634 - val_acc: 0.9801\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.008.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0680 - acc: 0.9797 - val_loss: 0.0634 - val_acc: 0.9800\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.010000000000000002.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0615 - acc: 0.9809 - val_loss: 0.0432 - val_acc: 0.9857\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.012.\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0584 - acc: 0.9825 - val_loss: 0.0516 - val_acc: 0.9843\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.014000000000000002.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0597 - acc: 0.9814 - val_loss: 0.0425 - val_acc: 0.9868\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.016.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0632 - acc: 0.9800 - val_loss: 0.0818 - val_acc: 0.9737\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = M.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=64, epochs=50, callbacks=[\n",
    "    keras.callbacks.LearningRateScheduler(lambda epoch,lr: 0.1*((epoch+1)/50), verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='loss', patience=2, verbose=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.012.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0477 - acc: 0.9852 - val_loss: 0.0521 - val_acc: 0.9836\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.01079422863405995.\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0406 - acc: 0.9874 - val_loss: 0.0490 - val_acc: 0.9848\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0075000000000000015.\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0289 - acc: 0.9909 - val_loss: 0.0443 - val_acc: 0.9868\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0030000000000000005.\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0159 - acc: 0.9950 - val_loss: 0.0352 - val_acc: 0.9896\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.012.\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0460 - acc: 0.9853 - val_loss: 0.0610 - val_acc: 0.9817\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.011774351209636415.\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0430 - acc: 0.9860 - val_loss: 0.0523 - val_acc: 0.9850\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.011108719811121773.\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0355 - acc: 0.9894 - val_loss: 0.0478 - val_acc: 0.9857\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.01003648334221227.\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0323 - acc: 0.9896 - val_loss: 0.0491 - val_acc: 0.9858\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.008611408216728603.\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0270 - acc: 0.9913 - val_loss: 0.0407 - val_acc: 0.9879\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.006904953652058024.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0511 - val_acc: 0.9859\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.00500268840560683.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0423 - val_acc: 0.9879\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.0030000000000000005.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0361 - val_acc: 0.9898\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.012.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0539 - acc: 0.9831 - val_loss: 0.0620 - val_acc: 0.9832\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.01195069705831446.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0395 - acc: 0.9872 - val_loss: 0.0556 - val_acc: 0.9836\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.011803328406604252.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0339 - acc: 0.9890 - val_loss: 0.0512 - val_acc: 0.9854\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.011559508646656384.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0330 - acc: 0.9893 - val_loss: 0.0475 - val_acc: 0.9859\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.011221909118783409.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0340 - acc: 0.9889 - val_loss: 0.0492 - val_acc: 0.9853\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.01079422863405995.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.0532 - val_acc: 0.9832\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.010281152949374529.\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0282 - acc: 0.9910 - val_loss: 0.0473 - val_acc: 0.9877\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.009688303429296549.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0272 - acc: 0.9914 - val_loss: 0.0556 - val_acc: 0.9827\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.009022175457229725.\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0269 - acc: 0.9911 - val_loss: 0.0429 - val_acc: 0.9886\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.008290067270632258.\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0197 - acc: 0.9935 - val_loss: 0.0457 - val_acc: 0.9860\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.0075000000000000015.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0163 - acc: 0.9943 - val_loss: 0.0385 - val_acc: 0.9896\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.006660629787682204.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0496 - val_acc: 0.9888\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.005781152949374527.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0448 - val_acc: 0.9876\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.004871205217359833.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0085 - acc: 0.9973 - val_loss: 0.0399 - val_acc: 0.9891\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.003940756169408881.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.0369 - val_acc: 0.9901\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.0030000000000000005.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0371 - val_acc: 0.9907\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.012.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0671 - acc: 0.9800 - val_loss: 0.0535 - val_acc: 0.9845\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.011988448564539476.\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0391 - acc: 0.9880 - val_loss: 0.0658 - val_acc: 0.9816\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.011953823910527059.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0339 - acc: 0.9892 - val_loss: 0.0482 - val_acc: 0.9857\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.011896214918953003.\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0325 - acc: 0.9895 - val_loss: 0.0522 - val_acc: 0.9841\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.011815769471272452.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0343 - acc: 0.9894 - val_loss: 0.0430 - val_acc: 0.9864\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.011712694069795838.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0324 - acc: 0.9898 - val_loss: 0.0616 - val_acc: 0.9834\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.01158725330760044.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0421 - acc: 0.9870 - val_loss: 0.0450 - val_acc: 0.9870\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.011439769189323726.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0299 - acc: 0.9905 - val_loss: 0.0445 - val_acc: 0.9885\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.011270620304582077.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0298 - acc: 0.9912 - val_loss: 0.0465 - val_acc: 0.9869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/60\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.011080240856136677.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0287 - acc: 0.9905 - val_loss: 0.0538 - val_acc: 0.9841\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.010869119545301238.\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0548 - val_acc: 0.9863\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.010637798317452759.\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0271 - acc: 0.9912 - val_loss: 0.0488 - val_acc: 0.9855\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.010386870970865488.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0478 - val_acc: 0.9859\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.010116981632439289.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0228 - acc: 0.9925 - val_loss: 0.0598 - val_acc: 0.9838\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.009828823104235117.\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0252 - acc: 0.9916 - val_loss: 0.0537 - val_acc: 0.9852\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.009523135085062081.\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.0528 - val_acc: 0.9862\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.00920070227168118.\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0218 - acc: 0.9929 - val_loss: 0.0535 - val_acc: 0.9862\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.008862352344500002.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0230 - acc: 0.9924 - val_loss: 0.0579 - val_acc: 0.9839\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.008508953842928966.\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0506 - val_acc: 0.9870\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.008141413935853131.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0130 - acc: 0.9955 - val_loss: 0.0410 - val_acc: 0.9887\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.0077606760929426625.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0180 - acc: 0.9941 - val_loss: 0.0486 - val_acc: 0.9875\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.0073677176627797315.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0448 - val_acc: 0.9870\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.006963547364018709.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.0454 - val_acc: 0.9890\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.006549202696019867.\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0536 - val_acc: 0.9865\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.006125747275603383.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0108 - acc: 0.9962 - val_loss: 0.0490 - val_acc: 0.9873\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.005694268106760224.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0517 - val_acc: 0.9877\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.005255872790328485.\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0537 - val_acc: 0.9866\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.004811686680797941.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0454 - val_acc: 0.9891\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.0043628499975411905.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0485 - val_acc: 0.9895\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.00391051489788689.\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0464 - val_acc: 0.9892\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.003455842519548415.\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0421 - val_acc: 0.9894\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.0030000000000000005.\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 9.7316e-04 - acc: 0.9998 - val_loss: 0.0464 - val_acc: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e16ed3668>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_annealing(i, lr_min, lr_max):\n",
    "    i = i + np.power(2,2)\n",
    "    log_i = np.log2(i)\n",
    "    t_min,t_max = np.power(2,np.floor(log_i)), np.power(2,np.floor(log_i)+1)-1\n",
    "    return lr_min + np.cos(((i-t_min)/(t_max-t_min))*(np.pi/2))*(lr_max-lr_min)\n",
    "M.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=64, epochs=60, callbacks=[\n",
    "    keras.callbacks.LearningRateScheduler(lambda epoch,lr: cosine_annealing(epoch,0.012*0.25,0.012), verbose=1)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
