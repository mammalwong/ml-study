{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import multiprocessing\n",
    "import inspect\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \n",
    "    input_size = 8\n",
    "    output_size = input_size + 4\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_weights(N):\n",
    "        weights = {}\n",
    "        weights['top'] = {\n",
    "            'kernel': K.eval(keras.initializers.he_normal()((64,10))),\n",
    "            'bias': K.eval(keras.initializers.zeros()((10,)))}\n",
    "        l_size_last = 3\n",
    "        for l in range(5):\n",
    "            l_size = 2**(2+l)\n",
    "            weights[(l,'pre')] = {\n",
    "                'kernel': K.eval(keras.initializers.he_normal()(\n",
    "                    (1,1,l_size_last,l_size))),\n",
    "                'bias': K.eval(keras.initializers.zeros()((l_size,)))}\n",
    "            for i in range(N+1):\n",
    "                for j in range(i+1,N+1):\n",
    "                    weights[(l,i,j)] = {\n",
    "                        'kernel': K.eval(keras.initializers.he_normal()(\n",
    "                            (3,3,l_size,l_size))),\n",
    "                        'bias': K.eval(keras.initializers.zeros()((l_size,)))}\n",
    "            l_size_last = l_size\n",
    "        return weights\n",
    "        \n",
    "    @staticmethod\n",
    "    def run(instructions, weights):\n",
    "        (X_train,Y_train),(X_test,Y_test) = keras.datasets.cifar10.load_data()\n",
    "        X_train, X_test = X_train/255-0.5, X_test/255-0.5\n",
    "        X_train_idx = np.random.randint(0, high=X_train.shape[0], size=(4000,))\n",
    "        X_train,Y_train = X_train[X_train_idx], Y_train[X_train_idx]\n",
    "        X_test_idx = np.random.randint(0, high=X_test.shape[0], size=(1000,))\n",
    "        X_test,Y_test = X_test[X_test_idx], Y_test[X_test_idx]\n",
    "        X = X_input = keras.layers.Input(X_train.shape[1:])\n",
    "        weights_update = {}\n",
    "        for l in range(5):\n",
    "            w = weights[(l,'pre')]\n",
    "            o = keras.layers.Conv2D(\n",
    "                w['kernel'].shape[-1],\n",
    "                w['kernel'].shape[:2],\n",
    "                padding='same',\n",
    "                kernel_initializer=keras.initializers.constant(value=w['kernel']),\n",
    "                bias_initializer=keras.initializers.constant(value=w['bias']))\n",
    "            weights_update[(l,'pre')] = o\n",
    "            X = o(X)\n",
    "            layers = {0:X}\n",
    "            connected = set()\n",
    "            for i,ins in enumerate(instructions):\n",
    "                previous_layer_a_idx = ins['previous_layer_a']\n",
    "                if previous_layer_a_idx is not None:\n",
    "                    connected.add(previous_layer_a_idx)\n",
    "                    w = weights[(l,previous_layer_a_idx,i+1)]\n",
    "                    previous_layer_a = layers[previous_layer_a_idx]\n",
    "                    o = keras.layers.Conv2D(\n",
    "                        w['kernel'].shape[-1],\n",
    "                        w['kernel'].shape[:2],\n",
    "                        padding='same',\n",
    "                        kernel_initializer= \\\n",
    "                            keras.initializers.constant(value=w['kernel']),\n",
    "                        bias_initializer= \\\n",
    "                            keras.initializers.constant(value=w['bias']))\n",
    "                    weights_update[(l,previous_layer_a_idx,i+1)] = o\n",
    "                    previous_layer_a = o(previous_layer_a)\n",
    "                previous_layer_b_idx = ins['previous_layer_b']\n",
    "                if previous_layer_b_idx is not None and \\\n",
    "                    previous_layer_b_idx == previous_layer_a_idx:\n",
    "                    previous_layer_b = previous_layer_a\n",
    "                elif previous_layer_b_idx is not None:\n",
    "                    connected.add(previous_layer_b_idx)\n",
    "                    w = weights[(l,previous_layer_b_idx,i+1)]\n",
    "                    previous_layer_b = layers[previous_layer_b_idx]\n",
    "                    o = keras.layers.Conv2D(\n",
    "                        w['kernel'].shape[-1],\n",
    "                        w['kernel'].shape[:2],\n",
    "                        padding='same',\n",
    "                        kernel_initializer= \\\n",
    "                            keras.initializers.constant(value=w['kernel']),\n",
    "                        bias_initializer= \\\n",
    "                            keras.initializers.constant(value=w['bias']))\n",
    "                    weights_update[(l,previous_layer_b_idx,i+1)] = o\n",
    "                    previous_layer_b = o(previous_layer_b)\n",
    "                if previous_layer_a_idx is None and \\\n",
    "                    previous_layer_b_idx is None:\n",
    "                    continue\n",
    "                if previous_layer_a_idx is not None and \\\n",
    "                    previous_layer_b_idx is not None:\n",
    "                    previous_layer_ab = [previous_layer_a,previous_layer_b]\n",
    "                    merge_method = ins['merge_method']\n",
    "                    if merge_method == 'add':\n",
    "                        X = keras.layers.Add()(previous_layer_ab)\n",
    "                    elif merge_method == 'sub':\n",
    "                        X = keras.layers.Subtract()(previous_layer_ab)\n",
    "                    elif merge_method == 'mul':\n",
    "                        X = keras.layers.Multiply()(previous_layer_ab)\n",
    "                    elif merge_method == 'mul_sigmoid(a)':\n",
    "                        X = keras.layers.Multiply()([\n",
    "                            keras.layers.Lambda(lambda x: K.sigmoid(x))(\n",
    "                                keras.layers.BatchNormalization()(previous_layer_a)),\n",
    "                            previous_layer_b])\n",
    "                    elif merge_method == 'mul_1-sigmoid(a)':\n",
    "                        X = keras.layers.Multiply()([\n",
    "                            keras.layers.Lambda(lambda x: 1.-K.sigmoid(x))(\n",
    "                                keras.layers.BatchNormalization()(previous_layer_a)),\n",
    "                            previous_layer_b])\n",
    "                    elif merge_method == 'avg':\n",
    "                        X = keras.layers.Average()(previous_layer_ab)\n",
    "                    else:\n",
    "                        raise Exception('unknown merge method')\n",
    "                elif previous_layer_a_idx is not None:\n",
    "                    X = previous_layer_a\n",
    "                elif previous_layer_b_idx is not None:\n",
    "                    X = previous_layer_b\n",
    "                X = keras.layers.BatchNormalization()(X)\n",
    "                X = keras.layers.Activation('relu')(X)\n",
    "                layers[i+1] = X\n",
    "            not_connected = set(layers.keys()) - connected\n",
    "            not_connected = [layers[nc] for nc in not_connected]\n",
    "            if not not_connected:\n",
    "                raise Exception('no output')\n",
    "            elif len(not_connected) == 1:\n",
    "                X = list(not_connected)[0]\n",
    "            else:\n",
    "                X = keras.layers.Average()(list(not_connected))\n",
    "            X = keras.layers.MaxPooling2D()(X)\n",
    "        X = keras.layers.GlobalAveragePooling2D()(X)\n",
    "        w = weights['top']\n",
    "        o = keras.layers.Dense(\n",
    "            w['kernel'].shape[-1],\n",
    "            kernel_initializer=keras.initializers.constant(value=w['kernel']),\n",
    "            bias_initializer=keras.initializers.constant(value=w['bias']),\n",
    "            activation='softmax')\n",
    "        weights_update['top'] = o\n",
    "        X = o(X)\n",
    "        M = keras.Model(X_input, X)\n",
    "        M_optimizer = keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "        M.compile(M_optimizer, 'sparse_categorical_crossentropy', ['acc'])\n",
    "        hist = M.fit(\n",
    "            X_train, Y_train,\n",
    "            validation_data=(X_test,Y_test),\n",
    "            batch_size=64, epochs=1)\n",
    "        acc = hist.history['val_acc'][-1]\n",
    "        acc = acc**2 * 100 if acc == acc else 0.\n",
    "        for k,v in weights_update.items():\n",
    "            w = v.get_weights()\n",
    "            if not np.any(np.isnan(w[0])):\n",
    "                weights[k]['kernel'] = w[0]\n",
    "            if not np.any(np.isnan(w[1])):\n",
    "                weights[k]['bias'] = w[1]\n",
    "        return acc, weights\n",
    "    \n",
    "    def __init__(self, N=6):\n",
    "        self.N = N\n",
    "        p = multiprocessing.Pool(processes=1)\n",
    "        try:\n",
    "            self.weights = p.apply(__class__.create_weights, [self.N])\n",
    "        finally:\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "        self.reset()\n",
    "    \n",
    "    def run_in_process(self, instructions):\n",
    "        print([tuple(i.values()) for i in instructions])\n",
    "        p = multiprocessing.Pool(processes=1)\n",
    "        try:\n",
    "            acc, weights = p.apply(\n",
    "                __class__.run,\n",
    "                [instructions, self.weights])\n",
    "        except:\n",
    "            acc, weights = 0., self.weights\n",
    "        finally:\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "        self.weights = weights\n",
    "        return acc\n",
    "    \n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.instructions = []\n",
    "        return np.zeros((__class__.output_size,))\n",
    "    \n",
    "    def step(self, action):\n",
    "        result = np.zeros((__class__.output_size),)\n",
    "        result[action] = 1\n",
    "        result[__class__.input_size+2] = len(self.actions)%3\n",
    "        result[__class__.input_size+3] = len(self.instructions)\n",
    "        if len(self.actions) < 3*self.N:\n",
    "            if len(self.actions) % 3 == 0:\n",
    "                self.instructions.append({})\n",
    "                options = [None] + list(range(len(self.instructions)))\n",
    "                self.instructions[-1]['previous_layer_a'] = \\\n",
    "                    options[action % len(options)]\n",
    "            elif len(self.actions) % 3 == 1:\n",
    "                options = [None] + list(range(len(self.instructions)))\n",
    "                self.instructions[-1]['previous_layer_b'] = \\\n",
    "                    options[action % len(options)]\n",
    "            elif len(self.actions) % 3 == 2:\n",
    "                options = ['add','mul_sigmoid(a)','mul_1-sigmoid(a)']\n",
    "                self.instructions[-1]['merge_method'] = \\\n",
    "                    options[action % len(options)]\n",
    "            self.actions.append(action)\n",
    "        if len(self.actions) >= 3*self.N:\n",
    "            score = self.run_in_process(self.instructions)\n",
    "            score = score * 10. if score > 0. else -10.\n",
    "            result[__class__.input_size] = score\n",
    "            result[__class__.input_size+1] = 1.\n",
    "            return result, score, True, {}\n",
    "        result[__class__.input_size] = 0.\n",
    "        result[__class__.input_size+1] = 0.\n",
    "        return result, 0., False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Env()\n",
    "env_input_size = Env.input_size\n",
    "env_output_size = Env.output_size\n",
    "env_prev_states = env.N * 3\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model_Sample = collections.namedtuple('Model_Sample',\n",
    "    ['state', 'state_1', 'action', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Controller:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, beta=0.0001, curiosity=1.):\n",
    "        \n",
    "        self.__curiosity = curiosity\n",
    "        \n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__l_shared = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.LSTM(32, return_sequences=False)]\n",
    "        self.__l_policy = [\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n),\n",
    "            keras.layers.Softmax()]\n",
    "        self.__l_value = [\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        self.__l_cfeature = [\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(32, activation='tanh')]\n",
    "        self.__l_cforward = [\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(32, activation='tanh')]\n",
    "        self.__l_creverse = [\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n, activation='softmax')]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_policy)\n",
    "        self.__m_policy = keras.models.Model([m_input], [m])\n",
    "        self.__m_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_value)\n",
    "        self.__m_value = keras.models.Model([m_input], [m])\n",
    "        self.__m_value.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared = apply_layers(m, self.__l_shared)\n",
    "        m_policy = apply_layers(m_shared, self.__l_policy)\n",
    "        m_value = apply_layers(m_shared, self.__l_value)\n",
    "        self.__m_value_policy = keras.models.Model([m_input], [m_value, m_policy])\n",
    "        self.__m_value_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m_input_s = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared_s = apply_layers(m_input_s, self.__l_shared)\n",
    "        m_shared_s = apply_layers(m_shared_s, self.__l_cfeature)\n",
    "        self.__m_cfeature = keras.models.Model([m_input_s], [m_shared_s])\n",
    "        self.__m_cfeature.compile('adam', 'mse')\n",
    "        \n",
    "        m_input_s = keras.layers.Input((None, self.__input_n,))\n",
    "        m_input_s_1 = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared_s = self.__m_cfeature(m_input_s)\n",
    "        m_shared_s_1 = self.__m_cfeature(m_input_s_1)\n",
    "        m = keras.layers.Concatenate()([m_shared_s, m_shared_s_1])\n",
    "        m = apply_layers(m, self.__l_creverse)\n",
    "        self.__m_creverse = keras.models.Model(\n",
    "            [m_input_s, m_input_s_1], [m])\n",
    "        self.__m_creverse.compile('adam', 'mse')\n",
    "        \n",
    "        m_input_s = keras.layers.Input((None, self.__input_n,))\n",
    "        m_input_s_1 = keras.layers.Input((None, self.__input_n,))\n",
    "        m_input_a = keras.layers.Input((self.__output_n,))\n",
    "        m_shared_s = self.__m_cfeature(m_input_s)\n",
    "        m_shared_s = keras.layers.Concatenate()([m_shared_s, m_input_a])\n",
    "        m_shared_s = apply_layers(m_shared_s, self.__l_cforward)\n",
    "        m_shared_s_1 = self.__m_cfeature(m_input_s_1)\n",
    "        m = keras.layers.Lambda(\n",
    "            lambda x: K.sum(K.square(x[0]-x[1]), axis=-1, keepdims=True)/2,\n",
    "            output_shape=(1,))([m_shared_s, m_shared_s_1])\n",
    "        self.__m_cforward = keras.models.Model(\n",
    "            [m_input_s, m_input_s_1, m_input_a], [m])\n",
    "        self.__m_cforward.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_value, m_policy = self.__m_value_policy(m)\n",
    "        m = keras.layers.Concatenate()([m_value, m_policy])\n",
    "        m_input_s_1 = keras.layers.Input((None, self.__input_n,))\n",
    "        m_input_a = keras.layers.Input((self.__output_n,))\n",
    "        m_cforward = self.__m_cforward([m_input, m_input_s_1, m_input_a])\n",
    "        m_creverse = self.__m_creverse([m_input, m_input_s_1])\n",
    "        self.__m_train = keras.models.Model(\n",
    "            [m_input, m_input_s_1, m_input_a], [m, m_cforward, m_creverse])\n",
    "        self.__m_train.compile('nadam', [\n",
    "            lambda y_true, y_pred: __class__.__loss(y_true, y_pred, beta),\n",
    "            'mse', 'categorical_crossentropy'])\n",
    "        \n",
    "        self.__m_policy.summary()\n",
    "        self.__m_value.summary()\n",
    "        self.__m_train.summary()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __loss(y_true, y_pred, beta):\n",
    "        r, action_onehot = y_true[:,:1], y_true[:,1:]\n",
    "        value, policy = y_pred[:,:1], y_pred[:,1:]\n",
    "        advantage = r - value\n",
    "        log_policy = K.log(policy + K.epsilon())\n",
    "        log_choosen_action_prob = K.sum(\n",
    "            action_onehot * log_policy, axis=-1, keepdims=True)\n",
    "        action_loss = -K.mean(log_choosen_action_prob * advantage)\n",
    "        value_loss = 0.5 * K.mean(K.square(advantage))\n",
    "        entropy = K.mean(-K.sum(policy * log_policy, axis=-1, keepdims=True))\n",
    "        return action_loss + value_loss - beta * entropy\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        self.__m_train.fit(\n",
    "            x=[\n",
    "                np.array([s.state for s in samples], dtype=np.float32),\n",
    "                np.array([s.state_1 for s in samples], dtype=np.float32),\n",
    "                keras.utils.to_categorical([\n",
    "                    s.action for s in samples], num_classes=self.__output_n)],\n",
    "            y=[\n",
    "                np.hstack([\n",
    "                    np.reshape(np.array([\n",
    "                        s.reward for s in samples], dtype=np.float32), (-1, 1)),\n",
    "                    keras.utils.to_categorical([\n",
    "                        s.action for s in samples], num_classes=self.__output_n)]),\n",
    "                np.zeros((len(samples),1)),\n",
    "                keras.utils.to_categorical([\n",
    "                    s.action for s in samples], num_classes=self.__output_n)],\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "    \n",
    "    def evalute_value(self, state, verbose=False):\n",
    "        v = self.__m_value.predict(\n",
    "            np.array([state], dtype=np.float32))[0,0]\n",
    "        if verbose:\n",
    "            print(v)\n",
    "        return v\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_policy.predict(\n",
    "            np.array([state], dtype=np.float32))[0]\n",
    "        if verbose:\n",
    "            print(action_prob)\n",
    "        return action_prob\n",
    "    \n",
    "    def get_intrinsic_reward(self, state_0, state_1, action, verbose=False):\n",
    "        r = self.__m_cforward.predict([\n",
    "            np.array([state_0], dtype=np.float32),\n",
    "            np.array([state_1], dtype=np.float32),\n",
    "            keras.utils.to_categorical([action], num_classes=self.__output_n)\n",
    "        ])[0,0] * self.__curiosity\n",
    "        if verbose:\n",
    "            print(r)\n",
    "        return r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiprocessController:\n",
    "    \n",
    "    @staticmethod\n",
    "    def run(pipe, init_args, init_kwargs):\n",
    "        import os\n",
    "        # enforce to run on cpu\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "        controller = Controller(*init_args, **init_kwargs)\n",
    "        pipe.send('controller inited')\n",
    "        while True:\n",
    "            cmd = pipe.recv()\n",
    "            if cmd[0] == 'train':\n",
    "                pipe.send(controller.train(*cmd[1], **cmd[2]))\n",
    "            elif cmd[0] == 'evalute_value':\n",
    "                pipe.send(controller.evalute_value(*cmd[1], **cmd[2]))\n",
    "            elif cmd[0] == 'get_action_prob':\n",
    "                pipe.send(controller.get_action_prob(*cmd[1], **cmd[2]))\n",
    "            elif cmd[0] == 'get_intrinsic_reward':\n",
    "                pipe.send(controller.get_intrinsic_reward(*cmd[1], **cmd[2]))\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pipe_l, pipe_r = multiprocessing.Pipe(duplex=True)\n",
    "        self.pipe = pipe_l\n",
    "        self.process = multiprocessing.Process(\n",
    "            target=__class__.run,\n",
    "            args=[pipe_r, args, kwargs],\n",
    "            daemon=True)\n",
    "        self.process.start()\n",
    "        print(self.pipe.recv())\n",
    "    \n",
    "    def train(self, *args, **kwargs):\n",
    "        self.pipe.send(('train', args, kwargs))\n",
    "        return self.pipe.recv()\n",
    "        \n",
    "    def evalute_value(self, *args, **kwargs):\n",
    "        self.pipe.send(('evalute_value', args, kwargs))\n",
    "        return self.pipe.recv()\n",
    "        \n",
    "    def get_action_prob(self, *args, **kwargs):\n",
    "        self.pipe.send(('get_action_prob', args, kwargs))\n",
    "        return self.pipe.recv()\n",
    "    \n",
    "    def get_intrinsic_reward(self, *args, **kwargs):\n",
    "        self.pipe.send(('get_intrinsic_reward', args, kwargs))\n",
    "        return self.pipe.recv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, gamma=0.98, max_steps=1000,\n",
    "         n_prev_states=8, epsilon=0., verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    rewards_intrinsic = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx, get_state_1=False):\n",
    "        states = [e.state if not get_state_1 else e.state_1 \\\n",
    "                  for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode, done):\n",
    "        if done:\n",
    "            discounted_reward = 0.\n",
    "        else:\n",
    "            discounted_reward = m.evalute_value(\n",
    "                get_prev_states(episode, len(episode)-1))\n",
    "        episode[-1] = Model_Sample(\n",
    "                get_prev_states(episode, len(episode)-1),\n",
    "                get_prev_states(episode, len(episode)-1, get_state_1=True),\n",
    "                episode[-1].action,\n",
    "                discounted_reward)\n",
    "        for i in reversed(range(len(episode)-1)):\n",
    "            discounted_reward = episode[i].reward + \\\n",
    "                gamma * discounted_reward\n",
    "            episode[i] = Model_Sample(\n",
    "                get_prev_states(episode, i),\n",
    "                get_prev_states(episode, i, get_state_1=True),\n",
    "                episode[i].action,\n",
    "                discounted_reward)\n",
    "        samples.extend(episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob = m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(action_prob)\n",
    "        action_prob = epsilon/action_prob.shape[-1] + (1.-epsilon)*action_prob\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state_1, reward, done, _ = env.step(action)\n",
    "        state_1_queue = state_queue + [state_1]\n",
    "        if len(state_1_queue) > n_prev_states:\n",
    "            state_1_queue.pop(0)\n",
    "        state_1_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_1_queue))) + state_1_queue\n",
    "        reward_intrinsic = m.get_intrinsic_reward(\n",
    "            state_queue_padded, state_1_queue_padded, action)\n",
    "        reward += reward_intrinsic\n",
    "        rewards_intrinsic.append(reward_intrinsic)\n",
    "        episode.append(Model_Sample(state_0, state_1, action, reward))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode, True)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "            if verbose:\n",
    "                print('e[reward_intrinsic]', np.mean(rewards_intrinsic))\n",
    "            rewards_intrinsic = []\n",
    "    if episode:\n",
    "        add_to_samples(episode, False)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "        if verbose:\n",
    "            print('e[reward_intrinsic]', np.mean(rewards_intrinsic))\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 12)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 12)          48        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                5760      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 7,256\n",
      "Trainable params: 7,168\n",
      "Non-trainable params: 88\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 12)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 12)          48        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                5760      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,025\n",
      "Trainable params: 6,937\n",
      "Non-trainable params: 88\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, None, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 [(None, 1), (None, 8 8473        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, None, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 9)            0           model_3[1][0]                    \n",
      "                                                                 model_3[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_6 (Model)                 (None, 1)            24048       input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Model)                 (None, 8)            24024       input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 36,065\n",
      "Trainable params: 35,145\n",
      "Non-trainable params: 920\n",
      "__________________________________________________________________________________________________\n",
      "controller inited\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 5s 87ms/step - loss: 136.3753 - concatenate_3_loss: 3.7281 - model_6_loss: 130.3104 - model_5_loss: 2.3368\n",
      "Epoch 1/1\n",
      "54/54 [==============================] - 0s 497us/step - loss: 185.4956 - concatenate_3_loss: 84.7460 - model_6_loss: 98.3089 - model_5_loss: 2.4408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-bbb0d6bdda28>\", line 12, in run\n",
      "    cmd = pipe.recv()\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib64/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "m = MultiprocessController(env_output_size, env_input_size, beta=1e-3, curiosity=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, None, 'mul_sigmoid(a)'), (0, 0, 'mul_1-sigmoid(a)'), (2, 2, 'add'), (3, 2, 'mul_sigmoid(a)'), (0, 4, 'mul_sigmoid(a)'), (4, 0, 'mul_1-sigmoid(a)')]\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 14s 3ms/step - loss: 2.2133 - acc: 0.1840 - val_loss: 2.2506 - val_acc: 0.2400\n",
      "e[reward_intrinsic] 0.18117723796102736\n",
      "[(None, None, 'add'), (None, 1, 'mul_sigmoid(a)'), (None, 2, 'mul_sigmoid(a)'), (0, None, 'mul_1-sigmoid(a)'), (0, 1, 'mul_sigmoid(a)'), (3, 5, 'mul_sigmoid(a)')]\n",
      "e[reward_intrinsic] 0.1615322146150801\n",
      "[(None, None, 'mul_sigmoid(a)'), (1, 0, 'mul_sigmoid(a)'), (2, 2, 'mul_1-sigmoid(a)'), (3, 1, 'add'), (None, 0, 'mul_1-sigmoid(a)'), (1, 5, 'mul_1-sigmoid(a)')]\n",
      "e[reward_intrinsic] 0.165747583243582\n",
      "std[action_prob] 0.012808673\n",
      "max game len 18\n",
      "epoch 1 completed\n",
      "[(0, None, 'add'), (None, None, 'add'), (1, None, 'add'), (1, None, 'add'), (None, 0, 'add'), (5, 2, 'mul_sigmoid(a)')]\n",
      "e[reward_intrinsic] 1.443660746680366\n",
      "[(None, None, 'add'), (None, None, 'add'), (1, 0, 'add'), (0, 0, 'mul_sigmoid(a)'), (3, 3, 'mul_sigmoid(a)'), (5, 2, 'mul_sigmoid(a)')]\n",
      "e[reward_intrinsic] 1.5806603696611192\n",
      "[(None, 0, 'add'), (None, None, 'mul_1-sigmoid(a)'), (None, 0, 'add'), (0, 1, 'mul_sigmoid(a)'), (1, 3, 'add'), (5, 2, 'mul_sigmoid(a)')]\n",
      "e[reward_intrinsic] 1.3198573854234483\n",
      "std[action_prob] 0.07882458\n",
      "max game len 18\n",
      "epoch 2 completed\n",
      "[(0, 0, 'mul_sigmoid(a)'), (0, None, 'add'), (1, 2, 'mul_1-sigmoid(a)'), (1, 0, 'add'), (0, None, 'mul_sigmoid(a)'), (None, None, 'add')]\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 2.1418 - acc: 0.2115 - val_loss: 2.3999 - val_acc: 0.1760\n",
      "e[reward_intrinsic] 1.0904617044660783\n",
      "[(0, 0, 'mul_sigmoid(a)'), (0, 0, 'add'), (2, 0, 'mul_sigmoid(a)'), (None, 0, 'mul_sigmoid(a)'), (None, None, 'add'), (3, 3, 'add')]\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 2.0275 - acc: 0.2450 - val_loss: 1.9728 - val_acc: 0.2890\n",
      "e[reward_intrinsic] 1.1998025523291693\n",
      "[(0, 0, 'mul_sigmoid(a)'), (0, 0, 'mul_1-sigmoid(a)'), (2, 0, 'mul_1-sigmoid(a)'), (2, 1, 'add'), (0, 3, 'mul_sigmoid(a)'), (None, 5, 'mul_1-sigmoid(a)')]\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-67d593fafe14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     samples = play(env, m, max_steps=env.N*3*3,\n\u001b[0;32m----> 4\u001b[0;31m         n_prev_states=env_prev_states, epsilon=0.1, verbose=True)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {} completed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-fb04f4efbe80>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, m, gamma, max_steps, n_prev_states, epsilon, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstate_1_queue_padded\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mstate_null\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_prev_states\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_1_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_1_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         reward_intrinsic = m.get_intrinsic_reward(\n\u001b[0;32m---> 52\u001b[0;31m             state_queue_padded, state_1_queue_padded, action)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward_intrinsic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mrewards_intrinsic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_intrinsic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bbb0d6bdda28>\u001b[0m in \u001b[0;36mget_intrinsic_reward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_intrinsic_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get_intrinsic_reward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "replays = []\n",
    "for i in range(500):\n",
    "    samples = play(env, m, max_steps=env.N*3*3,\n",
    "        n_prev_states=env_prev_states, epsilon=0.1, verbose=True)\n",
    "    m.train(samples, epochs=1, verbose=True)\n",
    "    print('epoch {} completed'.format(i+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
