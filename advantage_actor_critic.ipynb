{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 112 114 115  48   3  88  88  88  88  88   0  80  80  80  50  98   0\n",
      "   0   3   0   0   1   0   0   1   6   6 198   4  63   0  45   1   0 198\n",
      " 198   0   0   0   0  32  52   0   0 120   0 100 130   0   0 134   1 222\n",
      "   0   1   3   0   6  80 255 255   0 255 255  80 255 255  80 255 255  80\n",
      " 255 255  80 191 191  80 191 191  80 191 191  80 255 255  80 255 255  80\n",
      " 255 255  80 255 255   0 255 255  80 255 255  20 223  43 217 123 217 123\n",
      " 217 123 217 123 217 123 217 221   0  63   0   0   0   0   0   2  66 240\n",
      " 146 215]\n",
      "Box(128,)\n",
      "Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "def env_create():\n",
    "    env = gym.make('MsPacman-ram-v0')\n",
    "    return env\n",
    "env = env_create()\n",
    "env_max_possible_score = -1\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'action', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, beta=0.0001):\n",
    "        \n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__l_shared = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Conv1D(128, 3,\n",
    "                padding='same', kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Conv1D(128, 3,\n",
    "                padding='same', kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Conv1D(128, 3,\n",
    "                padding='same', kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False),\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu')]\n",
    "        self.__l_policy = [\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n, activation='softmax')]\n",
    "        self.__l_value = [\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_policy)\n",
    "        self.__m_policy = keras.models.Model([m_input], [m])\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_value)\n",
    "        self.__m_value = keras.models.Model([m_input], [m])\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared = apply_layers(m, self.__l_shared)\n",
    "        m_policy = apply_layers(m_shared, self.__l_policy)\n",
    "        m_value = apply_layers(m_shared, self.__l_value)\n",
    "        self.__m_value_policy = keras.models.Model([m_input], [m_value, m_policy])\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_value, m_policy = self.__m_value_policy(m)\n",
    "        m = keras.layers.Concatenate()([m_value, m_policy])\n",
    "        self.__m_train = keras.models.Model([m_input], [m])\n",
    "        self.__m_train.compile('nadam',\n",
    "            lambda y_true, y_pred: model.__loss(y_true, y_pred, beta))\n",
    "        \n",
    "        self.__m_policy.summary()\n",
    "        self.__m_value.summary()\n",
    "        self.__m_train.summary()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __loss(y_true, y_pred, beta):\n",
    "        r, action_onehot = y_true[:,:1], y_true[:,1:]\n",
    "        value, policy = y_pred[:,:1], y_pred[:,1:]\n",
    "        advantage = r - value\n",
    "        log_policy = K.log(policy + K.epsilon())\n",
    "        log_choosen_action_prob = K.sum(action_onehot * log_policy, axis=-1, keepdims=True)\n",
    "        action_loss = -K.mean(log_choosen_action_prob * advantage)\n",
    "        value_loss = 0.5 * K.mean(K.square(advantage))\n",
    "        entropy = K.mean(-K.sum(policy * log_policy, axis=-1, keepdims=True))\n",
    "        return action_loss + value_loss - beta * entropy\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        self.__m_train.fit(\n",
    "            x=np.array([s.state for s in samples], dtype=np.float32),\n",
    "            y=np.hstack([\n",
    "                np.reshape(np.array([s.reward for s in samples], dtype=np.float32), (-1, 1)),\n",
    "                keras.utils.to_categorical([s.action for s in samples], num_classes=self.__output_n)]),\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "    \n",
    "    def evalute_value(self, state, verbose=False):\n",
    "        v = self.__m_value.predict(\n",
    "            np.array([state], dtype=np.float32))[0,0]\n",
    "        if verbose:\n",
    "            print(v)\n",
    "        return v\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_policy.predict(\n",
    "            np.array([state], dtype=np.float32))[0]\n",
    "        if verbose:\n",
    "            print(action_prob)\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, gamma=0.98, max_steps=1000, n_prev_states=8, verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx):\n",
    "        states = [e.state for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode, done):\n",
    "        if done:\n",
    "            discounted_reward = 0.\n",
    "        else:\n",
    "            discounted_reward = m.evalute_value(get_prev_states(episode, len(episode)-1))\n",
    "        episode[-1] = model_sample(\n",
    "                get_prev_states(episode, len(episode)-1),\n",
    "                episode[-1].action,\n",
    "                discounted_reward)\n",
    "        for i in reversed(range(len(episode)-1)):\n",
    "            discounted_reward = episode[i].reward + \\\n",
    "                gamma * discounted_reward\n",
    "            episode[i] = model_sample(\n",
    "                get_prev_states(episode, i),\n",
    "                episode[i].action,\n",
    "                discounted_reward)\n",
    "        samples.extend(episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob = m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(action_prob)\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state_1, reward, done, _ = env.step(action)\n",
    "        episode.append(model_sample(state_0, action, reward))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode, True)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode, False)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 333,705\n",
      "Trainable params: 331,913\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 332,673\n",
      "Trainable params: 330,881\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, 128)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 [(None, 1), (None, 9 367882      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10)           0           model_3[1][0]                    \n",
      "                                                                 model_3[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 367,882\n",
      "Trainable params: 365,578\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.n, beta=20.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(m, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    while True:\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action_prob = m.get_action_prob(state_queue)\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/60)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = env_create()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std[action_prob] 0.01314588\n",
      "max game len 703\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 17.6535\n",
      "epoch 0 completed\n",
      "std[action_prob] 0.012818041\n",
      "max game len 799\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 1s 487us/step - loss: -12.7411\n",
      "epoch 1 completed\n",
      "std[action_prob] 0.012864577\n",
      "max game len 642\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 1s 480us/step - loss: -19.6922\n",
      "epoch 2 completed\n",
      "std[action_prob] 0.012848404\n",
      "max game len 661\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 2s 486us/step - loss: -25.3716\n",
      "epoch 3 completed\n",
      "std[action_prob] 0.016418105\n",
      "max game len 547\n",
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 2s 490us/step - loss: -19.4635\n",
      "epoch 4 completed\n",
      "test result mean 366.6666666666667 std 305.9956426705017\n",
      "std[action_prob] 0.02443367\n",
      "max game len 700\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 3s 496us/step - loss: -20.2721\n",
      "epoch 5 completed\n",
      "std[action_prob] 0.019466408\n",
      "max game len 512\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 3s 491us/step - loss: -20.9639\n",
      "epoch 6 completed\n",
      "std[action_prob] 0.022860568\n",
      "max game len 1000\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 4s 500us/step - loss: 37.5437\n",
      "epoch 7 completed\n",
      "std[action_prob] 0.021522466\n",
      "max game len 793\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 5s 507us/step - loss: -4.0988\n",
      "epoch 8 completed\n",
      "std[action_prob] 0.028374549\n",
      "max game len 651\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 5s 507us/step - loss: 20.1502\n",
      "epoch 9 completed\n",
      "test result mean 550.0 std 294.6183972531247\n",
      "std[action_prob] 0.0945079\n",
      "max game len 531\n",
      "Epoch 1/1\n",
      "11000/11000 [==============================] - 6s 500us/step - loss: -14.5094\n",
      "epoch 10 completed\n",
      "std[action_prob] 0.09540895\n",
      "max game len 757\n",
      "Epoch 1/1\n",
      "12000/12000 [==============================] - 6s 515us/step - loss: -19.2241\n",
      "epoch 11 completed\n",
      "std[action_prob] 0.10248032\n",
      "max game len 582\n",
      "Epoch 1/1\n",
      "13000/13000 [==============================] - 7s 502us/step - loss: -41.9026\n",
      "epoch 12 completed\n",
      "std[action_prob] 0.05406189\n",
      "max game len 749\n",
      "Epoch 1/1\n",
      "14000/14000 [==============================] - 7s 508us/step - loss: 711.6162\n",
      "epoch 13 completed\n",
      "std[action_prob] 3.8535005e-11\n",
      "max game len 628\n",
      "Epoch 1/1\n",
      "15000/15000 [==============================] - 8s 501us/step - loss: 313.4895\n",
      "epoch 14 completed\n",
      "test result mean 210.0 std 0.0\n",
      "std[action_prob] 7.1776218e-12\n",
      "max game len 617\n",
      "Epoch 1/1\n",
      "16000/16000 [==============================] - 8s 494us/step - loss: 274.9048\n",
      "epoch 15 completed\n",
      "std[action_prob] 2.6662997e-10\n",
      "max game len 637\n",
      "Epoch 1/1\n",
      "17000/17000 [==============================] - 9s 502us/step - loss: 91.7351\n",
      "epoch 16 completed\n",
      "std[action_prob] 1.9793982e-11\n",
      "max game len 633\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 9s 505us/step - loss: 89.6294\n",
      "epoch 17 completed\n",
      "std[action_prob] 1.3885936e-11\n",
      "max game len 633\n",
      "Epoch 1/1\n",
      "19000/19000 [==============================] - 10s 501us/step - loss: 140.3593\n",
      "epoch 18 completed\n",
      "std[action_prob] 6.6301084e-12\n",
      "max game len 628\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 10s 504us/step - loss: 35.5269\n",
      "epoch 19 completed\n"
     ]
    }
   ],
   "source": [
    "replays = []\n",
    "for i in range(100):\n",
    "    samples = play(\n",
    "        env, m, max_steps=1000, n_prev_states=8, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    m.train(replays, epochs=1, verbose=True)\n",
    "    if len(replays) > 100000:\n",
    "        random.shuffle(replays)\n",
    "        replays = replays[:100000]\n",
    "    print('epoch {} completed'.format(i))\n",
    "    if i % 5 != 4:\n",
    "        continue\n",
    "    test_result = [test(m, n_prev_states=8, render=(i==0)) for i in range(3)]\n",
    "    print('test result',\n",
    "          'mean', np.mean(test_result),\n",
    "          'std', np.std(test_result, ddof=1))\n",
    "    if np.mean(test_result) == env_max_possible_score and \\\n",
    "        np.std(test_result, ddof=1) == 0:\n",
    "        print('the network always gets full score, early exit')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(m, n_prev_states=8, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
