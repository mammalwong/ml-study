{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.virtualenvs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.03666234 -0.0134965   0.03547303  0.01610701]\n",
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env_max_possible_score = 200\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'action', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, beta=0.0001):\n",
    "        \n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__l_shared = [\n",
    "            keras.layers.LSTM(16, return_sequences=False)]\n",
    "        self.__l_policy = [\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Softmax()]\n",
    "        self.__l_value = [\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_policy)\n",
    "        self.__m_policy = keras.models.Model([m_input], [m])\n",
    "        self.__m_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, self.__l_shared)\n",
    "        m = apply_layers(m, self.__l_value)\n",
    "        self.__m_value = keras.models.Model([m_input], [m])\n",
    "        self.__m_value.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_shared = apply_layers(m, self.__l_shared)\n",
    "        m_policy = apply_layers(m_shared, self.__l_policy)\n",
    "        m_value = apply_layers(m_shared, self.__l_value)\n",
    "        self.__m_value_policy = keras.models.Model([m_input], [m_value, m_policy])\n",
    "        self.__m_value_policy.compile('adam', 'mse')\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m_value, m_policy = self.__m_value_policy(m)\n",
    "        m = keras.layers.Concatenate()([m_value, m_policy])\n",
    "        self.__m_optimizer = keras.optimizers.Adam(clipnorm=5.)\n",
    "        self.__m_train = keras.models.Model([m_input], [m])\n",
    "        self.__m_train.compile(self.__m_optimizer,\n",
    "            lambda y_true, y_pred: model.__loss(y_true, y_pred, beta))\n",
    "        \n",
    "        self.__m_policy.summary()\n",
    "        self.__m_value.summary()\n",
    "        self.__m_train.summary()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __loss(y_true, y_pred, beta):\n",
    "        r, action_onehot = y_true[:,:1], y_true[:,1:]\n",
    "        value, policy = y_pred[:,:1], y_pred[:,1:]\n",
    "        advantage = r - value\n",
    "        log_policy = K.log(policy + K.epsilon())\n",
    "        log_choosen_action_prob = K.sum(action_onehot * log_policy, axis=-1),\n",
    "        action_loss = -K.mean(log_choosen_action_prob * advantage)\n",
    "        value_loss = 0.5 * K.mean(K.square(advantage))\n",
    "        entropy = K.mean(-K.sum(policy * log_policy, axis=-1))\n",
    "        return action_loss + value_loss - beta * entropy\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        self.__m_train.fit(\n",
    "            x=np.array([s.state for s in samples], dtype=np.float32),\n",
    "            y=np.hstack([\n",
    "                np.reshape(np.array([s.reward for s in samples], dtype=np.float32), (-1, 1)),\n",
    "                keras.utils.to_categorical([s.action for s in samples], num_classes=self.__output_n)]),\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "    \n",
    "    def evalute_value(self, state, verbose=False):\n",
    "        v = self.__m_value.predict(\n",
    "            np.array([state], dtype=np.float32))[0,0]\n",
    "        if verbose:\n",
    "            print(v)\n",
    "        return v\n",
    "    \n",
    "    def get_action_prob(self, state, verbose=False):\n",
    "        action_prob = self.__m_policy.predict(\n",
    "            np.array([state], dtype=np.float32))[0]\n",
    "        if verbose:\n",
    "            print(action_prob)\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, m, gamma=0.98, max_steps=1000, n_prev_states=8, verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    action_probs = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx):\n",
    "        states = [e.state for e in episode[(idx-n_prev_states)+1:idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode, done):\n",
    "        if done:\n",
    "            discounted_reward = 0.\n",
    "        else:\n",
    "            discounted_reward = m.evalute_value(get_prev_states(episode, len(episode)-1))\n",
    "        episode[-1] = model_sample(\n",
    "                get_prev_states(episode, len(episode)-1),\n",
    "                episode[-1].action,\n",
    "                discounted_reward)\n",
    "        for i in reversed(range(len(episode)-1)):\n",
    "            discounted_reward = episode[i].reward + \\\n",
    "                gamma * discounted_reward\n",
    "            episode[i] = model_sample(\n",
    "                get_prev_states(episode, i),\n",
    "                episode[i].action,\n",
    "                discounted_reward)\n",
    "        samples.extend(episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action_prob = m.get_action_prob(state_queue_padded)\n",
    "        action_probs.append(action_prob)\n",
    "        action = int(np.random.choice(\n",
    "            list(range(action_prob.shape[-1])),\n",
    "            p=action_prob))\n",
    "        state_1, reward, done, _ = env.step(action)\n",
    "        episode.append(model_sample(state_0, action, reward))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode, True)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode, False)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action_prob]', np.mean(np.std(action_probs, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                1344      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,090\n",
      "Trainable params: 2,022\n",
      "Non-trainable params: 68\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 4)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                1344      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,049\n",
      "Trainable params: 1,985\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, 4)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 [(None, 1), (None, 2 2795        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3)            0           model_3[1][0]                    \n",
      "                                                                 model_3[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 2,795\n",
      "Trainable params: 2,663\n",
      "Non-trainable params: 132\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(4, 2, beta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(m, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    while True:\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action_prob = m.get_action_prob(state_queue)\n",
    "        action = np.argmax(action_prob)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/30)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = gym.make('CartPole-v0')\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std[action_prob] 0.027945545\n",
      "max game len 59\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 1s 879us/step - loss: 121.1329\n",
      "epoch 0 completed\n",
      "test result mean 19.0 std 10.381607668264957\n",
      "std[action_prob] 0.14635566\n",
      "max game len 47\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 0s 236us/step - loss: 103.5886\n",
      "epoch 1 completed\n",
      "test result mean 43.6 std 26.361588217202193\n",
      "std[action_prob] 0.121894434\n",
      "max game len 62\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 1s 234us/step - loss: 93.3010\n",
      "epoch 2 completed\n",
      "test result mean 73.0 std 53.88258840602717\n",
      "std[action_prob] 0.13454631\n",
      "max game len 54\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 1s 233us/step - loss: 81.0983\n",
      "epoch 3 completed\n",
      "test result mean 69.7 std 31.51031577118833\n",
      "std[action_prob] 0.11440548\n",
      "max game len 89\n",
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 1s 235us/step - loss: 65.5677\n",
      "epoch 4 completed\n",
      "test result mean 50.2 std 17.415191325072744\n",
      "std[action_prob] 0.09876129\n",
      "max game len 46\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1s 233us/step - loss: 40.6787\n",
      "epoch 5 completed\n",
      "test result mean 62.8 std 23.346662859318172\n",
      "std[action_prob] 0.10825719\n",
      "max game len 51\n",
      "Epoch 1/1\n",
      "7000/7000 [==============================] - 2s 231us/step - loss: 24.4209\n",
      "epoch 6 completed\n",
      "test result mean 42.9 std 10.660935960578486\n",
      "std[action_prob] 0.11154571\n",
      "max game len 56\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 18.2641\n",
      "epoch 7 completed\n",
      "test result mean 38.2 std 15.098197096195149\n",
      "std[action_prob] 0.10268025\n",
      "max game len 80\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 2s 230us/step - loss: 17.3538\n",
      "epoch 8 completed\n",
      "test result mean 50.7 std 19.92234925906079\n",
      "std[action_prob] 0.112351544\n",
      "max game len 89\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 18.2197\n",
      "epoch 9 completed\n",
      "test result mean 63.9 std 11.56094959575361\n",
      "std[action_prob] 0.13484132\n",
      "max game len 114\n",
      "Epoch 1/1\n",
      "11000/11000 [==============================] - 3s 228us/step - loss: 20.3616\n",
      "epoch 10 completed\n",
      "test result mean 69.2 std 20.082053901154854\n",
      "std[action_prob] 0.17796001\n",
      "max game len 85\n",
      "Epoch 1/1\n",
      "11000/11000 [==============================] - 3s 231us/step - loss: 21.9571\n",
      "epoch 11 completed\n",
      "test result mean 70.9 std 12.511772234357707\n",
      "std[action_prob] 0.29689375\n",
      "max game len 93\n",
      "Epoch 1/1\n",
      "11000/11000 [==============================] - 3s 231us/step - loss: 25.4970\n",
      "epoch 12 completed\n",
      "test result mean 200.0 std 0.0\n",
      "the network always gets full score, early exit\n"
     ]
    }
   ],
   "source": [
    "replays = []\n",
    "for i in range(100):\n",
    "    samples = play(env, m, max_steps=1000, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    m.train(replays, epochs=1, verbose=True)\n",
    "    if len(replays) > 10000:\n",
    "        random.shuffle(replays)\n",
    "        replays = replays[:10000]\n",
    "    print('epoch {} completed'.format(i))\n",
    "    test_result = [test(m, render=False) for _ in range(10)]\n",
    "    print('test result',\n",
    "          'mean', np.mean(test_result),\n",
    "          'std', np.std(test_result, ddof=1))\n",
    "    if np.mean(test_result) == env_max_possible_score and \\\n",
    "        np.std(test_result, ddof=1) == 0:\n",
    "        print('the network always gets full score, early exit')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(m, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
