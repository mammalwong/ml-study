{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train/255, X_test/255\n",
    "if len(X_train.shape) != 4:\n",
    "    X_train, X_test = X_train[:,:,:,np.newaxis], X_test[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrequencyMultiplicative(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, filters, freq_filters=None, **kwargs):\n",
    "        self.filters = filters\n",
    "        self.freq_filters = freq_filters\n",
    "        super(FrequencyMultiplicative, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.freq_filters is not None:\n",
    "            self.kernel_freq = self.add_weight(\n",
    "                shape=(1, input_shape[1], input_shape[2],self.freq_filters, input_shape[3]),\n",
    "                initializer='he_uniform', name='kernel_freq')\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(1, self.filters,\n",
    "                   input_shape[3] if self.freq_filters is None else self.freq_filters,\n",
    "                   input_shape[2], input_shape[1]),\n",
    "            initializer='he_uniform', name='kernel')\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.filters,),\n",
    "            initializer='zeros', name='bias')\n",
    "        super(FrequencyMultiplicative, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        if self.freq_filters is not None:\n",
    "            x = K.expand_dims(x, axis=-2)\n",
    "            x = x * self.kernel_freq\n",
    "            x = K.sum(x, axis=-1, keepdims=False)\n",
    "        x = K.permute_dimensions(x, (0,3,1,2))\n",
    "        x = tf.spectral.dct(x, norm='ortho')\n",
    "        x = K.permute_dimensions(x, (0,1,3,2))\n",
    "        x = tf.spectral.dct(x, norm='ortho')\n",
    "        x = K.expand_dims(x, axis=1)\n",
    "        x = x * self.kernel\n",
    "        x = K.sum(x, axis=2, keepdims=False)\n",
    "        x = tf.spectral.idct(x, norm='ortho')\n",
    "        x = K.permute_dimensions(x, (0,1,3,2))\n",
    "        x = tf.spectral.idct(x, norm='ortho')\n",
    "        x = K.permute_dimensions(x, (0,2,3,1))\n",
    "        x = x + self.bias\n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.filters,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionalPooling(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionalPooling, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        data_shape, att_shape = input_shape\n",
    "        if data_shape[-1] != att_shape[-1]:\n",
    "            raise Exception('channel count of data and attention required to be equal')\n",
    "        super(AttentionalPooling, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        data, att = inputs\n",
    "        data = K.reshape(data, (-1,data.shape[1]//2,2,data.shape[2]//2,2,data.shape[3]))\n",
    "        data = K.permute_dimensions(data, (0,1,3,2,4,5))\n",
    "        data = K.reshape(data, (-1,data.shape[1],data.shape[2],4,data.shape[-1]))\n",
    "        att = K.reshape(att, (-1,att.shape[1]//2,2,att.shape[2]//2,2,att.shape[-1]))\n",
    "        att = K.permute_dimensions(att, (0,1,3,2,4,5))\n",
    "        att = K.reshape(att, (-1,att.shape[1],att.shape[2],4,att.shape[-1]))\n",
    "        att = K.softmax(att, axis=-2)\n",
    "        data = data * att\n",
    "        data = K.sum(data, axis=-2, keepdims=False)\n",
    "        return data\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        data_shape, _ = input_shape\n",
    "        return (data_shape[0], data_shape[1]//2, data_shape[2]//2, data_shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 1)    4           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "frequency_multiplicative_1 (Fre (None, 28, 28, 6)    4710        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "frequency_multiplicative_2 (Fre (None, 28, 28, 2)    9410        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 7)    182         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 1)    2           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 16)   0           frequency_multiplicative_1[0][0] \n",
      "                                                                 frequency_multiplicative_2[0][0] \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 16)   64          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 4)    200         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 28, 28, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 16)   80          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attentional_pooling_1 (Attentio (None, 14, 14, 16)   0           activation_1[0][0]               \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "frequency_multiplicative_3 (Fre (None, 14, 14, 6)    18822       attentional_pooling_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "frequency_multiplicative_4 (Fre (None, 14, 14, 2)    14114       attentional_pooling_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 14, 7)    2807        attentional_pooling_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 14, 14, 1)    17          attentional_pooling_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 14, 16)   0           frequency_multiplicative_3[0][0] \n",
      "                                                                 frequency_multiplicative_4[0][0] \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 14, 14, 16)   64          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 14, 14, 4)    3140        attentional_pooling_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 14, 14, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 16)   80          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attentional_pooling_2 (Attentio (None, 7, 7, 16)     0           activation_2[0][0]               \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "frequency_multiplicative_5 (Fre (None, 7, 7, 6)      4710        attentional_pooling_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "frequency_multiplicative_6 (Fre (None, 7, 7, 2)      3530        attentional_pooling_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 7)      2807        attentional_pooling_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 7, 7, 1)      17          attentional_pooling_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7, 7, 16)     0           frequency_multiplicative_5[0][0] \n",
      "                                                                 frequency_multiplicative_6[0][0] \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7, 7, 16)     64          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 7, 7, 16)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 784)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           7850        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 72,674\n",
      "Trainable params: 72,576\n",
      "Non-trainable params: 98\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = X_input = keras.layers.Input(X_train.shape[1:])\n",
    "X = keras.layers.BatchNormalization()(X)\n",
    "X1 = FrequencyMultiplicative(6)(X)\n",
    "X2 = FrequencyMultiplicative(2, freq_filters=4)(X)\n",
    "X3 = keras.layers.Conv2D(7, (5,5), padding='same', kernel_initializer='he_uniform')(X)\n",
    "X4 = keras.layers.Conv2D(1, (1,1), padding='same', kernel_initializer='he_uniform')(X)\n",
    "XA = keras.layers.Conv2D(4, (7,7), padding='same', kernel_initializer='he_uniform', activation='relu')(X)\n",
    "XA = keras.layers.Conv2D(16, (1,1), padding='same', kernel_initializer='he_uniform')(XA)\n",
    "X = keras.layers.Concatenate()([X1,X2,X3,X4])\n",
    "X = keras.layers.BatchNormalization()(X)\n",
    "X = keras.layers.Activation('relu')(X)\n",
    "X = AttentionalPooling()([X,XA])\n",
    "X1 = FrequencyMultiplicative(6)(X)\n",
    "X2 = FrequencyMultiplicative(2, freq_filters=4)(X)\n",
    "X3 = keras.layers.Conv2D(7, (5,5), padding='same', kernel_initializer='he_uniform')(X)\n",
    "X4 = keras.layers.Conv2D(1, (1,1), padding='same', kernel_initializer='he_uniform')(X)\n",
    "XA = keras.layers.Conv2D(4, (7,7), padding='same', kernel_initializer='he_uniform', activation='relu')(X)\n",
    "XA = keras.layers.Conv2D(16, (1,1), padding='same', kernel_initializer='he_uniform')(XA)\n",
    "X = keras.layers.Concatenate()([X1,X2,X3,X4])\n",
    "X = keras.layers.BatchNormalization()(X)\n",
    "X = keras.layers.Activation('relu')(X)\n",
    "X = AttentionalPooling()([X,XA])\n",
    "X1 = FrequencyMultiplicative(6)(X)\n",
    "X2 = FrequencyMultiplicative(2, freq_filters=4)(X)\n",
    "X3 = keras.layers.Conv2D(7, (5,5), padding='same', kernel_initializer='he_uniform')(X)\n",
    "X4 = keras.layers.Conv2D(1, (1,1), padding='same', kernel_initializer='he_uniform')(X)\n",
    "X = keras.layers.Concatenate()([X1,X2,X3,X4])\n",
    "X = keras.layers.BatchNormalization()(X)\n",
    "X = keras.layers.Activation('relu')(X)\n",
    "X = keras.layers.Flatten()(X)\n",
    "X = keras.layers.Dense(10, activation='softmax')(X)\n",
    "M = keras.Model(X_input, X)\n",
    "M.compile('nadam', 'sparse_categorical_crossentropy', ['acc'])\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.2063 - acc: 0.9364 - val_loss: 0.0636 - val_acc: 0.9785\n",
      "Epoch 2/30\n",
      "938/937 [==============================] - 126s 134ms/step - loss: 0.0849 - acc: 0.9741 - val_loss: 0.0534 - val_acc: 0.9814\n",
      "Epoch 3/30\n",
      "938/937 [==============================] - 127s 135ms/step - loss: 0.0669 - acc: 0.9793 - val_loss: 0.0384 - val_acc: 0.9865\n",
      "Epoch 4/30\n",
      "938/937 [==============================] - 127s 136ms/step - loss: 0.0566 - acc: 0.9827 - val_loss: 0.0392 - val_acc: 0.9860\n",
      "Epoch 5/30\n",
      "938/937 [==============================] - 127s 135ms/step - loss: 0.0533 - acc: 0.9831 - val_loss: 0.0261 - val_acc: 0.9912\n",
      "Epoch 6/30\n",
      "938/937 [==============================] - 127s 136ms/step - loss: 0.0486 - acc: 0.9850 - val_loss: 0.0249 - val_acc: 0.9926\n",
      "Epoch 7/30\n",
      "938/937 [==============================] - 127s 136ms/step - loss: 0.0442 - acc: 0.9863 - val_loss: 0.0301 - val_acc: 0.9894\n",
      "Epoch 8/30\n",
      "938/937 [==============================] - 127s 135ms/step - loss: 0.0427 - acc: 0.9867 - val_loss: 0.0321 - val_acc: 0.9901\n",
      "Epoch 9/30\n",
      "938/937 [==============================] - 128s 136ms/step - loss: 0.0386 - acc: 0.9877 - val_loss: 0.0275 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/30\n",
      "938/937 [==============================] - 129s 137ms/step - loss: 0.0279 - acc: 0.9911 - val_loss: 0.0207 - val_acc: 0.9934\n",
      "Epoch 11/30\n",
      "938/937 [==============================] - 128s 137ms/step - loss: 0.0241 - acc: 0.9924 - val_loss: 0.0177 - val_acc: 0.9942\n",
      "Epoch 12/30\n",
      "938/937 [==============================] - 129s 137ms/step - loss: 0.0242 - acc: 0.9923 - val_loss: 0.0175 - val_acc: 0.9938\n",
      "Epoch 13/30\n",
      "938/937 [==============================] - 128s 137ms/step - loss: 0.0225 - acc: 0.9929 - val_loss: 0.0174 - val_acc: 0.9940\n",
      "Epoch 14/30\n",
      "938/937 [==============================] - 130s 138ms/step - loss: 0.0213 - acc: 0.9932 - val_loss: 0.0172 - val_acc: 0.9938\n",
      "Epoch 15/30\n",
      "938/937 [==============================] - 130s 139ms/step - loss: 0.0211 - acc: 0.9934 - val_loss: 0.0200 - val_acc: 0.9933\n",
      "Epoch 16/30\n",
      "938/937 [==============================] - 130s 139ms/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0180 - val_acc: 0.9940\n",
      "Epoch 17/30\n",
      "938/937 [==============================] - 131s 139ms/step - loss: 0.0200 - acc: 0.9936 - val_loss: 0.0175 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 18/30\n",
      "938/937 [==============================] - 132s 140ms/step - loss: 0.0179 - acc: 0.9942 - val_loss: 0.0169 - val_acc: 0.9945\n",
      "Epoch 19/30\n",
      "938/937 [==============================] - 127s 136ms/step - loss: 0.0182 - acc: 0.9945 - val_loss: 0.0167 - val_acc: 0.9947\n",
      "Epoch 20/30\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.0169 - val_acc: 0.9945\n",
      "Epoch 21/30\n",
      "938/937 [==============================] - 131s 140ms/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9943\n",
      "Epoch 22/30\n",
      "938/937 [==============================] - 131s 140ms/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0169 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "Epoch 23/30\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0169 - val_acc: 0.9944\n",
      "Epoch 24/30\n",
      "938/937 [==============================] - 129s 138ms/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0169 - val_acc: 0.9944\n",
      "Epoch 25/30\n",
      "938/937 [==============================] - 131s 139ms/step - loss: 0.0179 - acc: 0.9942 - val_loss: 0.0168 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.000000222324161e-07.\n",
      "Epoch 26/30\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.0168 - val_acc: 0.9944\n",
      "Epoch 27/30\n",
      "938/937 [==============================] - 128s 137ms/step - loss: 0.0185 - acc: 0.9942 - val_loss: 0.0168 - val_acc: 0.9944\n",
      "Epoch 28/30\n",
      "938/937 [==============================] - 132s 141ms/step - loss: 0.0181 - acc: 0.9944 - val_loss: 0.0168 - val_acc: 0.9944\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-08.\n",
      "Epoch 29/30\n",
      "938/937 [==============================] - 133s 142ms/step - loss: 0.0173 - acc: 0.9943 - val_loss: 0.0167 - val_acc: 0.9944\n",
      "Epoch 30/30\n",
      "938/937 [==============================] - 133s 142ms/step - loss: 0.0185 - acc: 0.9940 - val_loss: 0.0168 - val_acc: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f404faf28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.fit_generator(\n",
    "    datagen.flow(X_train, Y_train, batch_size=64), \n",
    "    validation_data=(X_test, Y_test),\n",
    "    steps_per_epoch=len(X_train) / 64, epochs=30, callbacks=[\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
