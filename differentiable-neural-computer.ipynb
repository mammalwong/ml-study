{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test) = keras.datasets.reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc47193d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_input_len_quantile_x = np.linspace(0, 1, num=100)\n",
    "data_input_len_quantile_y = np.quantile(\n",
    "    [len(x) for x in X_train] + [len(x) for x in X_test], data_input_len_quantile_x)\n",
    "plt.plot(data_input_len_quantile_x, data_input_len_quantile_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982, 64), (8982,), (2246, 64), (2246,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=64, padding='post', truncating='post')\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=64, padding='post', truncating='post')\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input_dim = max(np.max(X_train), np.max(X_test)) + 1\n",
    "data_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_output_dim = max(np.max(Y_train), np.max(Y_test)) + 1\n",
    "data_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNCCell(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 word_size,\n",
    "                 memory_size,\n",
    "                 register_size=2,\n",
    "                 read_head_count=3,\n",
    "                 enable_temporal=True,\n",
    "                 enable_final_bias=True,\n",
    "                 bypass_dropout_factor=None,\n",
    "                 **kwargs):\n",
    "        self.output_size = output_size\n",
    "        self.word_size = word_size\n",
    "        self.memory_size = memory_size\n",
    "        self.register_size = register_size\n",
    "        self.read_head_count = read_head_count\n",
    "        self.enable_temporal = enable_temporal\n",
    "        self.enable_final_bias = enable_final_bias\n",
    "        self.bypass_dropout_factor = bypass_dropout_factor\n",
    "        self.state_size = (output_size,)\n",
    "        self.state_size += (register_size * word_size,)\n",
    "        self.state_size += (register_size * word_size,)\n",
    "        self.state_size += (memory_size * word_size,)\n",
    "        self.state_size += (read_head_count * memory_size,)\n",
    "        self.state_size += (read_head_count * word_size,)\n",
    "        self.state_size += (memory_size,)\n",
    "        self.state_size += (memory_size,)\n",
    "        if enable_temporal:\n",
    "            self.state_size += (memory_size,)\n",
    "            self.state_size += (memory_size * memory_size,)\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        read_vec_size = self.word_size * self.read_head_count\n",
    "        controller_input_size = input_shape[-1]\n",
    "        controller_input_size += read_vec_size\n",
    "        controller_input_size += self.register_size*self.word_size\n",
    "        controller_kernel_size = self.register_size*self.word_size*4\n",
    "        controller_hidden_size = self.register_size*self.word_size\n",
    "        interface_vec_size = read_vec_size\n",
    "        interface_vec_size += self.word_size * 3\n",
    "        interface_vec_size += self.read_head_count * 2\n",
    "        if self.enable_temporal:\n",
    "            interface_vec_size += self.read_head_count * 3\n",
    "        interface_vec_size += 3\n",
    "        self.kernel_controller_hidden = self.add_weight(\n",
    "            shape=(controller_input_size, controller_kernel_size),\n",
    "            initializer='glorot_normal', name='kernel_controller_hidden')\n",
    "        self.bias_controller_hidden = self.add_weight(\n",
    "            shape=(controller_kernel_size,),\n",
    "            initializer='zeros', name='bias_controller_hidden')\n",
    "        self.kernel_controller_output = self.add_weight(\n",
    "            shape=(controller_hidden_size, self.output_size + interface_vec_size),\n",
    "            initializer='glorot_normal', name='kernel_controller_output')\n",
    "        self.kernel_read_vec_to_output = self.add_weight(\n",
    "            shape=(read_vec_size, self.output_size),\n",
    "            initializer='glorot_normal', name='kernel_read_vec_to_output')\n",
    "        if self.enable_final_bias:\n",
    "            self.bias_final_output = self.add_weight(\n",
    "                shape=(self.output_size,),\n",
    "                initializer='zeros', name='bias_final_output')\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        \n",
    "        def oneplus(x):\n",
    "            return K.softplus(x) + 1.\n",
    "        \n",
    "        def similarity(m, k, b):\n",
    "            dot = K.batch_dot(k, m, axes=2)\n",
    "            m_len = K.sqrt(K.sum(K.square(m), axis=-1))\n",
    "            k_len = K.sqrt(K.sum(K.square(k), axis=-1))\n",
    "            mk_len = K.expand_dims(k_len, axis=-1) @ K.expand_dims(m_len, axis=-2)\n",
    "            mk_len = K.switch(K.not_equal(mk_len, 0.), mk_len, K.ones_like(mk_len))\n",
    "            cos_sim = dot / mk_len\n",
    "            return K.softmax(cos_sim * K.expand_dims(b))\n",
    "        \n",
    "        def batch_invert_permutation(permutations):\n",
    "            # from https://github.com/deepmind/dnc/blob/master/dnc/util.py\n",
    "            perm = tf.cast(permutations, tf.float32)\n",
    "            dim = int(perm.get_shape()[-1])\n",
    "            size = tf.cast(tf.shape(perm)[0], tf.float32)\n",
    "            delta = tf.cast(tf.shape(perm)[-1], tf.float32)\n",
    "            rg = tf.range(0, size * delta, delta, dtype=tf.float32)\n",
    "            rg = tf.expand_dims(rg, 1)\n",
    "            rg = tf.tile(rg, [1, dim])\n",
    "            perm = tf.add(perm, rg)\n",
    "            flat = tf.reshape(perm, [-1])\n",
    "            perm = tf.invert_permutation(tf.cast(flat, tf.int32))\n",
    "            perm = tf.reshape(perm, [-1, dim])\n",
    "            return tf.subtract(perm, tf.cast(rg, tf.int32))\n",
    "\n",
    "        def batch_gather(values, indices):\n",
    "            # from https://github.com/deepmind/dnc/blob/master/dnc/util.py\n",
    "            idx = tf.expand_dims(indices, -1)\n",
    "            size = tf.shape(indices)[0]\n",
    "            rg = tf.range(size, dtype=tf.int32)\n",
    "            rg = tf.expand_dims(rg, -1)\n",
    "            rg = tf.tile(rg, [1, int(indices.get_shape()[-1])])\n",
    "            rg = tf.expand_dims(rg, -1)\n",
    "            gidx = tf.concat([rg, idx], -1)\n",
    "            return tf.gather_nd(values, gidx)\n",
    "        \n",
    "        _, register_s_last, register_h_last, memory_last, \\\n",
    "            read_weights_last, read_vec_last, write_weight_last, \\\n",
    "            usage_last, *temporal_states = states\n",
    "        memory_last = K.reshape(memory_last,\n",
    "                        (-1,self.memory_size,self.word_size))\n",
    "        read_weights_last = K.reshape(read_weights_last,\n",
    "                        (-1,self.read_head_count,self.memory_size))\n",
    "        if not self.enable_temporal:\n",
    "            assert not temporal_states\n",
    "        else:\n",
    "            preced_last, link_last = temporal_states\n",
    "            link_last = K.reshape(link_last,\n",
    "                        (-1,self.memory_size,self.memory_size))\n",
    "        \n",
    "        # feeding controller with current input and last read vecs\n",
    "        output = K.concatenate([inputs, read_vec_last, register_h_last])\n",
    "        output = output @ self.kernel_controller_hidden\n",
    "        output = output + self.bias_controller_hidden\n",
    "        ctr_input_gate = output[:,:self.register_size*self.word_size]\n",
    "        ctr_input_gate = K.sigmoid(ctr_input_gate)\n",
    "        ctr_forget_gate = output[:,\n",
    "            self.register_size*self.word_size:\n",
    "            self.register_size*self.word_size*2]\n",
    "        ctr_forget_gate = K.sigmoid(ctr_forget_gate)\n",
    "        register_s = output[:,\n",
    "            self.register_size*self.word_size*2:\n",
    "            self.register_size*self.word_size*3]\n",
    "        register_s = ctr_input_gate * K.tanh(register_s)\n",
    "        register_s = register_s + ctr_forget_gate*register_s_last\n",
    "        ctr_output_gate = output[:,\n",
    "            self.register_size*self.word_size*3:\n",
    "            self.register_size*self.word_size*4]\n",
    "        ctr_output_gate = K.sigmoid(ctr_output_gate)\n",
    "        register_h = ctr_output_gate * K.tanh(register_s)\n",
    "        output = register_h @ self.kernel_controller_output\n",
    "        \n",
    "        # break down controller output into semi final output and interface vec\n",
    "        interface_vec = output[:,self.output_size:]\n",
    "        output = output[:,:self.output_size]\n",
    "        if self.bypass_dropout_factor is not None:\n",
    "            output = K.dropout(output, self.bypass_dropout_factor)\n",
    "        \n",
    "        # break down interface vec\n",
    "        read_keys = interface_vec[:,:self.word_size*self.read_head_count]\n",
    "        read_keys = K.reshape(read_keys, (-1,self.read_head_count,self.word_size))\n",
    "        read_stre = interface_vec[:,\n",
    "            self.word_size*self.read_head_count:\n",
    "            (self.word_size+1)*self.read_head_count]\n",
    "        read_stre = oneplus(read_stre)\n",
    "        write_key = interface_vec[:,\n",
    "            (self.word_size+1)*self.read_head_count:\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size]\n",
    "        write_stre = interface_vec[:,\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size:\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size+1]\n",
    "        write_stre = oneplus(write_stre)\n",
    "        erase_vec = interface_vec[:,\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size+1:\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size*2+1]\n",
    "        erase_vec = K.sigmoid(erase_vec)\n",
    "        write_vec = interface_vec[:,\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size*2+1:\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size*3+1]\n",
    "        free_gates = interface_vec[:,\n",
    "            (self.word_size+1)*self.read_head_count+self.word_size*3+1:\n",
    "            (self.word_size+2)*self.read_head_count+self.word_size*3+1]\n",
    "        free_gates = K.sigmoid(free_gates)\n",
    "        alloc_gate = interface_vec[:,\n",
    "            (self.word_size+2)*self.read_head_count+self.word_size*3+1:\n",
    "            (self.word_size+2)*self.read_head_count+self.word_size*3+2]\n",
    "        alloc_gate = K.sigmoid(alloc_gate)\n",
    "        write_gate = interface_vec[:,\n",
    "            (self.word_size+2)*self.read_head_count+self.word_size*3+2:\n",
    "            (self.word_size+2)*self.read_head_count+self.word_size*3+3]\n",
    "        write_gate = K.sigmoid(write_gate)\n",
    "        if self.enable_temporal:\n",
    "            read_modes = interface_vec[:,\n",
    "                (self.word_size+2)*self.read_head_count+self.word_size*3+3:\n",
    "                (self.word_size+5)*self.read_head_count+self.word_size*3+3]\n",
    "            read_modes = K.reshape(read_modes, (-1,self.read_head_count,3))\n",
    "            read_modes = K.softmax(read_modes, axis=-1)\n",
    "        \n",
    "        # compute allocation vector\n",
    "        retention = K.prod(1.-(K.expand_dims(free_gates)*read_weights_last), axis=-2)\n",
    "        usage = ((usage_last+write_weight_last)-usage_last*write_weight_last)*retention\n",
    "        # quickfix for tf.cumprod grad bug\n",
    "        # https://github.com/tensorflow/tensorflow/issues/3862\n",
    "        usage_qfixed = 1e-6 + (1-1e-6) * usage\n",
    "        usage_asc,usage_perm = tf.nn.top_k(1. - usage_qfixed, k=self.memory_size)\n",
    "        usage_asc = 1. - usage_asc\n",
    "        alloc_asc = (1. - usage_asc) * tf.cumprod(usage_asc, axis=-1, exclusive=True)\n",
    "        alloc_perm = batch_invert_permutation(usage_perm)\n",
    "        alloc = batch_gather(alloc_asc, alloc_perm)\n",
    "        \n",
    "        # compute write weight\n",
    "        write_sim = K.reshape(\n",
    "            similarity(memory_last,K.expand_dims(write_key, axis=-2),write_stre),\n",
    "            (-1,self.memory_size))\n",
    "        write_weight = write_gate*(alloc_gate*alloc + (1.-alloc_gate)*write_sim)\n",
    "        \n",
    "        # compute precedence and temporal links\n",
    "        if self.enable_temporal:\n",
    "            preced = (1.-K.sum(\n",
    "                write_weight,axis=-1,keepdims=True))*preced_last + write_weight\n",
    "            write_weight_rep_h =  K.repeat_elements(K.expand_dims(\n",
    "                write_weight,axis=-1),self.memory_size,axis=-1)\n",
    "            write_weight_rep_v =  K.repeat_elements(K.expand_dims(\n",
    "                write_weight,axis=-2),self.memory_size,axis=-2)\n",
    "            link = (1.-(write_weight_rep_h+write_weight_rep_v))*link_last + \\\n",
    "                write_weight_rep_h * K.repeat_elements(K.expand_dims(\n",
    "                    preced_last,axis=-2),self.memory_size,axis=-2)\n",
    "            link_forward = read_weights_last @ link\n",
    "            link_backward = K.permute_dimensions(link @ K.permute_dimensions(\n",
    "                read_weights_last,(0,2,1)),(0,2,1))\n",
    "        \n",
    "        # compute read weights and read vectors\n",
    "        read_sims = similarity(memory_last,read_keys,read_stre)\n",
    "        if not self.enable_temporal:\n",
    "            read_weights = read_sims\n",
    "        else:\n",
    "            read_weights = read_modes[:,:,0:1] * read_sims + \\\n",
    "                            read_modes[:,:,1:2] * link_forward + \\\n",
    "                            read_modes[:,:,2:3] * link_backward\n",
    "        read_vecs = read_weights @ memory_last\n",
    "        read_vec = K.reshape(read_vecs, (-1,self.read_head_count*self.word_size))\n",
    "        \n",
    "        # update memory\n",
    "        m_keep = K.expand_dims(write_weight,axis=-1) @ K.expand_dims(erase_vec,axis=-2)\n",
    "        m_new = K.expand_dims(write_weight,axis=-1) @ K.expand_dims(write_vec,axis=-2)\n",
    "        memory = memory_last*(1.-m_keep) + m_new\n",
    "        \n",
    "        # compute final output from controller output and current read vecs\n",
    "        output = output + read_vec @ self.kernel_read_vec_to_output\n",
    "        if self.enable_final_bias:\n",
    "            output = output + self.bias_final_output\n",
    "        \n",
    "        return output, [\n",
    "            output,\n",
    "            register_s,\n",
    "            register_h,\n",
    "            K.reshape(memory, (-1,self.memory_size*self.word_size)),\n",
    "            K.reshape(read_weights, (-1,self.read_head_count*self.memory_size)),\n",
    "            read_vec,\n",
    "            write_weight,\n",
    "            usage, *([\n",
    "                preced,\n",
    "                K.reshape(link, (-1,self.memory_size*self.memory_size))\n",
    "            ] if self.enable_temporal else [])]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 32)     991328      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rnn_1 (RNN)                     [(None, None, 32), ( 2704        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "rnn_2 (RNN)                     [(None, None, 32), ( 2704        rnn_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32)           0           lambda_1[0][0]                   \n",
      "                                                                 rnn_1[0][1]                      \n",
      "                                                                 rnn_2[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 46)           1518        add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 998,254\n",
      "Trainable params: 998,254\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = X_inputs = keras.layers.Input((None,), dtype='int32')\n",
    "X = keras.layers.Embedding(data_input_dim, 32, mask_zero=True)(X)\n",
    "X_skip = keras.layers.Lambda(lambda x: K.mean(x, axis=-2))(X)\n",
    "X_skips = [X_skip]\n",
    "for _ in range(2):\n",
    "    X,X_skip,*_ = keras.layers.RNN(DNCCell(32, 4, 16,\n",
    "        bypass_dropout_factor=0.5), return_sequences=True, return_state=True)(X)\n",
    "    X_skips.append(X_skip)\n",
    "X = keras.layers.Add()(X_skips)\n",
    "X = keras.layers.Dense(data_output_dim, activation='softmax')(X)\n",
    "M = keras.Model(X_inputs, X)\n",
    "M.compile('nadam', 'sparse_categorical_crossentropy', ['acc'])\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 111s 12ms/step - loss: 2.4798 - acc: 0.3967 - val_loss: 1.8506 - val_acc: 0.5098\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 107s 12ms/step - loss: 1.7300 - acc: 0.5194 - val_loss: 1.7602 - val_acc: 0.5223\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 109s 12ms/step - loss: 1.5485 - acc: 0.5658 - val_loss: 1.7219 - val_acc: 0.5557\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 108s 12ms/step - loss: 1.4257 - acc: 0.6004 - val_loss: 1.7204 - val_acc: 0.5681\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 106s 12ms/step - loss: 1.3334 - acc: 0.6219 - val_loss: 1.7289 - val_acc: 0.5552\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 108s 12ms/step - loss: 1.2596 - acc: 0.6454 - val_loss: 1.7506 - val_acc: 0.5953\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 107s 12ms/step - loss: 1.1708 - acc: 0.6752 - val_loss: 1.6900 - val_acc: 0.6184\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 108s 12ms/step - loss: 1.0804 - acc: 0.7089 - val_loss: 1.7038 - val_acc: 0.6082\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 106s 12ms/step - loss: 0.9840 - acc: 0.7393 - val_loss: 1.7926 - val_acc: 0.6006\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 108s 12ms/step - loss: 0.9045 - acc: 0.7620 - val_loss: 1.7341 - val_acc: 0.6322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc446db080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.fit(X_train, Y_train, validation_data=(X_test,Y_test), batch_size=128, epochs=10, callbacks=[\n",
    "    keras.callbacks.ReduceLROnPlateau('loss', patience=3, verbose=1)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
