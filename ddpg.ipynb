{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 2.74678599e-03  4.85736644e-06 -6.33393265e-04 -1.60000312e-02\n",
      "  9.23076123e-02  1.47039292e-03  8.59987766e-01  2.33967400e-04\n",
      "  1.00000000e+00  3.26715894e-02  1.47031911e-03  8.53627235e-01\n",
      " -1.00045178e-03  1.00000000e+00  4.40813571e-01  4.45819676e-01\n",
      "  4.61422324e-01  4.89549696e-01  5.34102261e-01  6.02460444e-01\n",
      "  7.09148169e-01  8.85930896e-01  1.00000000e+00  1.00000000e+00]\n",
      "Box(24,)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "def env_create():\n",
    "    env = gym.make('BipedalWalker-v2')\n",
    "    return env\n",
    "env = env_create()\n",
    "env_max_possible_score = -1\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'state1', 'action', 'reward', 'gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, tau=0.9):\n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__tau = tau\n",
    "        self.__m_train,m_train_freeze, \\\n",
    "            self.__m_train_policy = self.__build_model()\n",
    "        self.__m_target,_, \\\n",
    "            self.__m_target_policy = self.__build_model()\n",
    "        self.__m_train.compile('nadam', 'mse')\n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = self.__m_train_policy(m)\n",
    "        m = m_train_freeze([m_input, m])\n",
    "        self.__m_train_policy_chain = keras.Model([m_input], [m])\n",
    "        self.__m_train_policy_chain.compile(\n",
    "            'nadam', __class__.__gradient_ascent)\n",
    "        self.__m_train_policy_chain.summary()\n",
    "        self.__sync_target(0.)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __gradient_ascent(y_true, y_pred):\n",
    "        y_pred = y_pred + 0.*y_true\n",
    "        y_pred = K.sum(y_pred, axis=-1)\n",
    "        return -K.mean(y_pred)\n",
    "        \n",
    "    def __build_model(self):\n",
    "        \n",
    "        l_state = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False)]\n",
    "        l_action = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu')]\n",
    "        l_value = [\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        l_policy = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False),\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, l_state)\n",
    "        m_a = m_input_a = keras.layers.Input((self.__output_n,))\n",
    "        m_a = apply_layers(m_a, l_action)\n",
    "        m = keras.layers.Concatenate()([m, m_a])\n",
    "        m = apply_layers(m, l_value)\n",
    "        m_train = keras.models.Model([m_input, m_input_a], [m])\n",
    "        m_train_freeze = keras.models.Model([m_input, m_input_a], [m])\n",
    "        m_train_freeze.trainable = False\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, l_policy)\n",
    "        m_train_policy = keras.models.Model([m_input], [m])\n",
    "        \n",
    "        return m_train, m_train_freeze, m_train_policy\n",
    "    \n",
    "    def __sync_target(self, tau):\n",
    "        for m_train, m_target in [\n",
    "            (self.__m_train, self.__m_target),\n",
    "            (self.__m_train_policy, self.__m_target_policy)]:\n",
    "            w_train = m_train.get_weights()\n",
    "            w_target = m_target.get_weights()\n",
    "            w = [tau*x + (1-tau)*y for x,y in zip(w_target,w_train)]\n",
    "            m_target.set_weights(w)\n",
    "    \n",
    "    def __get_sample_q(self, samples):\n",
    "        q = self.__m_target_policy.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32)],\n",
    "            batch_size=64)\n",
    "        q = self.__m_target.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),q],\n",
    "            batch_size=64)\n",
    "        q = np.array([s.reward for s in samples])[...,np.newaxis] + \\\n",
    "            q * np.array([s.gamma for s in samples])[...,np.newaxis]\n",
    "        return q\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        a = np.array([s.action for s in samples], dtype=np.float32)\n",
    "        q = self.__get_sample_q(samples)\n",
    "        self.__m_train.fit(\n",
    "            x=[np.array([s.state for s in samples], dtype=np.float32),a],\n",
    "            y=q,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "        self.__m_train_policy_chain.fit(\n",
    "            x=[np.array([s.state for s in samples], dtype=np.float32)],\n",
    "            y=np.zeros_like(q),\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "        self.__sync_target(self.__tau)\n",
    "    \n",
    "    def get_sample_priority(self, samples):\n",
    "        q = self.__get_sample_q(samples)[:,0]\n",
    "        t = self.__m_train.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),\n",
    "            np.array([s.action for s in samples], dtype=np.float32)],\n",
    "            batch_size=64)[:,0]\n",
    "        p = np.abs(q - t)\n",
    "        p = p / np.max(p)\n",
    "        p = p / np.sum(p)\n",
    "        return p\n",
    "    \n",
    "    def get_action(self, state, verbose=False):\n",
    "        action = self.__m_train_policy.predict([\n",
    "            np.array([state], dtype=np.float32)])[0]\n",
    "        if verbose:\n",
    "            print(action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, max_steps=1000, n_prev_states=8,\n",
    "         reward_steps=1, gamma=0.98, epsilon=0., verbose=False):\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    actions = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx, get_state):\n",
    "        states = [get_state(e) for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode):\n",
    "        processed_episode = []\n",
    "        for i in range(len(episode)):\n",
    "            episode_reward = 0\n",
    "            episode_gamma = 1.\n",
    "            for j in range(reward_steps):\n",
    "                if i + j >= len(episode):\n",
    "                    break\n",
    "                episode_reward += episode[i+j].reward * episode_gamma\n",
    "                episode_gamma *= episode[i+j].gamma * gamma\n",
    "            processed_episode.append(model_sample(\n",
    "                get_prev_states(episode, i, lambda e: e.state),\n",
    "                get_prev_states(episode, i, lambda e: e.state1),\n",
    "                episode[i].action,\n",
    "                episode_reward,\n",
    "                episode_gamma))\n",
    "        samples.extend(processed_episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action = m.get_action(state_queue_padded)\n",
    "        actions.append(action)\n",
    "        action = action + np.random.randn(*action.shape) * epsilon\n",
    "        action_clipped = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "        state_1, reward, done, _ = env.step(action_clipped)\n",
    "        episode.append(model_sample(\n",
    "            state_0, state_1, action, reward, 0. if done else 1.))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action]', np.mean(np.std(actions, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 24)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 4)            178212      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 1)            182417      input_7[0][0]                    \n",
      "                                                                 model_3[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 360,629\n",
      "Trainable params: 177,268\n",
      "Non-trainable params: 183,361\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.shape[0], tau=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(m, max_steps=10000, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    for _ in range(max_steps):\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action = m.get_action(state_queue)\n",
    "        action_clipped = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "        state, reward, done, _ = env.step(action_clipped)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/60)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = env_create()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replays = play(\n",
    "    env, m, max_steps=30000, n_prev_states=4,\n",
    "    reward_steps=3, epsilon=0.3, verbose=False)\n",
    "for i in range(200):\n",
    "    samples = play(\n",
    "        env, m, max_steps=3000, n_prev_states=4,\n",
    "        reward_steps=3, epsilon=0.3, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    if len(replays) > 100000:\n",
    "        replays = list(replays[-100000:])\n",
    "    samples = [model_sample(*s) for s in np.array(replays)[\n",
    "        np.random.choice(np.arange(len(replays), dtype=np.int32),\n",
    "            p=m.get_sample_priority(replays), size=30000)]]\n",
    "    m.train(samples, epochs=1, verbose=True)\n",
    "    print('epoch {} completed'.format(i))\n",
    "    if i % 5 != 4:\n",
    "        continue\n",
    "    test_result = [test(m, max_steps=3000,\n",
    "        n_prev_states=4, render=(i==0)) for i in range(3)]\n",
    "    print('test result',\n",
    "          'mean', np.mean(test_result),\n",
    "          'std', np.std(test_result, ddof=1))\n",
    "    if np.mean(test_result) == env_max_possible_score and \\\n",
    "        np.std(test_result, ddof=1) == 0:\n",
    "        print('the network always gets full score, early exit')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(m, n_prev_states=4, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
