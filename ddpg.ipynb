{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import keras.layers\n",
    "import keras.models\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.44073046 -0.89763949  0.42941648]\n",
      "Box(3,)\n",
      "Box(1,)\n"
     ]
    }
   ],
   "source": [
    "def env_create():\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    return env\n",
    "env = env_create()\n",
    "env_max_possible_score = -1\n",
    "print(env.reset())\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sample = collections.namedtuple('model_sample',\n",
    "    ['state', 'state1', 'action', 'reward', 'gamma', 'priority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model:\n",
    "    \n",
    "    def __init__(self, input_n, output_n, tau=0.9):\n",
    "        self.__input_n = input_n\n",
    "        self.__output_n = output_n\n",
    "        self.__tau = tau\n",
    "        self.__m_train,m_train_freeze, \\\n",
    "            self.__m_train_policy = self.__build_model()\n",
    "        self.__m_target,_, \\\n",
    "            self.__m_target_policy = self.__build_model()\n",
    "        self.__m_train.compile('nadam', 'mse')\n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = self.__m_train_policy(m)\n",
    "        m = m_train_freeze([m_input, m])\n",
    "        self.__m_train_policy_chain = keras.Model([m_input], [m])\n",
    "        self.__m_train_policy_chain.compile(\n",
    "            'nadam', __class__.__gradient_ascent)\n",
    "        self.__m_train_policy_chain.summary()\n",
    "        self.__sync_target(0.)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __gradient_ascent(y_true, y_pred):\n",
    "        y_pred = y_pred + 0.*y_true\n",
    "        y_pred = K.sum(y_pred, axis=-1)\n",
    "        return -K.mean(y_pred)\n",
    "        \n",
    "    def __build_model(self):\n",
    "        \n",
    "        l_state = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False)]\n",
    "        l_action = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(32, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu')]\n",
    "        l_value = [\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(1)]\n",
    "        l_policy = [\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.TimeDistributed(\n",
    "                keras.layers.Dense(128, kernel_initializer='he_uniform')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.LSTM(128, return_sequences=False),\n",
    "            keras.layers.Dense(128, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(64, kernel_initializer='he_uniform'),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Activation('relu'),\n",
    "            keras.layers.Dense(self.__output_n)]\n",
    "        def apply_layers(x, layers):\n",
    "            last_layer = x\n",
    "            for l in layers:\n",
    "                last_layer = l(last_layer)\n",
    "            return last_layer\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, l_state)\n",
    "        m_a = m_input_a = keras.layers.Input((self.__output_n,))\n",
    "        m_a = apply_layers(m_a, l_action)\n",
    "        m = keras.layers.Concatenate()([m, m_a])\n",
    "        m = apply_layers(m, l_value)\n",
    "        m_train = keras.models.Model([m_input, m_input_a], [m])\n",
    "        m_train_freeze = keras.models.Model([m_input, m_input_a], [m])\n",
    "        m_train_freeze.trainable = False\n",
    "        \n",
    "        m = m_input = keras.layers.Input((None, self.__input_n,))\n",
    "        m = apply_layers(m, l_policy)\n",
    "        m_train_policy = keras.models.Model([m_input], [m])\n",
    "        \n",
    "        return m_train, m_train_freeze, m_train_policy\n",
    "    \n",
    "    def __sync_target(self, tau):\n",
    "        for m_train, m_target in [\n",
    "            (self.__m_train, self.__m_target),\n",
    "            (self.__m_train_policy, self.__m_target_policy)]:\n",
    "            w_train = m_train.get_weights()\n",
    "            w_target = m_target.get_weights()\n",
    "            w = [tau*x + (1-tau)*y for x,y in zip(w_target,w_train)]\n",
    "            m_target.set_weights(w)\n",
    "    \n",
    "    def __get_sample_q(self, samples):\n",
    "        q = self.__m_target_policy.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32)],\n",
    "            batch_size=64)\n",
    "        q = self.__m_target.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),q],\n",
    "            batch_size=64)\n",
    "        q = np.array([s.reward for s in samples])[...,np.newaxis] + \\\n",
    "            q * np.array([s.gamma for s in samples])[...,np.newaxis]\n",
    "        return q\n",
    "    \n",
    "    def train(self, samples, epochs=1, verbose=False):\n",
    "        a = np.array([s.action for s in samples], dtype=np.float32)\n",
    "        q = self.__get_sample_q(samples)\n",
    "        self.__m_train.fit(\n",
    "            x=[np.array([s.state for s in samples], dtype=np.float32),a],\n",
    "            y=q,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "        self.__m_train_policy_chain.fit(\n",
    "            x=[np.array([s.state for s in samples], dtype=np.float32)],\n",
    "            y=np.zeros_like(q),\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose)\n",
    "        self.__sync_target(self.__tau)\n",
    "    \n",
    "    def get_sample_priority(self, samples):\n",
    "        q = self.__get_sample_q(samples)[:,0]\n",
    "        t = self.__m_train.predict([\n",
    "            np.array([s.state for s in samples], dtype=np.float32),\n",
    "            np.array([s.action for s in samples], dtype=np.float32)],\n",
    "            batch_size=64)[:,0]\n",
    "        p = np.abs(q - t)\n",
    "        return p\n",
    "    \n",
    "    def get_action(self, state, verbose=False):\n",
    "        action = self.__m_train_policy.predict([\n",
    "            np.array([state], dtype=np.float32)])[0]\n",
    "        if verbose:\n",
    "            print(action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ou_noise:\n",
    "    \n",
    "    def __init__(self, shape, sigma, mu=0., theta=0.15, dt=1.):\n",
    "        self.__shape = shape\n",
    "        self.__mu = np.ones(shape) * mu\n",
    "        self.__state = np.copy(self.__mu)\n",
    "        self.__theta = theta\n",
    "        self.__sigma = sigma\n",
    "        self.__dt = dt\n",
    "    \n",
    "    def sample(self):\n",
    "        self.__state += \\\n",
    "            self.__theta * (self.__mu - self.__state) * self.__dt + \\\n",
    "            self.__sigma * np.random.randn(*self.__shape) * self.__dt**2\n",
    "        return np.copy(self.__state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play(env, m, max_steps=1000, n_prev_states=8,\n",
    "         reward_steps=1, gamma=0.98, epsilon=0., verbose=False):\n",
    "    noise = ou_noise(env.action_space.shape, epsilon)\n",
    "    state_0 = env.reset()\n",
    "    state_null = np.zeros_like(state_0)\n",
    "    state_queue = []\n",
    "    episode = []\n",
    "    samples = []\n",
    "    actions = []\n",
    "    gamelen = 0\n",
    "    gamelen_max = 0\n",
    "    def get_prev_states(episode, idx, get_state):\n",
    "        states = [get_state(e) for e in episode[max(0,(idx-n_prev_states)+1):idx+1]]\n",
    "        states = [state_null]*(max(0,n_prev_states-len(states))) + states\n",
    "        return states\n",
    "    def add_to_samples(episode):\n",
    "        processed_episode = []\n",
    "        for i in range(len(episode)):\n",
    "            episode_reward = 0\n",
    "            episode_gamma = 1.\n",
    "            for j in range(reward_steps):\n",
    "                if i + j >= len(episode):\n",
    "                    break\n",
    "                episode_reward += episode[i+j].reward * episode_gamma\n",
    "                episode_gamma *= episode[i+j].gamma * gamma\n",
    "            processed_episode.append(model_sample(\n",
    "                get_prev_states(episode, i, lambda e: e.state),\n",
    "                get_prev_states(episode, i, lambda e: e.state1),\n",
    "                episode[i].action,\n",
    "                episode_reward,\n",
    "                episode_gamma, 0.))\n",
    "        priority = m.get_sample_priority(processed_episode)\n",
    "        for i in range(len(processed_episode)):\n",
    "            processed_episode[i] = processed_episode[i]._replace(priority=priority[i])\n",
    "        samples.extend(processed_episode)\n",
    "    for i in range(max_steps):\n",
    "        state_queue.append(state_0)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue\n",
    "        action = m.get_action(state_queue_padded)\n",
    "        actions.append(action)\n",
    "        action = action + noise.sample()\n",
    "        action_clipped = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "        state_1, reward, done, _ = env.step(action_clipped)\n",
    "        episode.append(model_sample(\n",
    "            state_0, state_1, action, reward, 0. if done else 1., 0.))\n",
    "        state_0 = state_1\n",
    "        gamelen += 1\n",
    "        if done:\n",
    "            add_to_samples(episode)\n",
    "            episode = []\n",
    "            state_0 = env.reset()\n",
    "            state_null = np.zeros_like(state_0)\n",
    "            state_queue = []\n",
    "            noise = ou_noise(env.action_space.shape, epsilon)\n",
    "            gamelen_max = max(gamelen_max, gamelen)\n",
    "            gamelen = 0\n",
    "    if episode:\n",
    "        add_to_samples(episode)\n",
    "        gamelen_max = max(gamelen_max, gamelen)\n",
    "        gamelen = 0\n",
    "    if verbose:\n",
    "        print('std[action]', np.mean(np.std(actions, ddof=1, axis=0)))\n",
    "        print('max game len', gamelen_max)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 1)            175245      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 1)            179537      input_7[0][0]                    \n",
      "                                                                 model_3[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 354,782\n",
      "Trainable params: 174,343\n",
      "Non-trainable params: 180,439\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "m = model(\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space.shape[0], tau=0.998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(m, max_steps=10000, render=False, n_prev_states=8):\n",
    "    global env\n",
    "    state = env.reset()\n",
    "    state_null = np.zeros_like(state)\n",
    "    state_queue = []\n",
    "    rewards = 0.\n",
    "    for _ in range(max_steps):\n",
    "        state_queue.append(state)\n",
    "        if len(state_queue) > n_prev_states:\n",
    "            state_queue.pop(0)\n",
    "        state_queue_padded = \\\n",
    "            [state_null]*(max(0,n_prev_states-len(state_queue))) + state_queue \n",
    "        action = m.get_action(state_queue)\n",
    "        action_clipped = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "        state, reward, done, _ = env.step(action_clipped)\n",
    "        rewards += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        if render:\n",
    "            time.sleep(1/60)\n",
    "    if render:\n",
    "        env.close()\n",
    "        env = env_create()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8f621ca3137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m replays = play(\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prev_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     reward_steps=3, epsilon=0.2, verbose=False)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     samples = play(\n",
      "\u001b[0;32m<ipython-input-6-4a69f9e02b07>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, m, max_steps, n_prev_states, reward_steps, gamma, epsilon, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mstate_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mstate_queue_padded\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mstate_null\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_prev_states\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_queue_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ca7e9fcec64d>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state, verbose)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         action = self.__m_train_policy.predict([\n\u001b[0;32m--> 138\u001b[0;31m             np.array([state], dtype=np.float32)])[0]\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replays = play(\n",
    "    env, m, max_steps=30000, n_prev_states=4,\n",
    "    reward_steps=3, epsilon=0.2, verbose=False)\n",
    "for i in range(300):\n",
    "    samples = play(\n",
    "        env, m, max_steps=3000, n_prev_states=4,\n",
    "        reward_steps=3, epsilon=0.2, verbose=True)\n",
    "    replays.extend(samples)\n",
    "    if len(replays) > 100000:\n",
    "        replays = list(replays[-100000:])\n",
    "    for j in range(300):\n",
    "        sample_p = np.array([r.priority for r in replays])\n",
    "        sample_p = np.power(sample_p, 0.5)\n",
    "        sample_p = sample_p / np.sum(sample_p)\n",
    "        sample_indices = np.random.choice(\n",
    "            np.arange(len(replays), dtype=np.int32), p=sample_p, size=128)\n",
    "        samples = [model_sample(*s) \\\n",
    "            for s in np.array(replays)[sample_indices]]\n",
    "        sample_p = m.get_sample_priority(samples)\n",
    "        for s,p in zip(sample_indices, sample_p):\n",
    "            replays[s] = replays[s]._replace(priority=p)\n",
    "        m.train(samples, epochs=1, verbose=(j+1)%100==0)\n",
    "    print('epoch {} completed'.format(i))\n",
    "    test_result = [test(m, max_steps=3000,\n",
    "        n_prev_states=4, render=(i==0)) for i in range(3)]\n",
    "    print('test result',\n",
    "          'mean', np.mean(test_result),\n",
    "          'std', np.std(test_result, ddof=1))\n",
    "    if np.mean(test_result) == env_max_possible_score and \\\n",
    "        np.std(test_result, ddof=1) == 0:\n",
    "        print('the network always gets full score, early exit')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(m, n_prev_states=4, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
