{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation of a neural network\n",
    "Let $\\aleph\\aleph(X|\\Theta) = Y$ be a neural network with inputs $x_{i} \\in X$, outputs $\\hat{y}_{j} \\in \\hat{Y}$ and weights $\\theta_{k} \\in \\Theta$. One could also treat $\\aleph\\aleph$ as a vector function $\\aleph\\aleph(X) = Y$.\n",
    "### To approximate a function\n",
    "To approximate a vector function $F(X) = Y$, where $Y$ is a vector with elements $y_{j} \\in Y$. With samples of $F(X)$, one could use the mean squared error loss function $\\mathcal{L}_{MSE}(X,Y|\\Theta) = \\mathbf{E}_{X,Y}[(\\aleph\\aleph(X|\\Theta) - Y)^{2}]$, where $\\mathbf{E}$ is the expected value notation. One could further perform gradient descent to minimize the loss function by updating the weights with $\\theta_{k} \\leftarrow \\theta_{k} - \\alpha\\frac{\\partial}{\\partial\\theta_{k}}\\mathcal{L}_{MSE}(X,Y|\\Theta)$, where $\\alpha$ is the learning rate.\n",
    "### To approximate the derivative of a function\n",
    "To approximate the derivative of a function, or more specifically, since the function is a vector function, to approximate the jacobian matrix of a function $F(X) = Y$, where the jacobain matrix is defined as $\\mathbf{J}_{ji} = \\frac{\\partial}{\\partial x_{i}}F(X)_{j} = \\frac{\\partial y_{j}}{\\partial x_{i}}$.\n",
    "If a neural network $\\aleph\\aleph(X|\\Theta)$ approximate $F(X) = Y$, where we denote as $F(X) \\approx \\aleph\\aleph(X|\\Theta)$, The jacobian matrix of the neural network with also approximate the jacobian matrix of the function, where we denote as $\\frac{\\partial}{\\partial x_{i}}F(X)_{j} \\approx \\frac{\\partial}{\\partial x_{i}}\\aleph\\aleph(X|\\Theta)_{j}$, and $\\frac{\\partial y_{j}}{\\partial x_{i}} \\approx \\frac{\\partial \\hat{y}_{j}}{\\partial x_{i}}$.\n",
    "We call this transformed network with inputs $x_{i} \\in X$, outputs $\\frac{\\partial \\hat{y}_{j}}{\\partial x_{i}} \\in \\mathbb{R}^{|\\hat{Y}|\\times|X|}$ and shared weights $\\theta_{k} \\in \\Theta$ as the derivative network with a fancy name dNN/dx.\n",
    "### The derivative network is also a universal function approximator\n",
    "If one have samples of $X$ to $\\mathbf{J}_{F}$ instead, one could still approximate $F(X) \\approx \\aleph\\aleph(X|\\Theta)$ by perform gradient descent on the derivative network with the modified loss function $\\mathcal{L}_{MSE}^{'}(X,\\mathbf{J}_{F}(X)|\\Theta) = \\mathbf{E}_{X,\\mathbf{J}_{F}(X)}[(\\frac{\\partial}{\\partial x_{i}}\\aleph\\aleph(X|\\Theta) - \\frac{\\partial}{\\partial x_{i}}F(X))^{2}]$, and updating the weights with $\\theta_{k} \\leftarrow \\theta_{k} - \\alpha\\frac{\\partial}{\\partial\\theta_{k}}\\mathcal{L}_{MSE}^{'}(X,\\mathbf{J}_{F}(X)|\\Theta)$.\n",
    "### The derivative network can be trianned with only part of the jacobian matrix\n",
    "Lets consider a dynamic equation set $G$ mapping inputs $x_{i} \\in X$ to outputs $y_{i}^{G} \\in Y^{G}$, where the number of inputs and outputs are equal, and its jacobian matrix $\\mathbf{J}_{G}$ is a square matrix. The samples of G is only consists of X mapping to the diagonal elements of its jacobian matrix, $j_{ii} \\in \\mathbf{J}_{G}$. one could performs gradient descent with loss function $\\mathcal{J}_{MSE}^{'}(X,j_{ii} \\forall i)$, and updating the weights with $\\theta_{k} \\leftarrow \\theta_{k} - \\alpha\\frac{\\partial}{\\partial\\theta_{k}}\\mathcal{L}_{MSE}^{'}(X,j_{ii} \\forall i|\\Theta)$. If the derivative network outputs approximate the diagonal elements of the jacobian matrix, $\\frac{\\partial}{\\partial x_{i}}\\aleph\\aleph(X|\\Theta)_{i} \\approx \\frac{\\partial y_{i}^{G}}{\\partial x_{i}}, \\forall i$, the original neural network will approximate $G(X)$, $\\aleph\\aleph(X|\\Theta) \\approx G(X)$. More importantly, the derivative network will also learns the correlations between the equations in the dynamic equation set and approximate the non-diagonal elements of $\\mathbf{J}_{G}$, $\\frac{\\partial}{\\partial x_{i}}\\aleph\\aleph(X|\\Theta)_{j} \\approx \\frac{\\partial y_{j}^{G}}{\\partial x_{i}}, \\forall ij, i \\neq j$. And this applies to not only trainning with only diagonal elements of $\\mathbf{J}_{G}$, but any elements combination, as long as the elements are sufficent to describe the correlation between the equations in the dynamic equation set.\n",
    "### To approximate the antiderivative of a function\n",
    "Lets continue to consider the dynamic equation set $G$. if the equations in $G$ is actually the derivatives of some function $H(X) \\in \\mathbb{R}^{|X|}\\rightarrow\\mathbb{R}^{|X|}$, where $y_{i}^{G} = \\frac{\\partial}{\\partial x_{i}}H(X)_{i}$, then $H(X)$ is equal to the antiderivative of the equations in $G$, $H(X) = \\int y_{i}^{G} d x_{i}$ and all antiderivative function is equal, one could again use the derivative network to approximate the antiderivative function $H(X)$ by perform gradient descent on the derivative network with the rephrased modified loss function $\\mathcal{L}_{MSE}^{'}(X,\\frac{\\partial}{\\partial x_{i}}H(X)_{i} \\forall i|\\Theta) = \\mathbf{E}_{X}[(\\frac{\\partial}{\\partial x_{i}}\\aleph\\aleph(X|\\Theta) - \\frac{\\partial}{\\partial x_{i}}H(X)_{i})^{2}]$, and updating the weights with $\\theta_{k} \\leftarrow \\theta_{k} - \\alpha\\frac{\\partial}{\\partial\\theta_{k}}\\mathcal{L}_{MSE}^{'}(X,\\frac{\\partial}{\\partial x_{i}}H(X)_{i} \\forall i|\\Theta)$. if the derivative network approximates the samples of G, denoted as $\\frac{\\partial}{\\partial x_{i}}H(X)_{i} \\approx \\frac{\\partial}{\\partial x_{i}}\\aleph\\aleph(X|\\Theta)_{i}$, then the original neural network will approximate the antiderivative function with a shifting constant $C^{'}$, denoted as $H(X) \\approx \\aleph\\aleph(X|\\Theta) + C^{'}$ and $\\int y_{i}^{G} d x_{i} \\approx \\aleph\\aleph(X|\\Theta) + C^{'}, \\forall i$.\n",
    "### The approximated definite integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
